{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 9 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation dataset 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/endteamschoolofai/END/blob/main/Session9/Assignment_9_Learning_Phrase_Representations_using_RNN_Encoder_Decoder_for_Statistical_Machine_Translation_dataset_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzxOJwN7EzFO"
      },
      "source": [
        "# 2 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n",
        "\n",
        "In this second notebook on sequence-to-sequence models using PyTorch and TorchText, we'll be implementing the model from [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078). This model will achieve improved test perplexity whilst only using a single layer RNN in both the encoder and the decoder.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Let's remind ourselves of the general encoder-decoder model.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq1.png?raw=1)\n",
        "\n",
        "We use our encoder (green) over the embedded source sequence (yellow) to create a context vector (red). We then use that context vector with the decoder (blue) and a linear layer (purple) to generate the target sentence.\n",
        "\n",
        "In the previous model, we used an multi-layered LSTM as the encoder and decoder.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq4.png?raw=1)\n",
        "\n",
        "One downside of the previous model is that the decoder is trying to cram lots of information into the hidden states. Whilst decoding, the hidden state will need to contain information about the whole of the source sequence, as well as all of the tokens have been decoded so far. By alleviating some of this information compression, we can create a better model!\n",
        "\n",
        "We'll also be using a GRU (Gated Recurrent Unit) instead of an LSTM (Long Short-Term Memory). Why? Mainly because that's what they did in the paper (this paper also introduced GRUs) and also because we used LSTMs last time. To understand how GRUs (and LSTMs) differ from standard RNNS, check out [this](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) link. Is a GRU better than an LSTM? [Research](https://arxiv.org/abs/1412.3555) has shown they're pretty much the same, and both are better than standard RNNs. \n",
        "\n",
        "## Preparing Data\n",
        "\n",
        "All of the data preparation will be (almost) the same as last time, so we'll very briefly detail what each code block does. See the previous notebook for a recap.\n",
        "\n",
        "We'll import PyTorch, TorchText, spaCy and a few standard modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUnMLdevEzFT"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext import data \n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srzErHAsEzFU"
      },
      "source": [
        "Then set a random seed for deterministic results/reproducability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_wP4J4LEzFX"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4davOyYEzFd"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkKukofWCC0x"
      },
      "source": [
        "# Dataset 2 - CommonsenseQA "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EosVtuAxwDDY",
        "outputId": "0edb8e86-741d-4a1d-ab2d-82f033e2ce8a"
      },
      "source": [
        "!wget https://s3.amazonaws.com/commensenseqa/train_rand_split.jsonl\r\n",
        "!wget https://s3.amazonaws.com/commensenseqa/dev_rand_split.jsonl\r\n",
        "!wget https://s3.amazonaws.com/commensenseqa/test_rand_split_no_answers.jsonl"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-08 16:12:37--  https://s3.amazonaws.com/commensenseqa/train_rand_split.jsonl\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.43.158\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.43.158|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3785890 (3.6M) [binary/octet-stream]\n",
            "Saving to: ‘train_rand_split.jsonl’\n",
            "\n",
            "train_rand_split.js 100%[===================>]   3.61M  1.83MB/s    in 2.0s    \n",
            "\n",
            "2021-01-08 16:12:40 (1.83 MB/s) - ‘train_rand_split.jsonl’ saved [3785890/3785890]\n",
            "\n",
            "--2021-01-08 16:12:40--  https://s3.amazonaws.com/commensenseqa/dev_rand_split.jsonl\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.43.158\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.43.158|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 471653 (461K) [binary/octet-stream]\n",
            "Saving to: ‘dev_rand_split.jsonl’\n",
            "\n",
            "dev_rand_split.json 100%[===================>] 460.60K   641KB/s    in 0.7s    \n",
            "\n",
            "2021-01-08 16:12:42 (641 KB/s) - ‘dev_rand_split.jsonl’ saved [471653/471653]\n",
            "\n",
            "--2021-01-08 16:12:42--  https://s3.amazonaws.com/commensenseqa/test_rand_split_no_answers.jsonl\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.96.157\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.96.157|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 423148 (413K) [binary/octet-stream]\n",
            "Saving to: ‘test_rand_split_no_answers.jsonl’\n",
            "\n",
            "test_rand_split_no_ 100%[===================>] 413.23K   567KB/s    in 0.7s    \n",
            "\n",
            "2021-01-08 16:12:43 (567 KB/s) - ‘test_rand_split_no_answers.jsonl’ saved [423148/423148]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sjs6K368wOGQ"
      },
      "source": [
        "train_df_second = pd.read_json('train_rand_split.jsonl', lines = True)\r\n",
        "valid_df_second = pd.read_json('dev_rand_split.jsonl', lines = True)\r\n",
        "test_df_second = pd.read_json('test_rand_split_no_answers.jsonl', lines = True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "8cv18o4GwVvy",
        "outputId": "36966a02-6f1b-4fae-83c6-957485563bea"
      },
      "source": [
        "train_df_second.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>answerKey</th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A</td>\n",
              "      <td>075e483d21c29a511267ef62bedc0461</td>\n",
              "      <td>{'question_concept': 'punishing', 'choices': [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>B</td>\n",
              "      <td>61fe6e879ff18686d7552425a36344c8</td>\n",
              "      <td>{'question_concept': 'people', 'choices': [{'l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A</td>\n",
              "      <td>4c1cb0e95b99f72d55c068ba0255c54d</td>\n",
              "      <td>{'question_concept': 'choker', 'choices': [{'l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>D</td>\n",
              "      <td>02e821a3e53cb320790950aab4489e85</td>\n",
              "      <td>{'question_concept': 'highway', 'choices': [{'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>C</td>\n",
              "      <td>23505889b94e880c3e89cff4ba119860</td>\n",
              "      <td>{'question_concept': 'fox', 'choices': [{'labe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  answerKey  ...                                           question\n",
              "0         A  ...  {'question_concept': 'punishing', 'choices': [...\n",
              "1         B  ...  {'question_concept': 'people', 'choices': [{'l...\n",
              "2         A  ...  {'question_concept': 'choker', 'choices': [{'l...\n",
              "3         D  ...  {'question_concept': 'highway', 'choices': [{'...\n",
              "4         C  ...  {'question_concept': 'fox', 'choices': [{'labe...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "s6rfVCUtN8tQ",
        "outputId": "332e246e-c607-4829-d353-566c1f0bd31f"
      },
      "source": [
        "valid_df_second.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>answerKey</th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A</td>\n",
              "      <td>1afa02df02c908a558b4036e80242fac</td>\n",
              "      <td>{'question_concept': 'revolving door', 'choice...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A</td>\n",
              "      <td>a7ab086045575bb497933726e4e6ad28</td>\n",
              "      <td>{'question_concept': 'people', 'choices': [{'l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>B</td>\n",
              "      <td>b8c0a4703079cf661d7261a60a1bcbff</td>\n",
              "      <td>{'question_concept': 'magazines', 'choices': [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A</td>\n",
              "      <td>e68fb2448fd74e402aae9982aa76e527</td>\n",
              "      <td>{'question_concept': 'hamburger', 'choices': [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A</td>\n",
              "      <td>2435de612dd69f2012b9e40d6af4ce38</td>\n",
              "      <td>{'question_concept': 'farmland', 'choices': [{...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  answerKey  ...                                           question\n",
              "0         A  ...  {'question_concept': 'revolving door', 'choice...\n",
              "1         A  ...  {'question_concept': 'people', 'choices': [{'l...\n",
              "2         B  ...  {'question_concept': 'magazines', 'choices': [...\n",
              "3         A  ...  {'question_concept': 'hamburger', 'choices': [...\n",
              "4         A  ...  {'question_concept': 'farmland', 'choices': [{...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "gHvKAW3XRwWS",
        "outputId": "07ad8179-e674-432e-cc6a-5bd20b15b826"
      },
      "source": [
        "test_df_second.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>90b30172e645ff91f7171a048582eb8b</td>\n",
              "      <td>{'question_concept': 'townhouse', 'choices': [...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000990552527b1353f98f1e1a7dfc643</td>\n",
              "      <td>{'question_concept': 'star', 'choices': [{'lab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dca0f2859f3c3dd43a9b2bfeff4936a8</td>\n",
              "      <td>{'question_concept': 'kids', 'choices': [{'lab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8795a949b39702af0e452c9e1229046d</td>\n",
              "      <td>{'question_concept': 'person', 'choices': [{'l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1f74ea1f73b9f5d91a665b4d90218a6e</td>\n",
              "      <td>{'question_concept': 'ignorance', 'choices': [...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                 id                                           question\n",
              "0  90b30172e645ff91f7171a048582eb8b  {'question_concept': 'townhouse', 'choices': [...\n",
              "1  000990552527b1353f98f1e1a7dfc643  {'question_concept': 'star', 'choices': [{'lab...\n",
              "2  dca0f2859f3c3dd43a9b2bfeff4936a8  {'question_concept': 'kids', 'choices': [{'lab...\n",
              "3  8795a949b39702af0e452c9e1229046d  {'question_concept': 'person', 'choices': [{'l...\n",
              "4  1f74ea1f73b9f5d91a665b4d90218a6e  {'question_concept': 'ignorance', 'choices': [..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2I3_jfeVmYt",
        "outputId": "278938e7-5fd7-442c-9770-0a5c27b84d67"
      },
      "source": [
        "len(train_df_second), len(valid_df_second), len(test_df_second)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9741, 1221, 1140)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_BWhpwzBZQR",
        "outputId": "a09bb9d2-4760-4b38-9ad3-3b5a6ba41106"
      },
      "source": [
        "train_df_second.question[0], train_df_second.answerKey[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'choices': [{'label': 'A', 'text': 'ignore'},\n",
              "   {'label': 'B', 'text': 'enforce'},\n",
              "   {'label': 'C', 'text': 'authoritarian'},\n",
              "   {'label': 'D', 'text': 'yell at'},\n",
              "   {'label': 'E', 'text': 'avoid'}],\n",
              "  'question_concept': 'punishing',\n",
              "  'stem': 'The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?'},\n",
              " 'A')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sb7mBnN6OBu7",
        "outputId": "4e76e2aa-19ac-4511-aa1a-f46545d8129e"
      },
      "source": [
        "valid_df_second.question[0], valid_df_second.answerKey[0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'choices': [{'label': 'A', 'text': 'bank'},\n",
              "   {'label': 'B', 'text': 'library'},\n",
              "   {'label': 'C', 'text': 'department store'},\n",
              "   {'label': 'D', 'text': 'mall'},\n",
              "   {'label': 'E', 'text': 'new york'}],\n",
              "  'question_concept': 'revolving door',\n",
              "  'stem': 'A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?'},\n",
              " 'A')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SgbAbJbR44Q",
        "outputId": "49908372-1a3d-41f8-8e78-68065b313c97"
      },
      "source": [
        "test_df_second.question[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'choices': [{'label': 'A', 'text': 'suburban development'},\n",
              "  {'label': 'B', 'text': 'apartment building'},\n",
              "  {'label': 'C', 'text': 'bus stop'},\n",
              "  {'label': 'D', 'text': 'michigan'},\n",
              "  {'label': 'E', 'text': 'suburbs'}],\n",
              " 'question_concept': 'townhouse',\n",
              " 'stem': 'The townhouse was a hard sell for the realtor, it was right next to a high rise what?'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqbCLhPKxfov",
        "outputId": "30f309bf-0970-4806-c888-0bcfd6ff7852"
      },
      "source": [
        "choice_labels = [train_df_second.question[0]['choices'][i]['label'] for i in range(len(train_df_second.question[0]['choices']))] #for j in range(1)]\r\n",
        "choice_labels"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A', 'B', 'C', 'D', 'E']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lfkNobADAjD"
      },
      "source": [
        "train_label_pos = [choice_labels.index(train_df_second.answerKey[i]) for i in range(len(train_df_second))]\r\n",
        "valid_label_pos = [choice_labels.index(valid_df_second.answerKey[j]) for j in range(len(valid_df_second))]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWXT_pfaDAaI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "528759e4-e8c4-47b8-817d-24fa91066e16"
      },
      "source": [
        "train_questions_second = [train_df_second.question[i]['stem'] for i in range(len(train_df_second))]\r\n",
        "train_answers_second = [train_df_second.question[i]['choices'][train_label_pos[i]]['text'] for i in range(len(train_df_second))]\r\n",
        "\r\n",
        "valid_questions_second = [valid_df_second.question[i]['stem'] for i in range(len(valid_df_second))]\r\n",
        "valid_answers_second = [valid_df_second.question[i]['choices'][valid_label_pos[i]]['text'] for i in range(len(valid_df_second))]\r\n",
        "\r\n",
        "test_questions_second = [test_df_second.question[i]['stem'] for i in range(len(test_df_second))]\r\n",
        "\r\n",
        "train_questions_second[:5], train_answers_second[:5], valid_questions_second[:5], valid_answers_second[:5], test_questions_second[:5] "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?',\n",
              "  'Sammy wanted to go to where the people were.  Where might he go?',\n",
              "  'To locate a choker not located in a jewelry box or boutique where would you go?',\n",
              "  'Google Maps and other highway and street GPS services have replaced what?',\n",
              "  'The fox walked from the city into the forest, what was it looking for?'],\n",
              " ['ignore', 'populated areas', 'jewelry store', 'atlas', 'natural habitat'],\n",
              " ['A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?',\n",
              "  'What do people aim to do at work?',\n",
              "  'Where would you find magazines along side many other printed works?',\n",
              "  'Where are  you likely to find a hamburger?',\n",
              "  'James was looking for a good place to buy farmland.  Where might he look?'],\n",
              " ['bank', 'complete job', 'bookstore', 'fast food restaurant', 'midwest'],\n",
              " ['The townhouse was a hard sell for the realtor, it was right next to a high rise what?',\n",
              "  'There is a star at the center of what group of celestial bodies?',\n",
              "  'What were the kids doing as they looked up at the sky and clouds?',\n",
              "  'The person taught an advanced class only for who?',\n",
              "  'What is a likely consequence of ignorance of rules?'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ems_JGsdjvQt"
      },
      "source": [
        "Questions_Second = data.Field(sequential = True, tokenize = 'spacy', batch_first =True)#, include_lengths=True)\r\n",
        "Answers_Second = data.Field(tokenize ='spacy', is_target=True, batch_first =True)#, sequential =False)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-gFmFbLlDTW"
      },
      "source": [
        "fields = [('questions_second', Questions_Second),('answers_second', Answers_Second)]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIBCFd9tlZm1"
      },
      "source": [
        "example1 = [data.Example.fromlist([train_questions_second[i], train_answers_second[i]], fields) for i in range(len(train_df_second))] \r\n",
        "example2 = [data.Example.fromlist([valid_questions_second[i], valid_answers_second[i]], fields) for i in range(len(valid_df_second))] \r\n",
        "example3 = [data.Example.fromlist([test_questions_second[i], None], fields) for i in range(len(test_df_second))] "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBWeZdpTlpfu"
      },
      "source": [
        "Train_Dataset_Second = data.Dataset(example1, fields)\r\n",
        "Valid_Dataset_Second = data.Dataset(example2, fields)\r\n",
        "Test_Dataset_Second = data.Dataset(example3, fields)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9NreRd3luzN",
        "outputId": "1bd781f0-3c86-4429-909a-b08c2a712236"
      },
      "source": [
        "len(Train_Dataset_Second), len(Valid_Dataset_Second), len(Test_Dataset_Second)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9741, 1221, 1140)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQbcmHi1lz2G",
        "outputId": "73377e23-a808-40e7-8488-9d4555929869"
      },
      "source": [
        "print(vars(Train_Dataset_Second.examples[1]))\r\n",
        "print(vars(Valid_Dataset_Second.examples[1]))\r\n",
        "print(vars(Test_Dataset_Second.examples[1]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'questions_second': ['Sammy', 'wanted', 'to', 'go', 'to', 'where', 'the', 'people', 'were', '.', ' ', 'Where', 'might', 'he', 'go', '?'], 'answers_second': ['populated', 'areas']}\n",
            "{'questions_second': ['What', 'do', 'people', 'aim', 'to', 'do', 'at', 'work', '?'], 'answers_second': ['complete', 'job']}\n",
            "{'questions_second': ['There', 'is', 'a', 'star', 'at', 'the', 'center', 'of', 'what', 'group', 'of', 'celestial', 'bodies', '?'], 'answers_second': None}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Crv-fUWuJsy2"
      },
      "source": [
        "Questions_Second.build_vocab(Train_Dataset_Second)\r\n",
        "Answers_Second.build_vocab(Train_Dataset_Second)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVI4qwWtWpVM",
        "outputId": "0a06f390-efbc-456c-d02e-015ebda8f995"
      },
      "source": [
        "print('Size of input vocab : ', len(Questions_Second.vocab))\r\n",
        "print('Size of output vocab : ', len(Answers_Second.vocab))\r\n",
        "print('Top 10 words appreared repeatedly :', list(Questions_Second.vocab.freqs.most_common(10)))\r\n",
        "print('Labels : ', Answers_Second.vocab.stoi)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  8634\n",
            "Size of output vocab :  3821\n",
            "Top 10 words appreared repeatedly : [('?', 9680), ('a', 5310), ('to', 5086), ('the', 3972), ('what', 3871), (',', 3641), ('you', 2613), ('What', 2330), ('is', 2251), ('was', 2208)]\n",
            "Labels :  defaultdict(<function _default_unk_index at 0x7f3d74d83f28>, {'<unk>': 0, '<pad>': 1, 'store': 2, 'house': 3, 'to': 4, 'of': 5, \"'s\": 6, 'office': 7, 'room': 8, 'city': 9, 'get': 10, 'building': 11, 'school': 12, 'money': 13, 'new': 14, 'have': 15, 'go': 16, 'home': 17, 'in': 18, 'music': 19, 'being': 20, 'water': 21, 'down': 22, 'food': 23, 'feel': 24, 'park': 25, 'fun': 26, 'area': 27, 'own': 28, 'restaurant': 29, 'shop': 30, 'work': 31, 'good': 32, 'game': 33, 'kitchen': 34, 'cabinet': 35, 'countryside': 36, 'out': 37, 'people': 38, 'make': 39, 'town': 40, 'getting': 41, 'table': 42, 'apartment': 43, 'desk': 44, 'play': 45, 'better': 46, 'tired': 47, 'up': 48, 'for': 49, 'michigan': 50, 'death': 51, 'feeling': 52, 'hotel': 53, 'market': 54, 'station': 55, 'street': 56, 'supermarket': 57, 'band': 58, 'car': 59, 'drawer': 60, 'show': 61, 'eat': 62, 'ground': 63, 'knowledge': 64, 'box': 65, 'at': 66, 'classroom': 67, 'fall': 68, 'happiness': 69, 'ocean': 70, 'pain': 71, 'rest': 72, 'time': 73, 'airport': 74, 'book': 75, 'bus': 76, 'grocery': 77, 'learn': 78, 'refrigerator': 79, 'television': 80, 'cupboard': 81, 'great': 82, 'mall': 83, 'meeting': 84, 'movie': 85, 'sporting': 86, 'drink': 87, 'hospital': 88, 'public': 89, 'space': 90, 'basement': 91, 'become': 92, 'bedroom': 93, 'closet': 94, 'earth': 95, 'friend': 96, 'garage': 97, 'jail': 98, 'large': 99, 'neighbor': 100, 'stress': 101, 'take': 102, 'toy': 103, 'train': 104, 'church': 105, 'gain': 106, 'high': 107, 'library': 108, 'person': 109, 'states': 110, 'york': 111, 'bad': 112, 'bathroom': 113, 'die': 114, 'full': 115, 'garden': 116, 'going': 117, 'life': 118, 'more': 119, 'other': 120, 'stop': 121, 'theater': 122, 'weight': 123, 'and': 124, 'falling': 125, 'front': 126, 'having': 127, 'museum': 128, 'open': 129, 'place': 130, 'pleasure': 131, 'relaxation': 132, 'state': 133, 'united': 134, 'america': 135, 'asleep': 136, 'each': 137, 'hard': 138, 'pocket': 139, 'shelf': 140, 'shopping': 141, 'shuttle': 142, 'south': 143, 'things': 144, 'use': 145, 'wet': 146, 'with': 147, 'yard': 148, 'backpack': 149, 'bed': 150, 'big': 151, 'california': 152, 'children': 153, 'country': 154, 'happy': 155, 'health': 156, 'jar': 157, 'love': 158, 'outside': 159, 'sleep': 160, 'theatre': 161, 'bar': 162, 'body': 163, 'center': 164, 'department': 165, 'living': 166, 'mexico': 167, 'not': 168, 'pantry': 169, 'sex': 170, 'sky': 171, 'themselves': 172, 'tree': 173, 'war': 174, 'about': 175, 'art': 176, 'away': 177, 'back': 178, 'concert': 179, 'disneyland': 180, 'england': 181, 'floor': 182, 'goods': 183, 'loss': 184, 'medicine': 185, 'off': 186, 'orchestra': 187, 'satisfaction': 188, 'see': 189, 'somewhere': 190, 'surface': 191, 'talk': 192, 'air': 193, 'alley': 194, 'cold': 195, 'container': 196, 'desktop': 197, 'door': 198, 'experience': 199, 'fast': 200, 'fear': 201, 'feelings': 202, 'hardware': 203, 'human': 204, 'information': 205, 'injury': 206, 'line': 207, 'listen': 208, 'military': 209, 'north': 210, 'old': 211, 'on': 212, 'soccer': 213, 'think': 214, 'university': 215, 'yourself': 216, 'anger': 217, 'bank': 218, 'buy': 219, 'class': 220, 'club': 221, 'computer': 222, 'do': 223, 'field': 224, 'forest': 225, 'frustration': 226, 'hockey': 227, 'mail': 228, 'mouth': 229, 'opera': 230, 'study': 231, 'suitcase': 232, 'together': 233, 'zoo': 234, 'anxiety': 235, 'boat': 236, 'books': 237, 'canada': 238, 'coffee': 239, 'college': 240, 'communication': 241, 'corner': 242, 'downtown': 243, 'event': 244, 'fish': 245, 'free': 246, 'gas': 247, 'hands': 248, 'headaches': 249, 'heart': 250, 'himself': 251, 'lake': 252, 'learning': 253, 'legs': 254, 'live': 255, 'movies': 256, 'need': 257, 'or': 258, 'outdoors': 259, 'pool': 260, 'sea': 261, 'stadium': 262, 'supply': 263, 'wait': 264, 'wall': 265, 'warm': 266, 'will': 267, 'write': 268, 'base': 269, 'control': 270, 'depot': 271, 'dinner': 272, 'exhaustion': 273, 'fire': 274, 'hall': 275, 'head': 276, 'increased': 277, 'laboratory': 278, 'meet': 279, 'pass': 280, 'problems': 281, 'produce': 282, 'purse': 283, 'read': 284, 'still': 285, 'very': 286, 'washington': 287, 'well': 288, 'working': 289, 'you': 290, 'amusement': 291, 'ask': 292, 'bowling': 293, 'cellar': 294, 'change': 295, 'cool': 296, 'cup': 297, 'demonstration': 298, 'energy': 299, 'every': 300, 'eyes': 301, 'fairgrounds': 302, 'fatigue': 303, 'fight': 304, 'fountain': 305, 'friends': 306, 'furniture': 307, 'losing': 308, 'making': 309, 'pay': 310, 'plate': 311, 'race': 312, 'roof': 313, 'ship': 314, 'smile': 315, 'stay': 316, 'stomach': 317, 'swimming': 318, 'turn': 319, 'two': 320, 'american': 321, 'beach': 322, 'boredom': 323, 'breath': 324, 'bridge': 325, 'business': 326, 'cafe': 327, 'case': 328, 'clear': 329, 'clothes': 330, 'comfortable': 331, 'conference': 332, 'doctor': 333, 'drug': 334, 'enjoy': 335, 'family': 336, 'farmer': 337, 'fridge': 338, 'fruit': 339, 'god': 340, 'hair': 341, 'headache': 342, 'hear': 343, 'help': 344, 'inspiration': 345, 'joy': 346, 'lie': 347, 'light': 348, 'lose': 349, 'machine': 350, 'many': 351, 'motel': 352, 'mountains': 353, 'newspaper': 354, 'northern': 355, 'party': 356, 'post': 357, 'relief': 358, 'reproduce': 359, 'river': 360, 'run': 361, 'satisfied': 362, 'say': 363, 'sit': 364, 'sweating': 365, 'system': 366, 'texas': 367, 'tool': 368, 'travel': 369, 'auditorium': 370, 'backyard': 371, 'bag': 372, 'board': 373, 'chess': 374, 'construction': 375, 'court': 376, 'deep': 377, 'dry': 378, 'early': 379, 'express': 380, 'feet': 381, 'freedom': 382, 'from': 383, 'grow': 384, 'hand': 385, 'injuries': 386, 'internet': 387, 'like': 388, 'look': 389, 'lot': 390, 'lower': 391, 'neighborhood': 392, 'night': 393, 'no': 394, 'pacific': 395, 'questions': 396, 'radio': 397, 'relax': 398, 'repair': 399, 'rock': 400, 'sentence': 401, 'synagogue': 402, 'track': 403, 'universe': 404, 'urban': 405, \"'re\": 406, 'accidents': 407, 'arm': 408, 'atlantic': 409, 'atlas': 410, 'attic': 411, 'blisters': 412, 'blood': 413, 'bones': 414, 'bored': 415, 'breathe': 416, 'brown': 417, 'campus': 418, 'care': 419, 'carnival': 420, 'carolina': 421, 'commercial': 422, 'community': 423, 'company': 424, 'complete': 425, 'create': 426, 'cross': 427, 'dance': 428, 'depression': 429, 'dream': 430, 'drugstore': 431, 'enjoyment': 432, 'entertained': 433, 'entertainment': 434, 'everything': 435, 'excitement': 436, 'flowers': 437, 'fly': 438, 'freeway': 439, 'guilty': 440, 'hurt': 441, 'lab': 442, 'laugh': 443, 'leave': 444, 'livingroom': 445, 'manhattan': 446, 'move': 447, 'muscles': 448, 'nature': 449, 'nausea': 450, 'noise': 451, 'one': 452, 'over': 453, 'paper': 454, 'pet': 455, 'pizza': 456, 'porch': 457, 'pot': 458, 'prepare': 459, 'pride': 460, 'prison': 461, 'problem': 462, 'punishment': 463, 'raise': 464, 'rug': 465, 'rural': 466, 'san': 467, 'set': 468, 'sitting': 469, 'small': 470, 'smoke': 471, 'solar': 472, 'sorrow': 473, 'speak': 474, 'sports': 475, 'stage': 476, 'stand': 477, 'sweat': 478, 'symphony': 479, 'tropical': 480, 'trouble': 481, 'trunk': 482, 'truth': 483, 'understand': 484, 'warehouse': 485, 'watch': 486, 'wear': 487, 'wedding': 488, 'win': 489, 'windowsill': 490, 'africa': 491, 'aircraft': 492, 'airplane': 493, 'attacks': 494, 'becoming': 495, 'beer': 496, 'bookstore': 497, 'breathing': 498, 'bring': 499, 'broken': 500, 'carpet': 501, 'catch': 502, 'cause': 503, 'celebrate': 504, 'chest': 505, 'cities': 506, 'clean': 507, 'compete': 508, 'complex': 509, 'cost': 510, 'course': 511, 'cow': 512, 'dangerous': 513, 'dentist': 514, 'detroit': 515, 'dishes': 516, 'east': 517, 'end': 518, 'enemy': 519, 'eye': 520, 'fairy': 521, 'fishing': 522, 'flower': 523, 'form': 524, 'four': 525, 'freezer': 526, 'french': 527, 'further': 528, 'games': 529, 'georgia': 530, 'give': 531, 'guilt': 532, 'hampshire': 533, 'hemisphere': 534, 'history': 535, 'hold': 536, 'houses': 537, 'hungry': 538, 'ideas': 539, 'kentucky': 540, 'kill': 541, 'kiss': 542, 'late': 543, 'let': 544, 'london': 545, 'louisiana': 546, 'marching': 547, 'middle': 548, 'might': 549, 'montana': 550, 'most': 551, 'mountain': 552, 'moving': 553, 'national': 554, 'natural': 555, 'nice': 556, 'nothing': 557, 'panic': 558, 'peace': 559, 'physical': 560, 'pile': 561, 'plants': 562, 'question': 563, 'rain': 564, 'road': 565, 'safe': 566, 'science': 567, 'sing': 568, 'site': 569, 'skill': 570, 'skin': 571, 'spend': 572, 'story': 573, 'strip': 574, 'studio': 575, 'subway': 576, 'suffering': 577, 'sun': 578, 'talking': 579, 'tall': 580, 'thinking': 581, 'tiredness': 582, 'traffic': 583, 'transportation': 584, 'west': 585, 'world': 586, 'accomplishment': 587, 'all': 588, 'angry': 589, 'antique': 590, 'appear': 591, 'areas': 592, 'arena': 593, 'arrested': 594, 'asia': 595, 'attention': 596, 'australia': 597, 'awake': 598, 'bankruptcy': 599, 'barn': 600, 'basket': 601, 'battlefield': 602, 'believe': 603, 'bread': 604, 'briefcase': 605, 'can': 606, 'card': 607, 'carry': 608, 'circus': 609, 'cleanliness': 610, 'clothing': 611, 'come': 612, 'competition': 613, 'concentrate': 614, 'confusion': 615, 'congress': 616, 'contact': 617, 'corn': 618, 'crime': 619, 'different': 620, 'direct': 621, 'dirty': 622, 'dishwasher': 623, 'dog': 624, 'drinking': 625, 'drop': 626, 'education': 627, 'encyclopedia': 628, 'enlightenment': 629, 'europe': 630, 'expensive': 631, 'fabric': 632, 'factory': 633, 'fair': 634, 'farm': 635, 'fat': 636, 'film': 637, 'find': 638, 'football': 639, 'gaining': 640, 'glass': 641, 'grade': 642, 'grass': 643, 'hang': 644, 'healthy': 645, 'heat': 646, 'heavy': 647, 'hunger': 648, 'interesting': 649, 'irritation': 650, 'jersey': 651, 'jewelry': 652, 'keep': 653, 'kit': 654, 'laughter': 655, 'law': 656, 'lay': 657, 'less': 658, 'letter': 659, 'liquid': 660, 'listening': 661, 'locker': 662, 'magazine': 663, 'maine': 664, 'math': 665, 'meat': 666, 'mind': 667, 'motion': 668, 'much': 669, 'ohio': 670, 'order': 671, 'oven': 672, 'painting': 673, 'parking': 674, 'pharmacy': 675, 'phone': 676, 'piano': 677, 'pick': 678, 'plant': 679, 'practice': 680, 'pressure': 681, 'progress': 682, 'property': 683, 'receive': 684, 'remorse': 685, 'roadblock': 686, 'rome': 687, 'sadness': 688, 'salon': 689, 'save': 690, 'serve': 691, 'shake': 692, 'shed': 693, 'shock': 694, 'shower': 695, 'sink': 696, 'slow': 697, 'soap': 698, 'social': 699, 'soft': 700, 'sore': 701, 'soup': 702, 'special': 703, 'spending': 704, 'strong': 705, 'suburbs': 706, 'suffer': 707, 'suit': 708, 'summer': 709, 'sunny': 710, 'surf': 711, 'tale': 712, 'tennessee': 713, 'throw': 714, 'ticket': 715, 'trash': 716, 'understanding': 717, 'utah': 718, 'victory': 719, 'video': 720, 'virginia': 721, 'voice': 722, 'vomiting': 723, 'waterfall': 724, 'were': 725, 'wine': 726, 'winning': 727, 'wrong': 728, 'able': 729, 'accident': 730, 'age': 731, 'ahead': 732, 'amount': 733, 'animal': 734, 'another': 735, 'answer': 736, 'anything': 737, 'anywhere': 738, 'around': 739, 'arrive': 740, 'attempt': 741, 'basin': 742, 'battle': 743, 'bay': 744, 'birth': 745, 'booth': 746, 'bottle': 747, 'bowl': 748, 'brain': 749, 'bright': 750, 'cage': 751, 'candy': 752, 'cars': 753, 'casino': 754, 'castle': 755, 'cemetary': 756, 'centre': 757, 'chair': 758, 'chemistry': 759, 'chicken': 760, 'close': 761, 'collection': 762, 'colorado': 763, 'commit': 764, 'conquer': 765, 'continent': 766, 'controversy': 767, 'cooler': 768, 'crash': 769, 'day': 770, 'desert': 771, 'destroy': 772, 'dining': 773, 'discomfort': 774, 'distress': 775, 'doors': 776, 'dress': 777, 'drive': 778, 'eating': 779, 'electric': 780, 'embarrassment': 781, 'emergency': 782, 'emotion': 783, 'engine': 784, 'excited': 785, 'exercise': 786, 'facts': 787, 'florida': 788, 'follow': 789, 'forgetfulness': 790, 'francisco': 791, 'gardens': 792, 'golf': 793, 'government': 794, 'green': 795, 'group': 796, 'gymnasium': 797, 'hallway': 798, 'heartburn': 799, 'higher': 800, 'hot': 801, 'ice': 802, 'illness': 803, 'important': 804, 'imprisoned': 805, 'increase': 806, 'instrument': 807, 'intelligence': 808, 'interest': 809, 'into': 810, 'japan': 811, 'jealousy': 812, 'job': 813, 'know': 814, 'laundromat': 815, 'lawn': 816, 'legal': 817, 'linen': 818, 'lobby': 819, 'looking': 820, 'loud': 821, 'main': 822, 'map': 823, 'marry': 824, 'meadow': 825, 'mess': 826, 'mistakes': 827, 'mood': 828, 'nebraska': 829, 'nest': 830, 'net': 831, 'nightclub': 832, 'nightmares': 833, 'notebook': 834, 'nursery': 835, 'opponent': 836, 'palace': 837, 'part': 838, 'places': 839, 'playground': 840, 'pond': 841, 'populated': 842, 'pregnancy': 843, 'press': 844, 'prices': 845, 'put': 846, 'rate': 847, 'red': 848, 'regret': 849, 'relaxing': 850, 'report': 851, 'residential': 852, 'respect': 853, 'shape': 854, 'share': 855, 'shelter': 856, 'sleeping': 857, 'smiles': 858, 'smiling': 859, 'socialize': 860, 'society': 861, 'sound': 862, 'spare': 863, 'speed': 864, 'standing': 865, 'stories': 866, 'strength': 867, 'sunshine': 868, 'teach': 869, 'telephone': 870, 'tension': 871, 'there': 872, 'thin': 873, 'thought': 874, 'trial': 875, 'truck': 876, 'underground': 877, 'unpredictable': 878, 'using': 879, 'valley': 880, 'vegetable': 881, 'walk': 882, 'walking': 883, 'wash': 884, 'washing': 885, 'welcome': 886, 'western': 887, 'white': 888, 'wild': 889, 'wilderness': 890, 'wings': 891, 'words': 892, 'yellow': 893, 'accomplish': 894, 'act': 895, 'action': 896, 'adequate': 897, 'against': 898, 'aggravation': 899, 'aggression': 900, 'alive': 901, 'allergies': 902, 'amazon': 903, 'ancient': 904, 'answers': 905, 'arguments': 906, 'army': 907, 'arrest': 908, 'arthritis': 909, 'article': 910, 'atmosphere': 911, 'automobile': 912, 'ballpark': 913, 'barber': 914, 'basketball': 915, 'be': 916, 'beautiful': 917, 'bedside': 918, 'begin': 919, 'bills': 920, 'bleeding': 921, 'blue': 922, 'branch': 923, 'break': 924, 'breakfast': 925, 'british': 926, 'brother': 927, 'buildings': 928, 'burn': 929, 'bush': 930, 'buying': 931, 'calm': 932, 'cancer': 933, 'capital': 934, 'captivity': 935, 'cards': 936, 'carpal': 937, 'cash': 938, 'cast': 939, 'cat': 940, 'cave': 941, 'central': 942, 'chicago': 943, 'child': 944, 'china': 945, 'choice': 946, 'christmas': 947, 'clerk': 948, 'climate': 949, 'climb': 950, 'columbia': 951, 'comfort': 952, 'coming': 953, 'common': 954, 'communicate': 955, 'competitiveness': 956, 'compost': 957, 'conclusion': 958, 'confession': 959, 'conscience': 960, 'contempt': 961, 'contract': 962, 'conversation': 963, 'cook': 964, 'coop': 965, 'coral': 966, 'cough': 967, 'countries': 968, 'county': 969, 'cramps': 970, 'creation': 971, 'd.c': 972, 'dancing': 973, 'deck': 974, 'delicious': 975, 'deny': 976, 'deposit': 977, 'depressed': 978, 'dictionary': 979, 'diego': 980, 'disease': 981, 'disorientation': 982, 'district': 983, 'done': 984, 'doorway': 985, 'dressing': 986, 'driveway': 987, 'driving': 988, 'drunk': 989, 'drunkenness': 990, 'dump': 991, 'dying': 992, 'earn': 993, 'ears': 994, 'eastern': 995, 'economic': 996, 'effort': 997, 'egg': 998, 'email': 999, 'emotional': 1000, 'entertain': 1001, 'entertaining': 1002, 'erections': 1003, 'estate': 1004, 'exceptional': 1005, 'exchange': 1006, 'exhibits': 1007, 'exhilaration': 1008, 'expect': 1009, 'expectations': 1010, 'expense': 1011, 'experiences': 1012, 'faces': 1013, 'far': 1014, 'favorite': 1015, 'first': 1016, 'flat': 1017, 'foreign': 1018, 'forrest': 1019, 'found': 1020, 'france': 1021, 'fraternity': 1022, 'fresh': 1023, 'friendly': 1024, 'friendships': 1025, 'funeral': 1026, 'funny': 1027, 'garbage': 1028, 'germany': 1029, 'gifts': 1030, 'goodbye': 1031, 'graveyard': 1032, 'growth': 1033, 'guard': 1034, 'gulf': 1035, 'gym': 1036, 'habitat': 1037, 'hangover': 1038, 'heaven': 1039, 'hell': 1040, 'here': 1041, 'homework': 1042, 'horrible': 1043, 'hunt': 1044, 'hurry': 1045, 'idaho': 1046, 'ignore': 1047, 'imagination': 1048, 'impatience': 1049, 'intoxication': 1050, 'island': 1051, 'jazz': 1052, 'judge': 1053, 'jump': 1054, 'jungle': 1055, 'justice': 1056, 'karma': 1057, 'killing': 1058, 'king': 1059, 'known': 1060, 'labyrinth': 1061, 'laid': 1062, 'land': 1063, 'landfill': 1064, 'language': 1065, 'larger': 1066, 'las': 1067, 'laziness': 1068, 'left': 1069, 'leg': 1070, 'loft': 1071, 'longer': 1072, 'low': 1073, 'lunch': 1074, 'man': 1075, 'manipulate': 1076, 'mansion': 1077, 'mass': 1078, 'may': 1079, 'medical': 1080, 'meditate': 1081, 'meditation': 1082, 'minnesota': 1083, 'misunderstanding': 1084, 'misunderstandings': 1085, 'mountainous': 1086, 'movement': 1087, 'mulberry': 1088, 'musical': 1089, 'news': 1090, 'nodding': 1091, 'nose': 1092, 'notes': 1093, 'nursing': 1094, 'obesity': 1095, 'object': 1096, 'offices': 1097, 'older': 1098, 'opening': 1099, 'opinions': 1100, 'orbit': 1101, 'orchard': 1102, 'orgasm': 1103, 'others': 1104, 'oxygen': 1105, 'pages': 1106, 'patience': 1107, 'pennsylvania': 1108, 'perspiration': 1109, 'petting': 1110, 'picture': 1111, 'pictures': 1112, 'plain': 1113, 'plan': 1114, 'plane': 1115, 'planet': 1116, 'playing': 1117, 'police': 1118, 'polite': 1119, 'poor': 1120, 'power': 1121, 'pretty': 1122, 'printer': 1123, 'puzzle': 1124, 'quartet': 1125, 'quickly': 1126, 'quiet': 1127, 'rainforest': 1128, 'ranch': 1129, 'rates': 1130, 'reach': 1131, 'real': 1132, 'record': 1133, 'reef': 1134, 'refreshed': 1135, 'register': 1136, 'reject': 1137, 'relationship': 1138, 'research': 1139, 'resentment': 1140, 'resort': 1141, 'restaurants': 1142, 'retail': 1143, 'retribution': 1144, 'ride': 1145, 'rope': 1146, 'row': 1147, 'running': 1148, 'rush': 1149, 'salad': 1150, 'salt': 1151, 'sand': 1152, 'seattle': 1153, 'selling': 1154, 'sense': 1155, 'separate': 1156, 'service': 1157, 'settle': 1158, 'sewer': 1159, 'sexual': 1160, 'shadow': 1161, 'shirt': 1162, 'shoes': 1163, 'shops': 1164, 'shortness': 1165, 'sick': 1166, 'side': 1167, 'singing': 1168, 'single': 1169, 'sleepiness': 1170, 'smart': 1171, 'smell': 1172, 'southern': 1173, 'spain': 1174, 'spread': 1175, 'spring': 1176, 'stairs': 1177, 'stairwell': 1178, 'stall': 1179, 'start': 1180, 'stopping': 1181, 'straight': 1182, 'strain': 1183, 'stream': 1184, 'string': 1185, 'stumbling': 1186, 'stupid': 1187, 'stupidity': 1188, 'subdivision': 1189, 'submarine': 1190, 'suburb': 1191, 'suburbia': 1192, 'suicide': 1193, 'swim': 1194, 'switzerland': 1195, 'syndrome': 1196, 'tank': 1197, 'tell': 1198, 'temperature': 1199, 'terrible': 1200, 'test': 1201, 'tidal': 1202, 'too': 1203, 'treasure': 1204, 'tropics': 1205, 'trust': 1206, 'tunnel': 1207, 'tv': 1208, 'understood': 1209, 'underwater': 1210, 'upright': 1211, 'vegas': 1212, 'vegetables': 1213, 'village': 1214, 'waiting': 1215, 'wallet': 1216, 'want': 1217, 'wave': 1218, 'weakness': 1219, 'wealth': 1220, 'weather': 1221, 'web': 1222, 'wheel': 1223, 'window': 1224, 'winter': 1225, 'wonder': 1226, 'wooded': 1227, 'woods': 1228, 'workplace': 1229, 'written': 1230, 'years': 1231, \"'ll\": 1232, 'abandoned': 1233, 'above': 1234, 'accelerate': 1235, 'accidental': 1236, 'accomplishing': 1237, 'ache': 1238, 'acquire': 1239, 'acting': 1240, 'active': 1241, 'added': 1242, 'addiction': 1243, 'admiration': 1244, 'admire': 1245, 'advantageous': 1246, 'affection': 1247, 'afternoon': 1248, 'agree': 1249, 'agreement': 1250, 'aid': 1251, 'alcohol': 1252, 'alone': 1253, 'along': 1254, 'anguish': 1255, 'animals': 1256, 'animated': 1257, 'answering': 1258, 'ante': 1259, 'apart': 1260, 'applaud': 1261, 'appliance': 1262, 'apply': 1263, 'aquarium': 1264, 'arcade': 1265, 'architecture': 1266, 'arctic': 1267, 'argument': 1268, 'arkansas': 1269, 'arms': 1270, 'arrangement': 1271, 'arriving': 1272, 'articulate': 1273, 'artifacts': 1274, 'artificial': 1275, 'artist': 1276, 'asexually': 1277, 'ashtray': 1278, 'asthma': 1279, 'attend': 1280, 'attraction': 1281, 'attractive': 1282, 'authority': 1283, 'available': 1284, 'avoid': 1285, 'babies': 1286, 'backache': 1287, 'baking': 1288, 'ball': 1289, 'barbeque': 1290, 'baseball': 1291, 'bath': 1292, 'bathe': 1293, 'battleship': 1294, 'behavior': 1295, 'best': 1296, 'betrayal': 1297, 'bible': 1298, 'bicycle': 1299, 'bird': 1300, 'birthday': 1301, 'black': 1302, 'blaring': 1303, 'bliss': 1304, 'block': 1305, 'bloody': 1306, 'bloom': 1307, 'boardroom': 1308, 'bodies': 1309, 'botanical': 1310, 'bottom': 1311, 'boy': 1312, 'brass': 1313, 'breathlessness': 1314, 'broadcast': 1315, 'broom': 1316, 'burrow': 1317, 'busy': 1318, 'cafeteria': 1319, 'calluses': 1320, 'camp': 1321, 'candle': 1322, 'canon': 1323, 'canyon': 1324, 'cape': 1325, 'careful': 1326, 'cargo': 1327, 'carrier': 1328, 'cart': 1329, 'carton': 1330, 'carved': 1331, 'catalogue': 1332, 'catching': 1333, 'cathedral': 1334, 'cd': 1335, 'cease': 1336, 'ceiling': 1337, 'celebrating': 1338, 'ceremony': 1339, 'charge': 1340, 'charming': 1341, 'cheese': 1342, 'chinatown': 1343, 'civilization': 1344, 'clap': 1345, 'clock': 1346, 'closed': 1347, 'closing': 1348, 'cloudy': 1349, 'code': 1350, 'colors': 1351, 'coma': 1352, 'communicating': 1353, 'compare': 1354, 'comparison': 1355, 'compassion': 1356, 'composted': 1357, 'connecticut': 1358, 'constellation': 1359, 'construct': 1360, 'continue': 1361, 'convention': 1362, 'cooking': 1363, 'correct': 1364, 'costa': 1365, 'cover': 1366, 'credit': 1367, 'criminal': 1368, 'crops': 1369, 'crowd': 1370, 'crown': 1371, 'cry': 1372, 'crying': 1373, 'curiosity': 1374, 'cute': 1375, 'dairy': 1376, 'dakota': 1377, 'dead': 1378, 'deadly': 1379, 'deaths': 1380, 'deceive': 1381, 'deception': 1382, 'decisions': 1383, 'decomposition': 1384, 'defend': 1385, 'deflation': 1386, 'demand': 1387, 'dense': 1388, 'derby': 1389, 'design': 1390, 'desire': 1391, 'despair': 1392, 'destruction': 1393, 'determination': 1394, 'detestable': 1395, 'device': 1396, 'dexterity': 1397, 'disagree': 1398, 'disaster': 1399, 'discount': 1400, 'discovery': 1401, 'disregard': 1402, 'distance': 1403, 'ditch': 1404, 'diverse': 1405, 'doing': 1406, 'doze': 1407, 'draw': 1408, 'dreams': 1409, 'dresser': 1410, 'drugs': 1411, 'dwarf': 1412, 'easily': 1413, 'easy': 1414, 'eden': 1415, 'efficiency': 1416, 'eggs': 1417, 'egypt': 1418, 'elderly': 1419, 'elevator': 1420, 'embarassment': 1421, 'embrace': 1422, 'english': 1423, 'enjoyable': 1424, 'enjoyed': 1425, 'enough': 1426, 'enter': 1427, 'errors': 1428, 'euphoria': 1429, 'evil': 1430, 'exaggerate': 1431, 'exchanging': 1432, 'excrete': 1433, 'exertion': 1434, 'exist': 1435, 'expected': 1436, 'expression': 1437, 'face': 1438, 'fail': 1439, 'failure': 1440, 'fairytale': 1441, 'faithful': 1442, 'farmyard': 1443, 'fart': 1444, 'female': 1445, 'fields': 1446, 'fights': 1447, 'filing': 1448, 'finger': 1449, 'fired': 1450, 'firm': 1451, 'fitness': 1452, 'fitting': 1453, 'flatulence': 1454, 'flea': 1455, 'fleas': 1456, 'floral': 1457, 'flowing': 1458, 'foot': 1459, 'forbid': 1460, 'forget': 1461, 'forgetting': 1462, 'fort': 1463, 'forward': 1464, 'fragile': 1465, 'frame': 1466, 'freight': 1467, 'friendship': 1468, 'fright': 1469, 'frightened': 1470, 'frustrated': 1471, 'fullness': 1472, 'funding': 1473, 'galaxy': 1474, 'gap': 1475, 'gather': 1476, 'gathering': 1477, 'gear': 1478, 'generous': 1479, 'geometry': 1480, 'girl': 1481, 'glee': 1482, 'grades': 1483, 'graduate': 1484, 'grand': 1485, 'greenhouse': 1486, 'grief': 1487, 'guitar': 1488, 'gun': 1489, 'hallucination': 1490, 'happen': 1491, 'harm': 1492, 'harmful': 1493, 'harsh': 1494, 'hat': 1495, 'hate': 1496, 'heading': 1497, 'heard': 1498, 'helicopter': 1499, 'hello': 1500, 'helpful': 1501, 'herself': 1502, 'historical': 1503, 'hole': 1504, 'hollow': 1505, 'hollywood': 1506, 'holster': 1507, 'homely': 1508, 'homes': 1509, 'homeschool': 1510, 'honor': 1511, 'horseradish': 1512, 'horses': 1513, 'hostility': 1514, 'hour': 1515, 'humdrum': 1516, 'hunting': 1517, 'illinois': 1518, 'illiterate': 1519, 'images': 1520, 'improve': 1521, 'improved': 1522, 'income': 1523, 'incompetent': 1524, 'indigestion': 1525, 'industrial': 1526, 'inhale': 1527, 'injured': 1528, 'ink': 1529, 'innocence': 1530, 'insight': 1531, 'insights': 1532, 'insomnia': 1533, 'inspiring': 1534, 'instability': 1535, 'instructions': 1536, 'insubstantial': 1537, 'investigation': 1538, 'isolation': 1539, 'istanbul': 1540, 'italy': 1541, 'jeans': 1542, 'jerusalem': 1543, 'jewish': 1544, 'join': 1545, 'junkyard': 1546, 'k': 1547, 'kansas': 1548, 'lack': 1549, 'landscape': 1550, 'lap': 1551, 'last': 1552, 'latin': 1553, 'laughing': 1554, 'laundry': 1555, 'lawyer': 1556, 'lessons': 1557, 'lies': 1558, 'lights': 1559, 'liquor': 1560, 'little': 1561, 'lives': 1562, 'lodge': 1563, 'log': 1564, 'lonely': 1565, 'long': 1566, 'loose': 1567, 'loved': 1568, 'madagascar': 1569, 'magazines': 1570, 'major': 1571, 'males': 1572, 'mammals': 1573, 'mandatory': 1574, 'marine': 1575, 'maritime': 1576, 'marriage': 1577, 'married': 1578, 'mart': 1579, 'matter': 1580, 'mean': 1581, 'mediterranean': 1582, 'melt': 1583, 'memories': 1584, 'memory': 1585, 'men': 1586, 'meow': 1587, 'metro': 1588, 'miracles': 1589, 'misery': 1590, 'mississippi': 1591, 'mistrial': 1592, 'mix': 1593, 'monarchy': 1594, 'moon': 1595, 'morning': 1596, 'motorbike': 1597, 'motorboat': 1598, 'motorcycle': 1599, 'names': 1600, 'near': 1601, 'neatness': 1602, 'necessary': 1603, 'needle': 1604, 'netherlands': 1605, 'nevada': 1606, 'normal': 1607, 'novel': 1608, 'objects': 1609, 'oceans': 1610, 'offer': 1611, 'oil': 1612, 'oklahoma': 1613, 'online': 1614, 'oral': 1615, 'orders': 1616, 'ore': 1617, 'oregon': 1618, 'organization': 1619, 'orleans': 1620, 'outrage': 1621, 'overeating': 1622, 'overspending': 1623, 'paid': 1624, 'painful': 1625, 'palpitations': 1626, 'panel': 1627, 'parents': 1628, 'paris': 1629, 'past': 1630, 'patient': 1631, 'paying': 1632, 'payment': 1633, 'pencil': 1634, 'period': 1635, 'piggy': 1636, 'player': 1637, 'players': 1638, 'pleasant': 1639, 'please': 1640, 'plug': 1641, 'point': 1642, 'poisoning': 1643, 'poisonous': 1644, 'poker': 1645, 'pollution': 1646, 'popularity': 1647, 'port': 1648, 'position': 1649, 'possessing': 1650, 'potato': 1651, 'pregnant': 1652, 'prepared': 1653, 'present': 1654, 'preserve': 1655, 'pretend': 1656, 'procrastination': 1657, 'procreate': 1658, 'program': 1659, 'programs': 1660, 'promise': 1661, 'prosecution': 1662, 'provide': 1663, 'purchasing': 1664, 'putting': 1665, 'quarry': 1666, 'rainy': 1667, 'range': 1668, 'rapport': 1669, 'rare': 1670, 'realization': 1671, 'reason': 1672, 'recorded': 1673, 'recording': 1674, 'recycled': 1675, 'refineries': 1676, 'region': 1677, 'relaxed': 1678, 'release': 1679, 'religion': 1680, 'religious': 1681, 'remember': 1682, 'reproduction': 1683, 'resent': 1684, 'response': 1685, 'responsibility': 1686, 'restroom': 1687, 'retaliation': 1688, 'reverse': 1689, 'revolution': 1690, 'rhode': 1691, 'rhyme': 1692, 'rica': 1693, 'rice': 1694, 'rich': 1695, 'right': 1696, 'rink': 1697, 'riots': 1698, 'rivalry': 1699, 'roadsides': 1700, 'rocks': 1701, 'rocky': 1702, 'rod': 1703, 'round': 1704, 'russia': 1705, 'sad': 1706, 'satisfy': 1707, 'saving': 1708, 'schools': 1709, 'score': 1710, 'sears': 1711, 'seaside': 1712, 'secret': 1713, 'seeing': 1714, 'sell': 1715, 'sensible': 1716, 'serious': 1717, 'serving': 1718, 'settlement': 1719, 'severe': 1720, 'sewing': 1721, 'shame': 1722, 'sharing': 1723, 'shoe': 1724, 'shore': 1725, 'short': 1726, 'shout': 1727, 'sickness': 1728, 'sign': 1729, 'sincere': 1730, 'situations': 1731, 'skiing': 1732, 'skinny': 1733, 'skyscraper': 1734, 'slapped': 1735, 'slavery': 1736, 'sleepwalking': 1737, 'sleepy': 1738, 'slender': 1739, 'slim': 1740, 'slot': 1741, 'sneezing': 1742, 'snore': 1743, 'soda': 1744, 'softness': 1745, 'software': 1746, 'soil': 1747, 'solitude': 1748, 'solution': 1749, 'solve': 1750, 'son': 1751, 'song': 1752, 'songs': 1753, 'sores': 1754, 'soundly': 1755, 'sounds': 1756, 'southwest': 1757, 'sparse': 1758, 'speaking': 1759, 'speeding': 1760, 'spot': 1761, 'spouse': 1762, 'square': 1763, 'stable': 1764, 'staggering': 1765, 'static': 1766, 'staying': 1767, 'steam': 1768, 'step': 1769, 'stew': 1770, 'stock': 1771, 'stores': 1772, 'storybook': 1773, 'stretch': 1774, 'student': 1775, 'students': 1776, 'success': 1777, 'suddenly': 1778, 'superior': 1779, 'switch': 1780, 'talent': 1781, 'taste': 1782, 'tasty': 1783, 'team': 1784, 'tears': 1785, 'teeth': 1786, 'temperate': 1787, 'tennis': 1788, 'tensions': 1789, 'tent': 1790, 'terrarium': 1791, 'text': 1792, 'thank': 1793, 'then': 1794, 'thick': 1795, 'throwing': 1796, 'thumb': 1797, 'tickets': 1798, 'tomb': 1799, 'toolbox': 1800, 'tops': 1801, 'tornado': 1802, 'torso': 1803, 'tow': 1804, 'towns': 1805, 'toys': 1806, 'trade': 1807, 'trap': 1808, 'trashcan': 1809, 'treated': 1810, 'trees': 1811, 'trip': 1812, 'triple': 1813, 'try': 1814, 'trying': 1815, 'tube': 1816, 'turbine': 1817, 'unconventional': 1818, 'union': 1819, 'unique': 1820, 'unobservant': 1821, 'unpleasant': 1822, 'upset': 1823, 'urinate': 1824, 'used': 1825, 'useless': 1826, 'utility': 1827, 'vacation': 1828, 'vengeance': 1829, 'vessel': 1830, 'violence': 1831, 'visual': 1832, 'wage': 1833, 'wake': 1834, 'wakefulness': 1835, 'warmth': 1836, 'waste': 1837, 'wasted': 1838, 'waters': 1839, 'way': 1840, 'weak': 1841, 'weapons': 1842, 'wellness': 1843, 'whiskers': 1844, 'whisper': 1845, 'whitehouse': 1846, 'wimbledon': 1847, 'winch': 1848, 'windows': 1849, 'winery': 1850, 'wisconsin': 1851, 'wishing': 1852, 'wood': 1853, 'worthy': 1854, 'wyoming': 1855, 'zero': 1856, \"'m\": 1857, '1': 1858, '100': 1859, 'abandon': 1860, 'abdominal': 1861, 'ability': 1862, 'abroad': 1863, 'absolution': 1864, 'abundant': 1865, 'acceptance': 1866, 'account': 1867, 'aches': 1868, 'achromatic': 1869, 'activity': 1870, 'acute': 1871, 'adore': 1872, 'adrenaline': 1873, 'adult': 1874, 'adults': 1875, 'advance': 1876, 'adverse': 1877, 'advertisement': 1878, 'advisory': 1879, 'advocate': 1880, 'aeroplane': 1881, 'affluent': 1882, 'afghanistan': 1883, 'african': 1884, 'aged': 1885, 'agency': 1886, 'ages': 1887, 'agility': 1888, 'agitated': 1889, 'agitation': 1890, 'agony': 1891, 'agreeable': 1892, 'aground': 1893, 'aids': 1894, 'aim': 1895, 'airbase': 1896, 'airports': 1897, 'alabama': 1898, 'albums': 1899, 'alcatraz': 1900, 'alcoholism': 1901, 'alien': 1902, 'alienate': 1903, 'alimentary': 1904, 'almost': 1905, 'aloft': 1906, 'alpenstock': 1907, 'already': 1908, 'altruistic': 1909, 'am': 1910, 'amazement': 1911, 'ambulance': 1912, 'amnesia': 1913, 'amok': 1914, 'amsterdam': 1915, 'angeles': 1916, 'angled': 1917, 'ankle': 1918, 'annoy': 1919, 'annoyance': 1920, 'annoyed': 1921, 'anterior': 1922, 'anthology': 1923, 'antidote': 1924, 'antipathy': 1925, 'anxious': 1926, 'apathetic': 1927, 'apathy': 1928, 'apiary': 1929, 'apparatus': 1930, 'appeals': 1931, 'appearance': 1932, 'applause': 1933, 'apple': 1934, 'applied': 1935, 'appreciate': 1936, 'appreciated': 1937, 'appreciation': 1938, 'approval': 1939, 'arcane': 1940, 'architect': 1941, 'ardor': 1942, 'argentina': 1943, 'arithmetic': 1944, 'arizona': 1945, 'arlington': 1946, 'armed': 1947, 'armies': 1948, 'armor': 1949, 'armored': 1950, 'armpits': 1951, 'array': 1952, 'artillery': 1953, 'arts': 1954, 'ascending': 1955, 'ashamed': 1956, 'asking': 1957, 'aspect': 1958, 'assembly': 1959, 'assistance': 1960, 'atheism': 1961, 'atheist': 1962, 'attached': 1963, 'attack': 1964, 'attacked': 1965, 'attempting': 1966, 'aunt': 1967, 'aunty': 1968, 'austral': 1969, 'australian': 1970, 'authentic': 1971, 'avalanches': 1972, 'average': 1973, 'aviary': 1974, 'awaken': 1975, 'awards': 1976, 'awe': 1977, 'awesome': 1978, 'baby': 1979, 'backward': 1980, 'backwards': 1981, 'backyards': 1982, 'baggage': 1983, 'bags': 1984, 'bake': 1985, 'bakery': 1986, 'balance': 1987, 'balloon': 1988, 'bamboo': 1989, 'banjo': 1990, 'banking': 1991, 'banned': 1992, 'barbers': 1993, 'bark': 1994, 'barrel': 1995, 'barrier': 1996, 'basic': 1997, 'bass': 1998, 'bat': 1999, 'bate': 2000, 'bathed': 2001, 'bathing': 2002, 'beans': 2003, 'bearable': 2004, 'bearing': 2005, 'beast': 2006, 'beautifull': 2007, 'beauty': 2008, 'bedsores': 2009, 'bee': 2010, 'beef': 2011, 'beehive': 2012, 'before': 2013, 'beginning': 2014, 'behind': 2015, 'belief': 2016, 'believed': 2017, 'belittle': 2018, 'belittlement': 2019, 'belligerent': 2020, 'bells': 2021, 'belong': 2022, 'belts': 2023, 'beneficial': 2024, 'benefit': 2025, 'bermuda': 2026, 'berries': 2027, 'between': 2028, 'bigger': 2029, 'bill': 2030, 'bind': 2031, 'binder': 2032, 'binge': 2033, 'biology': 2034, 'bit': 2035, 'bite': 2036, 'bitter': 2037, 'bitterness': 2038, 'blacksmith': 2039, 'bladders': 2040, 'bland': 2041, 'blank': 2042, 'bless': 2043, 'blizzard': 2044, 'bloodshot': 2045, 'blooms': 2046, 'blossom': 2047, 'blotter': 2048, 'blow': 2049, 'blowing': 2050, 'blues': 2051, 'blunt': 2052, 'boats': 2053, 'boiling': 2054, 'bomb': 2055, 'bonding': 2056, 'boom': 2057, 'boot': 2058, 'booze': 2059, 'bore': 2060, 'born': 2061, 'boss': 2062, 'botanic': 2063, 'bought': 2064, 'bound': 2065, 'bouquet': 2066, 'boutique': 2067, 'boxes': 2068, 'brainless': 2069, 'brains': 2070, 'brainy': 2071, 'brassiere': 2072, 'brave': 2073, 'brazil': 2074, 'breaking': 2075, 'breaks': 2076, 'bribe': 2077, 'britain': 2078, 'broke': 2079, 'brook': 2080, 'bruise': 2081, 'brush': 2082, 'buffet': 2083, 'build': 2084, 'bulgaria': 2085, 'bundle': 2086, 'burger': 2087, 'burlap': 2088, 'burns': 2089, 'burp': 2090, 'bushes': 2091, 'businesses': 2092, 'butcher': 2093, 'butt': 2094, 'butterflies': 2095, 'button': 2096, 'buzz': 2097, 'ca': 2098, 'cab': 2099, 'cabin': 2100, 'cactus': 2101, 'cakewalk': 2102, 'calcium': 2103, 'calculus': 2104, 'calmness': 2105, 'cambodia': 2106, 'camera': 2107, 'canal': 2108, 'candies': 2109, 'canopy': 2110, 'canteen': 2111, 'canter': 2112, 'capillaries': 2113, 'capitol': 2114, 'capsule': 2115, 'carefree': 2116, 'carefully': 2117, 'caribbean': 2118, 'carpenter': 2119, 'carpeting': 2120, 'carriage': 2121, 'cartoon': 2122, 'cartridges': 2123, 'casing': 2124, 'cassettes': 2125, 'cattle': 2126, 'caught': 2127, 'causing': 2128, 'cedar': 2129, 'celebration': 2130, 'cell': 2131, 'cells': 2132, 'challenge': 2133, 'challenged': 2134, 'changing': 2135, 'chaos': 2136, 'charges': 2137, 'cheap': 2138, 'cheat': 2139, 'checker': 2140, 'checkers': 2141, 'checks': 2142, 'cheers': 2143, 'chemise': 2144, 'chemist': 2145, 'chemotherapy': 2146, 'cherish': 2147, 'cherry': 2148, 'chesapeake': 2149, 'chewing': 2150, 'chills': 2151, 'chinese': 2152, 'choir': 2153, 'choke': 2154, 'choking': 2155, 'choose': 2156, 'chopped': 2157, 'chuck': 2158, 'churchyard': 2159, 'cigarettes': 2160, 'cinema': 2161, 'circular': 2162, 'circumstances': 2163, 'civil': 2164, 'civilisation': 2165, 'claim': 2166, 'clam': 2167, 'classical': 2168, 'clay': 2169, 'cleaning': 2170, 'clearly': 2171, 'cleverest': 2172, 'clients': 2173, 'cliff': 2174, 'climates': 2175, 'clinic': 2176, 'closeness': 2177, 'closer': 2178, 'cloud': 2179, 'cloy': 2180, 'cluster': 2181, 'clutter': 2182, 'coastal': 2183, 'coaster': 2184, 'coat': 2185, 'cockpit': 2186, 'cocktail': 2187, 'cod': 2188, 'coffeepot': 2189, 'coffin': 2190, 'coin': 2191, 'coldness': 2192, 'collaboration': 2193, 'collapse': 2194, 'colloquial': 2195, 'color': 2196, 'coloring': 2197, 'colours': 2198, 'comb': 2199, 'combustion': 2200, 'comfortably': 2201, 'comforted': 2202, 'comic': 2203, 'communism': 2204, 'companionable': 2205, 'compartment': 2206, 'compensation': 2207, 'competence': 2208, 'competency': 2209, 'competent': 2210, 'competitive': 2211, 'compile': 2212, 'complaining': 2213, 'complemented': 2214, 'complications': 2215, 'complimentary': 2216, 'compliments': 2217, 'composed': 2218, 'comprehension': 2219, 'compromises': 2220, 'compulsive': 2221, 'concentrating': 2222, 'conclave': 2223, 'concordance': 2224, 'conflicts': 2225, 'conformity': 2226, 'confuse': 2227, 'confusing': 2228, 'congratulated': 2229, 'conjunction': 2230, 'conscious': 2231, 'consciousness': 2232, 'conservatory': 2233, 'considerable': 2234, 'consideration': 2235, 'constant': 2236, 'constantly': 2237, 'consumed': 2238, 'consuming': 2239, 'consumption': 2240, 'contacts': 2241, 'contagious': 2242, 'contaminated': 2243, 'contentment': 2244, 'contrast': 2245, 'contribution': 2246, 'contumely': 2247, 'convenient': 2248, 'conventional': 2249, 'converse': 2250, 'cooked': 2251, 'cooling': 2252, 'cooties': 2253, 'copulate': 2254, 'copy': 2255, 'cordoba': 2256, 'cornfield': 2257, 'corps': 2258, 'corral': 2259, 'correctly': 2260, 'corvette': 2261, 'costly': 2262, 'cotton': 2263, 'coughing': 2264, 'could': 2265, 'council': 2266, 'count': 2267, 'couple': 2268, 'courtroom': 2269, 'courtyard': 2270, 'coverage': 2271, 'covers': 2272, 'cramp': 2273, 'crawl': 2274, 'crazy': 2275, 'cream': 2276, 'creatively': 2277, 'creativity': 2278, 'creek': 2279, 'creeks': 2280, 'cremated': 2281, 'crimes': 2282, 'crisp': 2283, 'critically': 2284, 'criticism': 2285, 'crossword': 2286, 'crowding': 2287, 'crucial': 2288, 'cruel': 2289, 'cruet': 2290, 'cruise': 2291, 'crumpled': 2292, 'crushed': 2293, 'crystals': 2294, 'cub': 2295, 'cuba': 2296, 'cube': 2297, 'cubes': 2298, 'cubicle': 2299, 'cultural': 2300, 'cum': 2301, 'cumbersome': 2302, 'cure': 2303, 'current': 2304, 'curry': 2305, 'curved': 2306, 'customers': 2307, 'cut': 2308, 'cuts': 2309, 'cyberspace': 2310, 'dale': 2311, 'dam': 2312, 'damage': 2313, 'damaged': 2314, 'damnation': 2315, 'danger': 2316, 'dark': 2317, 'darkness': 2318, 'dashboard': 2319, 'dauntless': 2320, 'dawn': 2321, 'daydreaming': 2322, 'days': 2323, 'daytime': 2324, 'dc': 2325, 'de': 2326, 'deaf': 2327, 'deafness': 2328, 'dealership': 2329, 'debt': 2330, 'decay': 2331, 'deceitful': 2332, 'deceptive': 2333, 'deciding': 2334, 'decision': 2335, 'decorative': 2336, 'dedication': 2337, 'deeply': 2338, 'defeat': 2339, 'defects': 2340, 'definitely': 2341, 'delaware': 2342, 'delay': 2343, 'delays': 2344, 'deli': 2345, 'delta': 2346, 'demands': 2347, 'demean': 2348, 'democracy': 2349, 'democratic': 2350, 'demotion': 2351, 'den': 2352, 'denmark': 2353, 'dental': 2354, 'dependence': 2355, 'derivative': 2356, 'descend': 2357, 'description': 2358, 'deserts': 2359, 'desination': 2360, 'desirable': 2361, 'destroying': 2362, 'detachment': 2363, 'details': 2364, 'detention': 2365, 'developed': 2366, 'devices': 2367, 'devil': 2368, 'diaper': 2369, 'diarrhea': 2370, 'diary': 2371, 'dictatorship': 2372, 'did': 2373, 'dies': 2374, 'differently': 2375, 'digestion': 2376, 'dim': 2377, 'diminish': 2378, 'diminishment': 2379, 'dimly': 2380, 'diner': 2381, 'direction': 2382, 'dirt': 2383, 'disagreements': 2384, 'disappointed': 2385, 'disappointment': 2386, 'disapproval': 2387, 'disastrous': 2388, 'discipline': 2389, 'discover': 2390, 'discovering': 2391, 'discreet': 2392, 'discussion': 2393, 'disgust': 2394, 'dish': 2395, 'dishearten': 2396, 'disinterest': 2397, 'disobey': 2398, 'disparagement': 2399, 'dispense': 2400, 'dispenser': 2401, 'disperse': 2402, 'display': 2403, 'disposed': 2404, 'dispute': 2405, 'disputes': 2406, 'disruption': 2407, 'diss': 2408, 'dissatisfaction': 2409, 'dissipate': 2410, 'distances': 2411, 'distinguish': 2412, 'distracting': 2413, 'distraction': 2414, 'distraught': 2415, 'distributed': 2416, 'divided': 2417, 'divorce': 2418, 'divorced': 2419, 'dock': 2420, 'documentary': 2421, 'domestic': 2422, 'dominated': 2423, 'donut': 2424, 'donuts': 2425, 'dorm': 2426, 'dormitory': 2427, 'doubt': 2428, 'drawing': 2429, 'dreaming': 2430, 'drenching': 2431, 'dribble': 2432, 'drill': 2433, 'drinks': 2434, 'dropping': 2435, 'drown': 2436, 'drowsiness': 2437, 'drum': 2438, 'dryer': 2439, 'dude': 2440, 'dull': 2441, 'dumpster': 2442, 'duty': 2443, 'dweller': 2444, 'dwelling': 2445, 'earning': 2446, 'earnings': 2447, 'easter': 2448, 'eaten': 2449, 'ecosphere': 2450, 'edge': 2451, 'edible': 2452, 'edinburgh': 2453, 'educated': 2454, 'educational': 2455, 'edward': 2456, 'effectively': 2457, 'effectiveness': 2458, 'elapsed': 2459, 'elation': 2460, 'elbow': 2461, 'election': 2462, 'elections': 2463, 'elective': 2464, 'electrical': 2465, 'electricity': 2466, 'electronic': 2467, 'embarrassed': 2468, 'emblem': 2469, 'emit': 2470, 'emotions': 2471, 'empathy': 2472, 'employability': 2473, 'employee': 2474, 'employees': 2475, 'employment': 2476, 'empowering': 2477, 'empowerment': 2478, 'encourage': 2479, 'encouraging': 2480, 'endanger': 2481, 'enema': 2482, 'engage': 2483, 'engineering': 2484, 'engines': 2485, 'engraving': 2486, 'enjoying': 2487, 'enthusiastic': 2488, 'entrance': 2489, 'envelope': 2490, 'equal': 2491, 'equation': 2492, 'erase': 2493, 'erect': 2494, 'escape': 2495, 'eternal': 2496, 'european': 2497, 'evaluate': 2498, 'evaluating': 2499, 'even': 2500, 'evening': 2501, 'events': 2502, 'everglades': 2503, 'everywhere': 2504, 'evidence': 2505, 'examples': 2506, 'excavations': 2507, 'exception': 2508, 'exhaust': 2509, 'exhausted': 2510, 'existing': 2511, 'exists': 2512, 'exit': 2513, 'exotic': 2514, 'expectation': 2515, 'expend': 2516, 'experiment': 2517, 'expiration': 2518, 'explosion': 2519, 'exposure': 2520, 'expressive': 2521, 'extra': 2522, 'facing': 2523, 'fainting': 2524, 'fairness': 2525, 'faith': 2526, 'families': 2527, 'farmers': 2528, 'farming': 2529, 'farmland': 2530, 'farts': 2531, 'fashion': 2532, 'father': 2533, 'favourite': 2534, 'fearful': 2535, 'fearless': 2536, 'feather': 2537, 'feathers': 2538, 'feces': 2539, 'fed': 2540, 'feeble': 2541, 'feels': 2542, 'fees': 2543, 'feild': 2544, 'feline': 2545, 'fence': 2546, 'festival': 2547, 'fever': 2548, 'few': 2549, 'fiction': 2550, 'fiddle': 2551, 'fidelity': 2552, 'fidgeting': 2553, 'fieldhouse': 2554, 'fifties': 2555, 'fighting': 2556, 'figure': 2557, 'file': 2558, 'files': 2559, 'fill': 2560, 'filled': 2561, 'financial': 2562, 'fingernails': 2563, 'fingers': 2564, 'finish': 2565, 'finite': 2566, 'firearm': 2567, 'fireworks': 2568, 'fists': 2569, 'fitter': 2570, 'flashlight': 2571, 'flats': 2572, 'flesh': 2573, 'flight': 2574, 'flirt': 2575, 'floors': 2576, 'flour': 2577, 'flowerpot': 2578, 'fluidity': 2579, 'flush': 2580, 'flustered': 2581, 'folder': 2582, 'force': 2583, 'forceful': 2584, 'forces': 2585, 'foreigner': 2586, 'forests': 2587, 'forgive': 2588, 'fork': 2589, 'formal': 2590, 'formed': 2591, 'forms': 2592, 'forth': 2593, 'foundation': 2594, 'freezing': 2595, 'frequent': 2596, 'fresher': 2597, 'freshwater': 2598, 'fries': 2599, 'frigate': 2600, 'frightening': 2601, 'frivolous': 2602, 'frown': 2603, 'frying': 2604, 'fucking': 2605, 'fuel': 2606, 'fulfilled': 2607, 'fulfilling': 2608, 'fulfillment': 2609, 'fundamental': 2610, 'fur': 2611, 'fuse': 2612, 'gallbladder': 2613, 'garder': 2614, 'garish': 2615, 'garments': 2616, 'gate': 2617, 'geek': 2618, 'generation': 2619, 'geography': 2620, 'ghetto': 2621, 'giggling': 2622, 'gilded': 2623, 'gills': 2624, 'globular': 2625, 'glove': 2626, 'goals': 2627, 'godless': 2628, 'gods': 2629, 'gorge': 2630, 'gorgeous': 2631, 'got': 2632, 'grandma': 2633, 'grandmother': 2634, 'graph': 2635, 'grassy': 2636, 'gratification': 2637, 'gratifying': 2638, 'gratitude': 2639, 'gravesite': 2640, 'gravity': 2641, 'greater': 2642, 'greece': 2643, 'greedy': 2644, 'greek': 2645, 'greet': 2646, 'greeting': 2647, 'grieve': 2648, 'groceries': 2649, 'grotesque': 2650, 'grove': 2651, 'growing': 2652, 'grown': 2653, 'guest': 2654, 'guests': 2655, 'guillotine': 2656, 'gum': 2657, 'habitual': 2658, 'had': 2659, 'haircut': 2660, 'ham': 2661, 'hamburger': 2662, 'hamper': 2663, 'handbag': 2664, 'hanger': 2665, 'hanging': 2666, 'hangovers': 2667, 'happened': 2668, 'harbor': 2669, 'hardship': 2670, 'harmed': 2671, 'harmony': 2672, 'harvesting': 2673, 'haste': 2674, 'hats': 2675, 'haunted': 2676, 'hawaiian': 2677, 'haystack': 2678, 'heal': 2679, 'healthier': 2680, 'healthiness': 2681, 'hearing': 2682, 'heartache': 2683, 'heavens': 2684, 'hedgerow': 2685, 'heights': 2686, 'hells': 2687, 'helped': 2688, 'hen': 2689, 'henhouse': 2690, 'herb': 2691, 'herd': 2692, 'heretical': 2693, 'herpes': 2694, 'hesitation': 2695, 'hi': 2696, 'hiccups': 2697, 'hide': 2698, 'hideous': 2699, 'hiding': 2700, 'highway': 2701, 'hiking': 2702, 'hill': 2703, 'hinder': 2704, 'hired': 2705, 'hive': 2706, 'hoarse': 2707, 'hock': 2708, 'holding': 2709, 'hollowness': 2710, 'homeless': 2711, 'honest': 2712, 'hop': 2713, 'hope': 2714, 'horrendous': 2715, 'hospitals': 2716, 'hotels': 2717, 'houseplant': 2718, 'how': 2719, 'hug': 2720, 'humane': 2721, 'humans': 2722, 'humidifier': 2723, 'humiliated': 2724, 'humiliation': 2725, 'humor': 2726, 'hundredweight': 2727, 'hurtful': 2728, 'hurting': 2729, 'hydration': 2730, 'hydroelectric': 2731, 'hyperbolic': 2732, 'hyperventilation': 2733, 'hysteria': 2734, 'hysterical': 2735, 'i': 2736, 'i.d': 2737, 'identification': 2738, 'idols': 2739, 'ignition': 2740, 'ignoble': 2741, 'ignorable': 2742, 'ignorance': 2743, 'ignorant': 2744, 'ignoring': 2745, 'ill': 2746, 'illuminate': 2747, 'illumination': 2748, 'illustrate': 2749, 'imagine': 2750, 'imagining': 2751, 'immune': 2752, 'imperfect': 2753, 'impossibility': 2754, 'imprisonment': 2755, 'impudence': 2756, 'inaccurate': 2757, 'inappropriate': 2758, 'incarceration': 2759, 'incinerator': 2760, 'include': 2761, 'incomplete': 2762, 'incorrect': 2763, 'increasing': 2764, 'indeterminate': 2765, 'india': 2766, 'indian': 2767, 'indiana': 2768, 'indifference': 2769, 'indifferent': 2770, 'individual': 2771, 'indoor': 2772, 'industrialized': 2773, 'inebriation': 2774, 'ineffectual': 2775, 'inelegant': 2776, 'infect': 2777, 'infected': 2778, 'infinite': 2779, 'inform': 2780, 'informal': 2781, 'inhaling': 2782, 'injustices': 2783, 'input': 2784, 'inside': 2785, 'insignificant': 2786, 'inspired': 2787, 'inspires': 2788, 'instruments': 2789, 'intake': 2790, 'intangible': 2791, 'integrate': 2792, 'intellectual': 2793, 'intelligent': 2794, 'interaction': 2795, 'interested': 2796, 'interests': 2797, 'intermission': 2798, 'international': 2799, 'intimacy': 2800, 'introduce': 2801, 'introduced': 2802, 'introducing': 2803, 'invent': 2804, 'invigorating': 2805, 'invisible': 2806, 'iowa': 2807, 'ipod': 2808, 'ireland': 2809, 'irish': 2810, 'irreverence': 2811, 'irritability': 2812, 'irritated': 2813, 'italian': 2814, 'itches': 2815, 'itself': 2816, 'jamb': 2817, 'jams': 2818, 'janeiro': 2819, 'japanese': 2820, 'jerseys': 2821, 'jet': 2822, 'jewellers': 2823, 'jigsaw': 2824, 'jog': 2825, 'joining': 2826, 'joke': 2827, 'joyful': 2828, 'judgement': 2829, 'juice': 2830, 'junk': 2831, 'jury': 2832, 'just': 2833, 'kalahari': 2834, 'keg': 2835, 'kenne': 2836, 'kennedy': 2837, 'kew': 2838, 'key': 2839, 'keys': 2840, 'killed': 2841, 'kind': 2842, 'kindergarten': 2843, 'kindness': 2844, 'kinds': 2845, 'kingdom': 2846, 'kisses': 2847, 'kissing': 2848, 'kitchens': 2849, 'knee': 2850, 'kneel': 2851, 'knife': 2852, 'knit': 2853, 'knocking': 2854, 'knowing': 2855, 'knowingly': 2856, 'knowledgable': 2857, 'knowlege': 2858, 'knox': 2859, 'la': 2860, 'labor': 2861, 'lacerations': 2862, 'lady': 2863, 'lag': 2864, 'lamb': 2865, 'lame': 2866, 'lamp': 2867, 'landmine': 2868, 'landowner': 2869, 'lands': 2870, 'landscaping': 2871, 'lane': 2872, 'lark': 2873, 'latency': 2874, 'laughed': 2875, 'lawbook': 2876, 'laws': 2877, 'lazy': 2878, 'lead': 2879, 'leadership': 2880, 'leak': 2881, 'leash': 2882, 'leisure': 2883, 'lend': 2884, 'lens': 2885, 'letters': 2886, 'level': 2887, 'liberal': 2888, 'lighting': 2889, 'lightly': 2890, 'lightning': 2891, 'lightweight': 2892, 'liked': 2893, 'lines': 2894, 'liquidated': 2895, 'lists': 2896, 'lit': 2897, 'literate': 2898, 'literature': 2899, 'loading': 2900, 'loaf': 2901, 'lobster': 2902, 'location': 2903, 'lock': 2904, 'locked': 2905, 'locomotion': 2906, 'logging': 2907, 'logic': 2908, 'longing': 2909, 'loosely': 2910, 'loosing': 2911, 'loquacious': 2912, 'los': 2913, 'losers': 2914, 'lost': 2915, 'lotion': 2916, 'lover': 2917, 'loving': 2918, 'lowering': 2919, 'loyal': 2920, 'lubricate': 2921, 'lucky': 2922, 'lumberyard': 2923, 'lung': 2924, 'lute': 2925, 'machines': 2926, 'mad': 2927, 'made': 2928, 'maintain': 2929, 'malaise': 2930, 'male': 2931, 'malls': 2932, 'managed': 2933, 'manual': 2934, 'maps': 2935, 'mark': 2936, 'masturbation': 2937, 'match': 2938, 'mate': 2939, 'material': 2940, 'maternity': 2941, 'maze': 2942, 'meal': 2943, 'meals': 2944, 'measureing': 2945, 'melting': 2946, 'memorize': 2947, 'mentally': 2948, 'menu': 2949, 'met': 2950, 'metal': 2951, 'metaphor': 2952, 'meter': 2953, 'metric': 2954, 'metropolis': 2955, 'microphone': 2956, 'milk': 2957, 'milky': 2958, 'millions': 2959, 'millpond': 2960, 'milwaukee': 2961, 'mining': 2962, 'minor': 2963, 'misfire': 2964, 'misfortune': 2965, 'misrepresent': 2966, 'misshapen': 2967, 'missile': 2968, 'missing': 2969, 'missouri': 2970, 'mistrust': 2971, 'misunderstood': 2972, 'mit': 2973, 'mixed': 2974, 'mixer': 2975, 'mobile': 2976, 'mobility': 2977, 'modern': 2978, 'molehill': 2979, 'momentum': 2980, 'monastic': 2981, 'monitor': 2982, 'monochromatic': 2983, 'montreal': 2984, 'mop': 2985, 'moral': 2986, 'morgue': 2987, 'mortal': 2988, 'mortuary': 2989, 'mother': 2990, 'motherboard': 2991, 'motor': 2992, 'mouse': 2993, 'moustache': 2994, 'moved': 2995, 'mower': 2996, 'multiple': 2997, 'multiplied': 2998, 'multiply': 2999, 'murder': 3000, 'murdered': 3001, 'muscle': 3002, 'mute': 3003, 'muttering': 3004, 'nailed': 3005, 'naked': 3006, 'name': 3007, 'nanotechnology': 3008, 'nap': 3009, 'napster': 3010, 'nasa': 3011, 'native': 3012, 'nearest': 3013, 'neat': 3014, 'neck': 3015, 'negative': 3016, 'neglect': 3017, 'negligence': 3018, 'negligible': 3019, 'nepal': 3020, 'network': 3021, 'networked': 3022, 'neutral': 3023, 'never': 3024, 'niece': 3025, 'nightgown': 3026, 'noises': 3027, 'noisy': 3028, 'nonprofit': 3029, 'nonsense': 3030, 'nonstandard': 3031, 'northeast': 3032, 'northwest': 3033, 'nostalgia': 3034, 'notice': 3035, 'noticeable': 3036, 'notoriety': 3037, 'nt': 3038, 'nuclear': 3039, 'nudity': 3040, 'number': 3041, 'nurse': 3042, 'nutrient': 3043, 'nuts': 3044, 'oath': 3045, 'obligatory': 3046, 'obscure': 3047, 'obsessive': 3048, 'obsolete': 3049, 'obstruct': 3050, 'obtaining': 3051, 'occasionally': 3052, 'odors': 3053, 'ontario': 3054, 'opaque': 3055, 'openness': 3056, 'operating': 3057, 'operation': 3058, 'opinion': 3059, 'opportunity': 3060, 'optical': 3061, 'ordered': 3062, 'organ': 3063, 'organism': 3064, 'original': 3065, 'orphan': 3066, 'orphanage': 3067, 'oscilloscope': 3068, 'ostracism': 3069, 'otherwise': 3070, 'outback': 3071, 'outcry': 3072, 'outer': 3073, 'outsides': 3074, 'overconfidence': 3075, 'overpopulation': 3076, 'overweight': 3077, 'owning': 3078, 'pack': 3079, 'packaged': 3080, 'paddy': 3081, 'page': 3082, 'painless': 3083, 'paint': 3084, 'pair': 3085, 'palm': 3086, 'pan': 3087, 'pants': 3088, 'parade': 3089, 'paragraph': 3090, 'parcel': 3091, 'parent': 3092, 'parliament': 3093, 'parlor': 3094, 'partial': 3095, 'parties': 3096, 'partner': 3097, 'parts': 3098, 'passenger': 3099, 'passing': 3100, 'passionate': 3101, 'paste': 3102, 'pasture': 3103, 'patch': 3104, 'patiently': 3105, 'paws': 3106, 'payments': 3107, 'pc': 3108, 'peaceful': 3109, 'peacefulness': 3110, 'peck': 3111, 'peculiar': 3112, 'pedaling': 3113, 'pee': 3114, 'peel': 3115, 'perennial': 3116, 'perfect': 3117, 'perform': 3118, 'perfume': 3119, 'perfumery': 3120, 'peripheral': 3121, 'permanent': 3122, 'permitted': 3123, 'persistent': 3124, 'personal': 3125, 'personality': 3126, 'perspectives': 3127, 'persuaded': 3128, 'petals': 3129, 'petrify': 3130, 'pets': 3131, 'petty': 3132, 'philosophy': 3133, 'phoenix': 3134, 'phonebook': 3135, 'phoning': 3136, 'photo': 3137, 'photograph': 3138, 'photosynthesis': 3139, 'picking': 3140, 'picnic': 3141, 'pillow': 3142, 'pills': 3143, 'pilot': 3144, 'pine': 3145, 'pink': 3146, 'pipe': 3147, 'pirate': 3148, 'plains': 3149, 'planetarium': 3150, 'plantation': 3151, 'planting': 3152, 'plastic': 3153, 'plasticulture': 3154, 'playful': 3155, 'playfulness': 3156, 'playroom': 3157, 'pleasing': 3158, 'pleasurable': 3159, 'plebeian': 3160, 'pleistocene': 3161, 'plentitude': 3162, 'plethora': 3163, 'plumbing': 3164, 'pockets': 3165, 'poems': 3166, 'pointed': 3167, 'points': 3168, 'poland': 3169, 'pole': 3170, 'polished': 3171, 'poop': 3172, 'pop': 3173, 'popcorn': 3174, 'popular': 3175, 'pornography': 3176, 'posession': 3177, 'positive': 3178, 'possibility': 3179, 'potence': 3180, 'potential': 3181, 'pots': 3182, 'pouch': 3183, 'poverty': 3184, 'practical': 3185, 'praising': 3186, 'precious': 3187, 'predetermination': 3188, 'predictable': 3189, 'prehistoric': 3190, 'prehistory': 3191, 'prejudice': 3192, 'preschool': 3193, 'presence': 3194, 'presentation': 3195, 'presents': 3196, 'president': 3197, 'pressing': 3198, 'price': 3199, 'prime': 3200, 'prince': 3201, 'printed': 3202, 'prior': 3203, 'prisoner': 3204, 'processed': 3205, 'products': 3206, 'profane': 3207, 'profit': 3208, 'project': 3209, 'promotion': 3210, 'promotions': 3211, 'prompt': 3212, 'pronoun': 3213, 'propeller': 3214, 'propitious': 3215, 'proposals': 3216, 'prostitute': 3217, 'protection': 3218, 'protein': 3219, 'protests': 3220, 'proud': 3221, 'province': 3222, 'proximity': 3223, 'pta': 3224, 'pub': 3225, 'pull': 3226, 'pump': 3227, 'punch': 3228, 'punctured': 3229, 'purchase': 3230, 'purpose': 3231, 'purr': 3232, 'push': 3233, 'qualification': 3234, 'quark': 3235, 'quebec': 3236, 'queensland': 3237, 'queue': 3238, 'quietness': 3239, 'quintuplets': 3240, 'quitting': 3241, 'rabbit': 3242, 'racetrack': 3243, 'rack': 3244, 'racket': 3245, 'radical': 3246, 'railway': 3247, 'rainbow': 3248, 'randomness': 3249, 'rattle': 3250, 'raw': 3251, 'razor': 3252, 'reaction': 3253, 'reading': 3254, 'ready': 3255, 'reality': 3256, 'ream': 3257, 'reasonable': 3258, 'rebel': 3259, 'rebirth': 3260, 'rebound': 3261, 'receiving': 3262, 'receptionist': 3263, 'recession': 3264, 'reckless': 3265, 'recognition': 3266, 'reconciled': 3267, 'recorder': 3268, 'recovery': 3269, 'recreational': 3270, 'recur': 3271, 'recyclable': 3272, 'recycling': 3273, 'redemption': 3274, 'reduce': 3275, 'reduced': 3276, 'reflecting': 3277, 'reflection': 3278, 'refreshment': 3279, 'refrigerators': 3280, 'refuge': 3281, 'refund': 3282, 'refuse': 3283, 'regressive': 3284, 'regular': 3285, 'rejected': 3286, 'rejection': 3287, 'relatives': 3288, 'relieved': 3289, 'religiosity': 3290, 'reluctance': 3291, 'remake': 3292, 'remarkable': 3293, 'remote': 3294, 'renewal': 3295, 'repeat': 3296, 'representative': 3297, 'reproducing': 3298, 'republican': 3299, 'reputable': 3300, 'require': 3301, 'resentful': 3302, 'reservations': 3303, 'reserve': 3304, 'residence': 3305, 'resist': 3306, 'resistant': 3307, 'resources': 3308, 'respiration': 3309, 'respond': 3310, 'responsibilities': 3311, 'restitution': 3312, 'restlessness': 3313, 'restrained': 3314, 'restricted': 3315, 'result': 3316, 'results': 3317, 'resuscitate': 3318, 'retina': 3319, 'retire': 3320, 'retirement': 3321, 'retract': 3322, 'return': 3323, 'reunion': 3324, 'revelation': 3325, 'revelations': 3326, 'revenge': 3327, 'reverence': 3328, 'review': 3329, 'revised': 3330, 'reviving': 3331, 'reward': 3332, 'rhythm': 3333, 'rhythmic': 3334, 'ribs': 3335, 'richer': 3336, 'rid': 3337, 'ridiculous': 3338, 'ring': 3339, 'rio': 3340, 'riot': 3341, 'rise': 3342, 'rising': 3343, 'roads': 3344, 'roast': 3345, 'roll': 3346, 'roman': 3347, 'rooms': 3348, 'rooster': 3349, 'root': 3350, 'rose': 3351, 'roulette': 3352, 'ruin': 3353, 'ruined': 3354, 'ruling': 3355, 'rumors': 3356, 'runway': 3357, 'sack': 3358, 'sail': 3359, 'sailing': 3360, 'saltwater': 3361, 'salty': 3362, 'sanctuary': 3363, 'sandwich': 3364, 'sandy': 3365, 'saturated': 3366, 'sauce': 3367, 'saucepan': 3368, 'savings': 3369, 'scare': 3370, 'scars': 3371, 'scattering': 3372, 'scene': 3373, 'scheme': 3374, 'schoolroom': 3375, 'schoolyard': 3376, 'scientific': 3377, 'scotland': 3378, 'screen': 3379, 'screened': 3380, 'scrutinizing': 3381, 'seafood': 3382, 'searching': 3383, 'seas': 3384, 'season': 3385, 'seasons': 3386, 'seat': 3387, 'seats': 3388, 'secondary': 3389, 'secrets': 3390, 'sector': 3391, 'secure': 3392, 'seed': 3393, 'seeds': 3394, 'seizure': 3395, 'seldom': 3396, 'selfless': 3397, 'seminary': 3398, 'send': 3399, 'sentences': 3400, 'septic': 3401, 'sequence': 3402, 'setting': 3403, 'sever': 3404, 'several': 3405, 'sew': 3406, 'sewers': 3407, 'seychelles': 3408, 'shack': 3409, 'shady': 3410, 'shaking': 3411, 'shameful': 3412, 'sharp': 3413, 'shattering': 3414, 'shaving': 3415, 'sheath': 3416, 'sheet': 3417, 'shift': 3418, 'shipyard': 3419, 'shiver': 3420, 'shivering': 3421, 'shocked': 3422, 'shoelace': 3423, 'shoreline': 3424, 'shorter': 3425, 'should': 3426, 'shovelling': 3427, 'shredder': 3428, 'shrinkage': 3429, 'shrinking': 3430, 'shut': 3431, 'shyness': 3432, 'sides': 3433, 'sierra': 3434, 'sigh': 3435, 'sight': 3436, 'sights': 3437, 'signed': 3438, 'silence': 3439, 'silent': 3440, 'silently': 3441, 'sill': 3442, 'silliness': 3443, 'silly': 3444, 'silo': 3445, 'simile': 3446, 'simplicity': 3447, 'sites': 3448, 'situation': 3449, 'skateboard': 3450, 'skating': 3451, 'skeleton': 3452, 'skepticism': 3453, 'ski': 3454, 'skills': 3455, 'skip': 3456, 'skirt': 3457, 'slack': 3458, 'slaughter': 3459, 'slaughterhouse': 3460, 'slight': 3461, 'slip': 3462, 'sloth': 3463, 'slowly': 3464, 'slurred': 3465, 'smells': 3466, 'smooth': 3467, 'snoring': 3468, 'snow': 3469, 'snowball': 3470, 'so': 3471, 'sock': 3472, 'socks': 3473, 'solutions': 3474, 'solving': 3475, 'somber': 3476, 'someone': 3477, 'someplace': 3478, 'sorry': 3479, 'sort': 3480, 'sour': 3481, 'spacecraft': 3482, 'spaghetti': 3483, 'spanish': 3484, 'sparks': 3485, 'speaker': 3486, 'specialty': 3487, 'specific': 3488, 'spectacles': 3489, 'speech': 3490, 'speechless': 3491, 'spell': 3492, 'spillage': 3493, 'spills': 3494, 'sping': 3495, 'spinning': 3496, 'spiritual': 3497, 'spit': 3498, 'splats': 3499, 'spontaneous': 3500, 'sprain': 3501, 'spreadsheet': 3502, 'stagnant': 3503, 'stammering': 3504, 'standard': 3505, 'standards': 3506, 'stanza': 3507, 'stapler': 3508, 'star': 3509, 'stares': 3510, 'stars': 3511, 'starting': 3512, 'startled': 3513, 'starvation': 3514, 'starve': 3515, 'stationery': 3516, 'steak': 3517, 'stems': 3518, 'steppe': 3519, 'stiffness': 3520, 'stimulation': 3521, 'stink': 3522, 'stitches': 3523, 'stocking': 3524, 'stockings': 3525, 'stockmarket': 3526, 'stockpile': 3527, 'stoic': 3528, 'stolen': 3529, 'stone': 3530, 'storage': 3531, 'storm': 3532, 'straighten': 3533, 'straightforward': 3534, 'stressed': 3535, 'strikes': 3536, 'stringed': 3537, 'stripes': 3538, 'stroked': 3539, 'stronger': 3540, 'studies': 3541, 'studios': 3542, 'stumble': 3543, 'submachine': 3544, 'submarines': 3545, 'subordinate': 3546, 'suburban': 3547, 'succeeding': 3548, 'successful': 3549, 'suck': 3550, 'sucking': 3551, 'sudan': 3552, 'sufficient': 3553, 'suite': 3554, 'suits': 3555, 'sum': 3556, 'sump': 3557, 'sundown': 3558, 'sunlight': 3559, 'super': 3560, 'supplies': 3561, 'surprise': 3562, 'surprised': 3563, 'surrender': 3564, 'survive': 3565, 'sushi': 3566, 'suspension': 3567, 'sustaining': 3568, 'swallow': 3569, 'swallowing': 3570, 'swamp': 3571, 'sweater': 3572, 'sweet': 3573, 'swing': 3574, 'swiss': 3575, 'sword': 3576, 'syllable': 3577, 'tag': 3578, 'tahoe': 3579, 'tail': 3580, 'tailor': 3581, 'tails': 3582, 'taking': 3583, 'tape': 3584, 'tardiness': 3585, 'target': 3586, 'tax': 3587, 'taxes': 3588, 'taxi': 3589, 'tea': 3590, 'teachers': 3591, 'teams': 3592, 'tear': 3593, 'teetotaller': 3594, 'telegraph': 3595, 'telescope': 3596, 'telling': 3597, 'temple': 3598, 'temporary': 3599, 'ten': 3600, 'tended': 3601, 'tentative': 3602, 'term': 3603, 'terminal': 3604, 'terrain': 3605, 'terrorism': 3606, 'testify': 3607, 'testimony': 3608, 'tetrahedron': 3609, 'than': 3610, 'that': 3611, 'theaters': 3612, 'thing': 3613, 'thirst': 3614, 'thorough': 3615, 'though': 3616, 'thoughtful': 3617, 'thoughts': 3618, 'three': 3619, 'thrift': 3620, 'throne': 3621, 'thunderstorm': 3622, 'tickle': 3623, 'tide': 3624, 'tidepools': 3625, 'tie': 3626, 'times': 3627, 'timing': 3628, 'tire': 3629, 'toenails': 3630, 'toilet': 3631, 'tokio': 3632, 'tomales': 3633, 'toolkit': 3634, 'tools': 3635, 'tornadoes': 3636, 'torture': 3637, 'tower': 3638, 'toxic': 3639, 'toxicity': 3640, 'tradesmen': 3641, 'tradition': 3642, 'trailer': 3643, 'trained': 3644, 'tramp': 3645, 'transistor': 3646, 'transit': 3647, 'transmission': 3648, 'trapped': 3649, 'trauma': 3650, 'traveling': 3651, 'treetops': 3652, 'trek': 3653, 'trenches': 3654, 'tricks': 3655, 'trigonometry': 3656, 'trips': 3657, 'tromsø': 3658, 'trophy': 3659, 'troubles': 3660, 'trouser': 3661, 'true': 3662, 'trunks': 3663, 'truthful': 3664, 'tsunami': 3665, 'tub': 3666, 'tugboat': 3667, 'tumor': 3668, 'tuna': 3669, 'tundra': 3670, 'turmoil': 3671, 'turning': 3672, 'tuxedo': 3673, 'twins': 3674, 'typewriter': 3675, 'u.s': 3676, 'ugliness': 3677, 'ugly': 3678, 'unbreakable': 3679, 'uncertainty': 3680, 'uncomfortable': 3681, 'uncommon': 3682, 'undecided': 3683, 'under': 3684, 'underage': 3685, 'underbrush': 3686, 'undergrowth': 3687, 'underneath': 3688, 'underpants': 3689, 'unemployed': 3690, 'unfeeling': 3691, 'unfinished': 3692, 'ungulate': 3693, 'unhappiness': 3694, 'unhappy': 3695, 'unimportant': 3696, 'unite': 3697, 'universal': 3698, 'unknowable': 3699, 'unnecessary': 3700, 'unwanted': 3701, 'ups': 3702, 'upstairs': 3703, 'upstream': 3704, 'us': 3705, 'vacationgoer': 3706, 'vacuuming': 3707, 'vagina': 3708, 'vain': 3709, 'value': 3710, 'valve': 3711, 'various': 3712, 'vase': 3713, 'vault': 3714, 'vehicle': 3715, 'vehicles': 3716, 'vein': 3717, 'venereal': 3718, 'venezuela': 3719, 'venice': 3720, 'venue': 3721, 'verbal': 3722, 'verbose': 3723, 'verdict': 3724, 'versailles': 3725, 'vertigo': 3726, 'vessels': 3727, 'victorian': 3728, 'villa': 3729, 'ville': 3730, 'vinegar': 3731, 'vineyard': 3732, 'violent': 3733, 'violin': 3734, 'virtuous': 3735, 'vision': 3736, 'visit': 3737, 'visiting': 3738, 'vocation': 3739, 'void': 3740, 'volunteer': 3741, 'vomit': 3742, 'vote': 3743, 'wade': 3744, 'waist': 3745, 'wallflower': 3746, 'walls': 3747, 'walmart': 3748, 'wanting': 3749, 'ward': 3750, 'warren': 3751, 'wastebasket': 3752, 'wasteland': 3753, 'wasting': 3754, 'watching': 3755, 'watering': 3756, 'waves': 3757, 'waving': 3758, 'ways': 3759, 'week': 3760, 'weigh': 3761, 'weird': 3762, 'westerly': 3763, 'wetness': 3764, 'wetting': 3765, 'what': 3766, 'whatever': 3767, 'wheeze': 3768, 'wherever': 3769, 'whistle': 3770, 'whistles': 3771, 'whorehouse': 3772, 'wide': 3773, 'wildlife': 3774, 'wind': 3775, 'windmill': 3776, 'windowless': 3777, 'windy': 3778, 'winner': 3779, 'winners': 3780, 'wins': 3781, 'wires': 3782, 'witness': 3783, 'witty': 3784, 'woman': 3785, 'women': 3786, 'wonderful': 3787, 'wooden': 3788, 'woodlands': 3789, 'wool': 3790, 'word': 3791, 'worked': 3792, 'workers': 3793, 'workload': 3794, 'workmanship': 3795, 'works': 3796, 'worrying': 3797, 'worship': 3798, 'worthlessness': 3799, 'would': 3800, 'wounds': 3801, 'wrap': 3802, 'wreak': 3803, 'wreck': 3804, 'wrinkled': 3805, 'wrinkles': 3806, 'wristwatch': 3807, 'writer': 3808, 'writing': 3809, 'yawn': 3810, 'year': 3811, 'yeast': 3812, 'yell': 3813, 'ymca': 3814, 'zealand': 3815, 'zip': 3816, 'zone': 3817, 'zones': 3818, 'zoological': 3819, 'zoos': 3820})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nls5YSCcW7w8"
      },
      "source": [
        "BATCH_SIZE = 128\r\n",
        "train_iterator_second, valid_iterator_second, test_iterator_second = BucketIterator.splits(\r\n",
        "                                (Train_Dataset_Second, Valid_Dataset_Second, Test_Dataset_Second),  \r\n",
        "                                batch_size = BATCH_SIZE,\r\n",
        "                                sort_key = lambda x: len(x.questions_second),\r\n",
        "                                sort_within_batch=True ,\r\n",
        "                                device = device)\r\n",
        "\r\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7S-yswfXGqS",
        "outputId": "f8b000c2-75f5-4504-a10c-c15c52b67aad"
      },
      "source": [
        "for i, batch in enumerate(valid_iterator_second):\r\n",
        "  print(type(batch), type(batch.answers_second), type(batch.questions_second))\r\n",
        "  break"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torchtext.data.batch.Batch'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq7BwOO4EzFd"
      },
      "source": [
        "## Building the Seq2Seq Model\n",
        "\n",
        "### Encoder\n",
        "\n",
        "The encoder is similar to the previous one, with the multi-layer LSTM swapped for a single-layer GRU. We also don't pass the dropout as an argument to the GRU as that dropout is used between each layer of a multi-layered RNN. As we only have a single layer, PyTorch will display a warning if we try and use pass a dropout value to it.\n",
        "\n",
        "Another thing to note about the GRU is that it only requires and returns a hidden state, there is no cell state like in the LSTM.\n",
        "\n",
        "$$\\begin{align*}\n",
        "h_t &= \\text{GRU}(e(x_t), h_{t-1})\\\\\n",
        "(h_t, c_t) &= \\text{LSTM}(e(x_t), h_{t-1}, c_{t-1})\\\\\n",
        "h_t &= \\text{RNN}(e(x_t), h_{t-1})\n",
        "\\end{align*}$$\n",
        "\n",
        "From the equations above, it looks like the RNN and the GRU are identical. Inside the GRU, however, is a number of *gating mechanisms* that control the information flow in to and out of the hidden state (similar to an LSTM). Again, for more info, check out [this](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) excellent post. \n",
        "\n",
        "The rest of the encoder should be very familar from the last session, it takes in a sequence, $X = \\{x_1, x_2, ... , x_T\\}$, passes it through the embedding layer, recurrently calculates hidden states, $H = \\{h_1, h_2, ..., h_T\\}$, and returns a context vector (the final hidden state), $z=h_T$.\n",
        "\n",
        "$$h_t = \\text{EncoderGRU}(e(x_t), h_{t-1})$$\n",
        "\n",
        "This is identical to the encoder of the general seq2seq model, with all the \"magic\" happening inside the GRU (green).\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq5.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11CnDhTkEzFd"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim) #no dropout as only one layer!\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded) #no cell state!\n",
        "        \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhBViJ4-EzFe"
      },
      "source": [
        "## Decoder\n",
        "\n",
        "The decoder is where the implementation differs significantly from the previous model and we alleviate some of the information compression.\n",
        "\n",
        "Instead of the GRU in the decoder taking just the embedded target token, $d(y_t)$ and the previous hidden state $s_{t-1}$ as inputs, it also takes the context vector $z$. \n",
        "\n",
        "$$s_t = \\text{DecoderGRU}(d(y_t), s_{t-1}, z)$$\n",
        "\n",
        "Note how this context vector, $z$, does not have a $t$ subscript, meaning we re-use the same context vector returned by the encoder for every time-step in the decoder. \n",
        "\n",
        "Before, we predicted the next token, $\\hat{y}_{t+1}$, with the linear layer, $f$, only using the top-layer decoder hidden state at that time-step, $s_t$, as $\\hat{y}_{t+1}=f(s_t^L)$. Now, we also pass the embedding of current token, $d(y_t)$ and the context vector, $z$ to the linear layer.\n",
        "\n",
        "$$\\hat{y}_{t+1} = f(d(y_t), s_t, z)$$\n",
        "\n",
        "Thus, our decoder now looks something like this:\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq6.png?raw=1)\n",
        "\n",
        "Note, the initial hidden state, $s_0$, is still the context vector, $z$, so when generating the first token we are actually inputting two identical context vectors into the GRU.\n",
        "\n",
        "How do these two changes reduce the information compression? Well, hypothetically the decoder hidden states, $s_t$, no longer need to contain information about the source sequence as it is always available as an input. Thus, it only needs to contain information about what tokens it has generated so far. The addition of $y_t$ to the linear layer also means this layer can directly see what the token is, without having to get this information from the hidden state. \n",
        "\n",
        "However, this hypothesis is just a hypothesis, it is impossible to determine how the model actually uses the information provided to it (don't listen to anyone that says differently). Nevertheless, it is a solid intuition and the results seem to indicate that this modifications are a good idea!\n",
        "\n",
        "Within the implementation, we will pass $d(y_t)$ and $z$ to the GRU by concatenating them together, so the input dimensions to the GRU are now `emb_dim + hid_dim` (as context vector will be of size `hid_dim`). The linear layer will take $d(y_t), s_t$ and $z$ also by concatenating them together, hence the input dimensions are now `emb_dim + hid_dim*2`. We also don't pass a value of dropout to the GRU as it only uses a single layer.\n",
        "\n",
        "`forward` now takes a `context` argument. Inside of `forward`, we concatenate $y_t$ and $z$ as `emb_con` before feeding to the GRU, and we concatenate $d(y_t)$, $s_t$ and $z$ together as `output` before feeding it through the linear layer to receive our predictions, $\\hat{y}_{t+1}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRA8hkiLEzFh"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, context):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #context = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n layers and n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        #context = [1, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        emb_con = torch.cat((embedded, context), dim = 2)\n",
        "            \n",
        "        #emb_con = [1, batch size, emb dim + hid dim]\n",
        "            \n",
        "        output, hidden = self.rnn(emb_con, hidden)\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        \n",
        "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), \n",
        "                           dim = 1)\n",
        "        \n",
        "        #output = [batch size, emb dim + hid dim * 2]\n",
        "        \n",
        "        prediction = self.fc_out(output)\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqR66FM2EzFj"
      },
      "source": [
        "## Seq2Seq Model\n",
        "\n",
        "Putting the encoder and decoder together, we get:\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq7.png?raw=1)\n",
        "\n",
        "Again, in this implementation we need to ensure the hidden dimensions in both the encoder and the decoder are the same.\n",
        "\n",
        "Briefly going over all of the steps:\n",
        "- the `outputs` tensor is created to hold all predictions, $\\hat{Y}$\n",
        "- the source sequence, $X$, is fed into the encoder to receive a `context` vector\n",
        "- the initial decoder hidden state is set to be the `context` vector, $s_0 = z = h_T$\n",
        "- we use a batch of `<sos>` tokens as the first `input`, $y_1$\n",
        "- we then decode within a loop:\n",
        "  - inserting the input token $y_t$, previous hidden state, $s_{t-1}$, and the context vector, $z$, into the decoder\n",
        "  - receiving a prediction, $\\hat{y}_{t+1}$, and a new hidden state, $s_t$\n",
        "  - we then decide if we are going to teacher force or not, setting the next input as appropriate (either the ground truth next token in the target sequence or the highest predicted next token)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDyNrQ8VEzFk"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is the context\n",
        "        context = self.encoder(src)\n",
        "        \n",
        "        #context also used as the initial hidden state of the decoder\n",
        "        hidden = context\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state and the context state\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden = self.decoder(input, hidden, context)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxVMMPUyEzFk"
      },
      "source": [
        "# Training the Seq2Seq Model\n",
        "\n",
        "The rest of this session is very similar to the previous one. \n",
        "\n",
        "We initialise our encoder, decoder and seq2seq model (placing it on the GPU if we have one). As before, the embedding dimensions and the amount of dropout used can be different between the encoder and the decoder, but the hidden dimensions must remain the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDG6jOSuEzFk"
      },
      "source": [
        "INPUT_DIM = len(Questions_Second.vocab)\n",
        "OUTPUT_DIM = len(Answers_Second.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLIc1l6CEzFk"
      },
      "source": [
        "Next, we initialize our parameters. The paper states the parameters are initialized from a normal distribution with a mean of 0 and a standard deviation of 0.01, i.e. $\\mathcal{N}(0, 0.01)$. \n",
        "\n",
        "It also states we should initialize the recurrent parameters to a special initialization, however to keep things simple we'll also initialize them to $\\mathcal{N}(0, 0.01)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgqMqq-oEzFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b374e808-9a73-4559-b0cc-301213c71c03"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(8634, 256)\n",
              "    (rnn): GRU(256, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(3821, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc_out): Linear(in_features=1280, out_features=3821, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd1QoOsUEzFl"
      },
      "source": [
        "We print out the number of parameters.\n",
        "\n",
        "Even though we only have a single layer RNN for our encoder and decoder we actually have **more** parameters  than the last model. This is due to the increased size of the inputs to the GRU and the linear layer. However, it is not a significant amount of parameters and causes a minimal amount of increase in training time (~3 seconds per epoch extra)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IggCwIBgEzFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae57a583-c4b8-40a0-a3da-629f1d662453"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 11,235,053 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiH54qzHEzFl"
      },
      "source": [
        "We initiaize our optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO1_eoG7EzFl"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwkV3FKlEzFl"
      },
      "source": [
        "We also initialize the loss function, making sure to ignore the loss on `<pad>` tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0DAbGbcEzFl"
      },
      "source": [
        "TRG_PAD_IDX = Answers_Second.vocab.stoi[Answers_Second.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTay7E9rEzFm"
      },
      "source": [
        "We then create the training loop..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5OYuoFdEzFm"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.questions_second\n",
        "        trg = batch.answers_second\n",
        "        \n",
        "        src = src.permute(1, 0)\n",
        "        trg = trg.permute(1, 0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:]\n",
        "        trg = torch.reshape(trg, (-1,)) #trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rfUx5lhEzFm"
      },
      "source": [
        "...and the evaluation loop, remembering to set the model to `eval` mode and turn off teaching forcing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw022pw0EzFm"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.questions_second\n",
        "            trg = batch.answers_second\n",
        "            \n",
        "            src = src.permute(1, 0)\n",
        "            trg = trg.permute(1, 0)\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:]\n",
        "            trg = torch.reshape(trg, (-1,)) #trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E43h8dnQEzFm"
      },
      "source": [
        "We'll also define the function that calculates how long an epoch takes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTAmu3-EEzFm"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbCGyO4ZEzFm"
      },
      "source": [
        "Then, we train our model, saving the parameters that give us the best validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjFyRUK9EzFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a90c0ee7-fec1-4b71-ed81-27bc40f19610"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator_second, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator_second, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 54s\n",
            "\tTrain Loss: 7.035 | Train PPL: 1135.915\n",
            "\t Val. Loss: 6.565 |  Val. PPL: 709.561\n",
            "Epoch: 02 | Time: 0m 55s\n",
            "\tTrain Loss: 6.029 | Train PPL: 415.394\n",
            "\t Val. Loss: 6.385 |  Val. PPL: 593.117\n",
            "Epoch: 03 | Time: 0m 54s\n",
            "\tTrain Loss: 5.485 | Train PPL: 240.986\n",
            "\t Val. Loss: 5.979 |  Val. PPL: 395.019\n",
            "Epoch: 04 | Time: 0m 54s\n",
            "\tTrain Loss: 4.866 | Train PPL: 129.810\n",
            "\t Val. Loss: 5.928 |  Val. PPL: 375.242\n",
            "Epoch: 05 | Time: 0m 58s\n",
            "\tTrain Loss: 4.276 | Train PPL:  71.935\n",
            "\t Val. Loss: 5.979 |  Val. PPL: 395.212\n",
            "Epoch: 06 | Time: 0m 58s\n",
            "\tTrain Loss: 3.728 | Train PPL:  41.615\n",
            "\t Val. Loss: 5.724 |  Val. PPL: 306.121\n",
            "Epoch: 07 | Time: 0m 55s\n",
            "\tTrain Loss: 3.166 | Train PPL:  23.702\n",
            "\t Val. Loss: 5.856 |  Val. PPL: 349.310\n",
            "Epoch: 08 | Time: 0m 54s\n",
            "\tTrain Loss: 2.657 | Train PPL:  14.255\n",
            "\t Val. Loss: 6.009 |  Val. PPL: 407.119\n",
            "Epoch: 09 | Time: 0m 53s\n",
            "\tTrain Loss: 2.181 | Train PPL:   8.859\n",
            "\t Val. Loss: 6.213 |  Val. PPL: 499.393\n",
            "Epoch: 10 | Time: 0m 53s\n",
            "\tTrain Loss: 1.779 | Train PPL:   5.925\n",
            "\t Val. Loss: 6.524 |  Val. PPL: 681.017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztR5mNm8EzFn"
      },
      "source": [
        "Finally, we test the model on the test set using these \"best\" parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaJo3X9aEzFn"
      },
      "source": [
        "#model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "#test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "#print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY7SsC8TEzFn"
      },
      "source": [
        "Just looking at the test loss, we get better performance. This is a pretty good sign that this model architecture is doing something right! Relieving the information compression seems like the way forard, and in the next tutorial we'll expand on this even further with *attention*."
      ]
    }
  ]
}