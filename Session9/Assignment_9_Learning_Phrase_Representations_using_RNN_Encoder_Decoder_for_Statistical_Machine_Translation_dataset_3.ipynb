{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 9 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation dataset 3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/endteamschoolofai/END/blob/main/Session9/Assignment_9_Learning_Phrase_Representations_using_RNN_Encoder_Decoder_for_Statistical_Machine_Translation_dataset_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzxOJwN7EzFO"
      },
      "source": [
        "# 2 - Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n",
        "\n",
        "In this second notebook on sequence-to-sequence models using PyTorch and TorchText, we'll be implementing the model from [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078). This model will achieve improved test perplexity whilst only using a single layer RNN in both the encoder and the decoder.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Let's remind ourselves of the general encoder-decoder model.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq1.png?raw=1)\n",
        "\n",
        "We use our encoder (green) over the embedded source sequence (yellow) to create a context vector (red). We then use that context vector with the decoder (blue) and a linear layer (purple) to generate the target sentence.\n",
        "\n",
        "In the previous model, we used an multi-layered LSTM as the encoder and decoder.\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq4.png?raw=1)\n",
        "\n",
        "One downside of the previous model is that the decoder is trying to cram lots of information into the hidden states. Whilst decoding, the hidden state will need to contain information about the whole of the source sequence, as well as all of the tokens have been decoded so far. By alleviating some of this information compression, we can create a better model!\n",
        "\n",
        "We'll also be using a GRU (Gated Recurrent Unit) instead of an LSTM (Long Short-Term Memory). Why? Mainly because that's what they did in the paper (this paper also introduced GRUs) and also because we used LSTMs last time. To understand how GRUs (and LSTMs) differ from standard RNNS, check out [this](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) link. Is a GRU better than an LSTM? [Research](https://arxiv.org/abs/1412.3555) has shown they're pretty much the same, and both are better than standard RNNs. \n",
        "\n",
        "## Preparing Data\n",
        "\n",
        "All of the data preparation will be (almost) the same as last time, so we'll very briefly detail what each code block does. See the previous notebook for a recap.\n",
        "\n",
        "We'll import PyTorch, TorchText, spaCy and a few standard modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUnMLdevEzFT"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext import data \n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srzErHAsEzFU"
      },
      "source": [
        "Then set a random seed for deterministic results/reproducability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_wP4J4LEzFX"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4davOyYEzFd"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4w-kFcNFZJz"
      },
      "source": [
        "# Dataset 3 - AmbigQA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8enB3cfFYqi",
        "outputId": "2b75e5da-606a-41bb-ea7f-acbd757fdcce"
      },
      "source": [
        "!wget https://nlp.cs.washington.edu/ambigqa/data/ambignq.zip \r\n",
        "!unzip ambignq.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-08 16:31:33--  https://nlp.cs.washington.edu/ambigqa/data/ambignq.zip\n",
            "Resolving nlp.cs.washington.edu (nlp.cs.washington.edu)... 128.208.3.120, 2607:4000:200:12::78\n",
            "Connecting to nlp.cs.washington.edu (nlp.cs.washington.edu)|128.208.3.120|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18639517 (18M) [application/zip]\n",
            "Saving to: ‘ambignq.zip’\n",
            "\n",
            "ambignq.zip         100%[===================>]  17.78M  35.5MB/s    in 0.5s    \n",
            "\n",
            "2021-01-08 16:31:33 (35.5 MB/s) - ‘ambignq.zip’ saved [18639517/18639517]\n",
            "\n",
            "Archive:  ambignq.zip\n",
            "  inflating: LICENSE                 \n",
            "  inflating: train.json              \n",
            "  inflating: dev.json                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehXKpdwsF5YT"
      },
      "source": [
        "train_df_third = pd.read_json('train.json')\r\n",
        "test_df_third = pd.read_json('dev.json')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "ceQ0p_CgF_Tu",
        "outputId": "677b4948-f1ca-4cc0-cb26-45f2dd60db16"
      },
      "source": [
        "train_df_third.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>viewed_doc_titles</th>\n",
              "      <th>used_queries</th>\n",
              "      <th>annotations</th>\n",
              "      <th>nq_answer</th>\n",
              "      <th>id</th>\n",
              "      <th>nq_doc_title</th>\n",
              "      <th>question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[The Simpsons]</td>\n",
              "      <td>[{'query': 'When did the simpsons first air on...</td>\n",
              "      <td>[{'type': 'multipleQAs', 'qaPairs': [{'questio...</td>\n",
              "      <td>[December 17 , 1989]</td>\n",
              "      <td>-4469503464110108160</td>\n",
              "      <td>The Simpsons</td>\n",
              "      <td>When did the simpsons first air on television?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[John Adams (miniseries)]</td>\n",
              "      <td>[{'query': 'John adams tv', 'results': [{'titl...</td>\n",
              "      <td>[{'type': 'singleAnswer', 'answer': ['David Mo...</td>\n",
              "      <td>[David Morse]</td>\n",
              "      <td>4790842463458965504</td>\n",
              "      <td>John Adams (miniseries)</td>\n",
              "      <td>Who played george washington in the john adams...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Marriage age in the United States]</td>\n",
              "      <td>[{'query': 'legal age of marriage in usa', 're...</td>\n",
              "      <td>[{'type': 'multipleQAs', 'qaPairs': [{'questio...</td>\n",
              "      <td>[18, Nebraska ( 19 ), Mississippi ( 21 )]</td>\n",
              "      <td>-6631915997977101312</td>\n",
              "      <td>Age of marriage in the United States</td>\n",
              "      <td>What is the legal age of marriage in usa?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[Barefoot in the Park, Barefoot in the Park (f...</td>\n",
              "      <td>[{'query': 'Who starred in barefoot in the par...</td>\n",
              "      <td>[{'type': 'multipleQAs', 'qaPairs': [{'questio...</td>\n",
              "      <td>[Robert Redford, Elizabeth Ashley]</td>\n",
              "      <td>-3098213414945179648</td>\n",
              "      <td>Barefoot in the Park</td>\n",
              "      <td>Who starred in barefoot in the park on broadway?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Timeline of the Manhattan Project, Manhattan ...</td>\n",
              "      <td>[{'query': 'When did the manhattan project beg...</td>\n",
              "      <td>[{'type': 'multipleQAs', 'qaPairs': [{'questio...</td>\n",
              "      <td>[From 1942 to 1946]</td>\n",
              "      <td>-927805218867163520</td>\n",
              "      <td>Timeline of the Manhattan Project</td>\n",
              "      <td>When did the manhattan project began and end?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   viewed_doc_titles  ...                                           question\n",
              "0                                     [The Simpsons]  ...     When did the simpsons first air on television?\n",
              "1                          [John Adams (miniseries)]  ...  Who played george washington in the john adams...\n",
              "2                [Marriage age in the United States]  ...          What is the legal age of marriage in usa?\n",
              "3  [Barefoot in the Park, Barefoot in the Park (f...  ...   Who starred in barefoot in the park on broadway?\n",
              "4  [Timeline of the Manhattan Project, Manhattan ...  ...      When did the manhattan project began and end?\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "cMqMnOeVGStq",
        "outputId": "61f5bc9a-02ef-4a85-8f8f-740e2e1eeccb"
      },
      "source": [
        "test_df_third.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>viewed_doc_titles</th>\n",
              "      <th>used_queries</th>\n",
              "      <th>annotations</th>\n",
              "      <th>nq_answer</th>\n",
              "      <th>id</th>\n",
              "      <th>nq_doc_title</th>\n",
              "      <th>question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Dexter (season 1)]</td>\n",
              "      <td>[{'query': 'Who plays the doctor in dexter sea...</td>\n",
              "      <td>[{'type': 'singleAnswer', 'answer': ['Tony Gol...</td>\n",
              "      <td>[Tony Goldwyn]</td>\n",
              "      <td>-807825952267713152</td>\n",
              "      <td>Dexter (season 1)</td>\n",
              "      <td>Who plays the doctor in dexter season 1?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[Spermatogenesis]</td>\n",
              "      <td>[{'query': 'How often does spermatogeneis—the ...</td>\n",
              "      <td>[{'type': 'singleAnswer', 'answer': ['usually ...</td>\n",
              "      <td>[74 days]</td>\n",
              "      <td>8266116451988110336</td>\n",
              "      <td>Spermatogenesis</td>\n",
              "      <td>How often does spermatogeneis—the production o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[Remote control]</td>\n",
              "      <td>[{'query': 'When was the first remote control ...</td>\n",
              "      <td>[{'type': 'singleAnswer', 'answer': ['1950']},...</td>\n",
              "      <td>[1950]</td>\n",
              "      <td>7336174019902289920</td>\n",
              "      <td>Remote control</td>\n",
              "      <td>When was the first remote control tv invented?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[History of the St. Louis Cardinals (NFL), His...</td>\n",
              "      <td>[{'query': 'Why did the st louis cardinals mov...</td>\n",
              "      <td>[{'type': 'singleAnswer', 'answer': ['mediocri...</td>\n",
              "      <td>[1988]</td>\n",
              "      <td>8630912480840635392</td>\n",
              "      <td>History of the St. Louis Cardinals (NFL)</td>\n",
              "      <td>Why did the st louis cardinals move to arizona?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[List of Chesapeake Shores episodes, Chesapeak...</td>\n",
              "      <td>[{'query': 'How many episodes are in season 2 ...</td>\n",
              "      <td>[{'type': 'singleAnswer', 'answer': ['10']}, {...</td>\n",
              "      <td>[10]</td>\n",
              "      <td>9187719029377880064</td>\n",
              "      <td>Chesapeake Shores</td>\n",
              "      <td>How many episodes are in season 2 of chesapeak...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   viewed_doc_titles  ...                                           question\n",
              "0                                [Dexter (season 1)]  ...           Who plays the doctor in dexter season 1?\n",
              "1                                  [Spermatogenesis]  ...  How often does spermatogeneis—the production o...\n",
              "2                                   [Remote control]  ...     When was the first remote control tv invented?\n",
              "3  [History of the St. Louis Cardinals (NFL), His...  ...    Why did the st louis cardinals move to arizona?\n",
              "4  [List of Chesapeake Shores episodes, Chesapeak...  ...  How many episodes are in season 2 of chesapeak...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVZfzO_gGa8l",
        "outputId": "3565c500-5bd5-457b-8e4b-65e6c0176c8e"
      },
      "source": [
        "test_df_third.question , test_df_third.annotations"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0                Who plays the doctor in dexter season 1?\n",
              " 1       How often does spermatogeneis—the production o...\n",
              " 2          When was the first remote control tv invented?\n",
              " 3         Why did the st louis cardinals move to arizona?\n",
              " 4       How many episodes are in season 2 of chesapeak...\n",
              "                               ...                        \n",
              " 1997                How long is a rainbow six siege game?\n",
              " 1998     Where did the free settlers settle in australia?\n",
              " 1999        Real name of gwen stacy in amazing spiderman?\n",
              " 2000     Who owns the biggest house in the united states?\n",
              " 2001               Who played ben parish in the 5th wave?\n",
              " Name: question, Length: 2002, dtype: object,\n",
              " 0       [{'type': 'singleAnswer', 'answer': ['Tony Gol...\n",
              " 1       [{'type': 'singleAnswer', 'answer': ['usually ...\n",
              " 2       [{'type': 'singleAnswer', 'answer': ['1950']},...\n",
              " 3       [{'type': 'singleAnswer', 'answer': ['mediocri...\n",
              " 4       [{'type': 'singleAnswer', 'answer': ['10']}, {...\n",
              "                               ...                        \n",
              " 1997    [{'type': 'multipleQAs', 'qaPairs': [{'questio...\n",
              " 1998    [{'type': 'multipleQAs', 'qaPairs': [{'questio...\n",
              " 1999    [{'type': 'multipleQAs', 'qaPairs': [{'questio...\n",
              " 2000    [{'type': 'singleAnswer', 'answer': ['William ...\n",
              " 2001    [{'type': 'singleAnswer', 'answer': ['Nick Rob...\n",
              " Name: annotations, Length: 2002, dtype: object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvJLJhM4Ge_D",
        "outputId": "e7c9048d-f401-4e7e-a160-0d895035a59f"
      },
      "source": [
        "train_df_third.question[1] , train_df_third.annotations[1]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Who played george washington in the john adams series?',\n",
              " [{'answer': ['David Morse'], 'type': 'singleAnswer'}])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2RfzynrGhJG",
        "outputId": "98f0838d-c204-4b34-88ee-0072595b7a21"
      },
      "source": [
        "train_df_third.question[0] , train_df_third.annotations[0]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('When did the simpsons first air on television?',\n",
              " [{'qaPairs': [{'answer': ['April 19, 1987'],\n",
              "     'question': 'When did the Simpsons first air on television as an animated short on the Tracey Ullman Show?'},\n",
              "    {'answer': ['December 17, 1989'],\n",
              "     'question': 'When did the Simpsons first air as a half-hour prime time show?'}],\n",
              "   'type': 'multipleQAs'}])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDyBpBMNGhrQ",
        "outputId": "eeb28c5f-f52e-45a3-d3ec-b2657555c1eb"
      },
      "source": [
        "train_df_third.shape, test_df_third.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10036, 7), (2002, 7))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHqLVkDbtTiZ"
      },
      "source": [
        "train_questions_third = []\r\n",
        "train_answers_third = []\r\n",
        "for i in range(len(train_df_third)):\r\n",
        "  if (train_df_third.annotations[i][0]['type'] == 'singleAnswer'):\r\n",
        "    question = train_df_third.question[i]\r\n",
        "    answer = train_df_third.annotations[i][0]['answer'][0]\r\n",
        "    train_questions_third.append(question)\r\n",
        "    train_answers_third.append(answer)\r\n",
        "  elif (train_df_third.annotations[i][0]['type'] == 'multipleQAs'):\r\n",
        "    for j in range(len(train_df_third.annotations[i][0]['qaPairs'])):\r\n",
        "      question = train_df_third.annotations[i][0]['qaPairs'][j]['question']\r\n",
        "      answer = train_df_third.annotations[i][0]['qaPairs'][j]['answer'][0]\r\n",
        "      train_questions_third.append(question)\r\n",
        "      train_answers_third.append(answer)\r\n",
        "\r\n",
        "\r\n",
        "test_questions_third = []\r\n",
        "test_answers_third = []\r\n",
        "for i in range(len(test_df_third)):\r\n",
        "  if (test_df_third.annotations[i][0]['type'] == 'singleAnswer'):\r\n",
        "    question = test_df_third.question[i]\r\n",
        "    answer = test_df_third.annotations[i][0]['answer'][0]\r\n",
        "    test_questions_third.append(question)\r\n",
        "    test_answers_third.append(answer)\r\n",
        "  elif (test_df_third.annotations[i][0]['type'] == 'multipleQAs'):\r\n",
        "    for j in range(len(test_df_third.annotations[i][0]['qaPairs'])):\r\n",
        "      question = test_df_third.annotations[i][0]['qaPairs'][j]['question']\r\n",
        "      answer = test_df_third.annotations[i][0]['qaPairs'][j]['answer'][0]\r\n",
        "      test_questions_third.append(question)\r\n",
        "      test_answers_third.append(answer)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI3UU5rdtZ7C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1a9b1db-0b7a-44ee-e610-bea4aa86ae7f"
      },
      "source": [
        "len(train_questions_third), len(train_answers_third), len(test_questions_third), len(test_answers_third)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19244, 19244, 4377, 4377)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38wG3xIqGj8y"
      },
      "source": [
        "Questions_Third = data.Field(sequential = True, tokenize = 'spacy', batch_first =True)#, include_lengths=True)\r\n",
        "Answers_Third = data.Field(tokenize ='spacy', is_target=True, batch_first =True)#, sequential =False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH8g1UVOG-K8"
      },
      "source": [
        "fields = [('questions_third', Questions_Third),('answers_third', Answers_Third)]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag6QNcz6HU3R"
      },
      "source": [
        "example1 = [data.Example.fromlist([train_questions_third[i],train_answers_third[i]], fields) for i in range(len(train_questions_third))] \r\n",
        "example2 = [data.Example.fromlist([test_questions_third[i],test_answers_third[i]], fields) for i in range(len(test_questions_third))] "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XALghCjkHVh_"
      },
      "source": [
        "Train_Dataset_Third = data.Dataset(example1, fields)\r\n",
        "Valid_Dataset_Third = data.Dataset(example2, fields)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsUnWM5bHXji",
        "outputId": "a64eaa9e-bbd2-45d0-84d8-745a07a77ad4"
      },
      "source": [
        "(len(Train_Dataset_Third), len(Valid_Dataset_Third))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19244, 4377)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWRHgNRCHc9Z",
        "outputId": "96032505-8e63-4fb4-9905-1a064ff63ed7"
      },
      "source": [
        "print(vars(Train_Dataset_Third.examples[0]))\r\n",
        "print(vars(Valid_Dataset_Third.examples[0]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'questions_third': ['When', 'did', 'the', 'Simpsons', 'first', 'air', 'on', 'television', 'as', 'an', 'animated', 'short', 'on', 'the', 'Tracey', 'Ullman', 'Show', '?'], 'answers_third': ['April', '19', ',', '1987']}\n",
            "{'questions_third': ['Who', 'plays', 'the', 'doctor', 'in', 'dexter', 'season', '1', '?'], 'answers_third': ['Tony', 'Goldwyn']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rV1jT69Hc1P"
      },
      "source": [
        "Questions_Third.build_vocab(Train_Dataset_Third, min_freq = 2)\r\n",
        "Answers_Third.build_vocab(Train_Dataset_Third, min_freq = 2)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJ_geU_LIFIu",
        "outputId": "41d331b8-d1ed-4ded-a75c-4d9cabe1492e"
      },
      "source": [
        "print('Size of input vocab : ', len(Questions_Third.vocab))\r\n",
        "print('Size of label vocab : ', len(Answers_Third.vocab))\r\n",
        "print('Top 10 words appreared repeatedly :', list(Questions_Third.vocab.freqs.most_common(10)))\r\n",
        "print('Labels : ', Answers_Third.vocab.stoi)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  9694\n",
            "Size of label vocab :  5359\n",
            "Top 10 words appreared repeatedly : [('the', 21143), ('?', 19276), ('in', 10076), ('of', 8273), ('Who', 7319), ('is', 4142), ('When', 3927), ('What', 3149), ('did', 2889), ('was', 2823)]\n",
            "Labels :  defaultdict(<function _default_unk_index at 0x7f94ca7b8f28>, {'<unk>': 0, '<pad>': 1, ',': 2, 'and': 3, 'the': 4, 'of': 5, '2017': 6, '-': 7, '\"': 8, '2018': 9, 'The': 10, 'October': 11, 'September': 12, '\\t': 13, 'May': 14, 'July': 15, 'November': 16, 'June': 17, 'December': 18, 'March': 19, 'April': 20, 'January': 21, '1': 22, 'John': 23, 'August': 24, '2016': 25, '10': 26, 'New': 27, 'in': 28, '2015': 29, 'to': 30, '12': 31, '6': 32, 'United': 33, '2': 34, '4': 35, 'February': 36, '13': 37, '15': 38, '7': 39, '8': 40, 'a': 41, '3': 42, '5': 43, '17': 44, '26': 45, 'James': 46, '18': 47, \"'s\": 48, '11': 49, '9': 50, 'Michael': 51, '21': 52, 'States': 53, '°': 54, ')': 55, '(': 56, '2019': 57, '25': 58, '20': 59, '22': 60, '16': 61, '24': 62, '.': 63, '14': 64, '19': 65, '23': 66, '30': 67, ':': 68, 'South': 69, '2014': 70, 'George': 71, '2011': 72, '27': 73, '29': 74, 'David': 75, '2012': 76, 'York': 77, '28': 78, 'William': 79, '2008': 80, 'Paul': 81, 'Stadium': 82, '31': 83, 'II': 84, 'Jr.': 85, '2010': 86, '2013': 87, '%': 88, '2009': 89, 'Charles': 90, 'de': 91, 'Thomas': 92, 'or': 93, 'Richard': 94, 'years': 95, '2007': 96, '2006': 97, 'from': 98, 'England': 99, '2005': 100, 'North': 101, 'with': 102, '1998': 103, 'Robert': 104, ' ': 105, 'City': 106, '2003': 107, 'Chris': 108, '/': 109, 'Mexico': 110, 'Peter': 111, 'California': 112, 'on': 113, 'Australia': 114, 'Lee': 115, 'Taylor': 116, '2004': 117, 'Joseph': 118, 'Martin': 119, 'Tom': 120, '1999': 121, 'India': 122, 'Smith': 123, '1991': 124, '1994': 125, 'Edward': 126, 'Partial': 127, 'France': 128, 'Jones': 129, 'King': 130, 'million': 131, 'season': 132, '&': 133, 'Christopher': 134, '2002': 135, 'American': 136, 'Carolina': 137, 'Henry': 138, 'Washington': 139, 'at': 140, '–': 141, '1964': 142, '1992': 143, 'Africa': 144, 'West': 145, 'World': 146, '1995': 147, 'A': 148, 'Canada': 149, 'China': 150, 'Germany': 151, 'Park': 152, 'Patrick': 153, 'Season': 154, '[': 155, '1997': 156, '2001': 157, 'America': 158, 'Daniel': 159, 'Los': 160, 'Louis': 161, 'Sir': 162, ']': 163, '1973': 164, '1980': 165, '1984': 166, '1990': 167, 'Bob': 168, 'Kingdom': 169, 'Mark': 170, 'Mike': 171, 'Republic': 172, 'Williams': 173, 'for': 174, 'two': 175, '1977': 176, '2000': 177, 'Airport': 178, 'Al': 179, 'Angeles': 180, 'Bill': 181, 'Jackson': 182, 'Wilson': 183, 'Davis': 184, 'Kevin': 185, 'President': 186, 'State': 187, 'a][4': 188, '1988': 189, 'B.': 190, 'Frank': 191, 'Alan': 192, 'Bay': 193, 'Johnny': 194, 'London': 195, 'Mary': 196, 'River': 197, 'War': 198, 'by': 199, 'century': 200, '1945': 201, 'Jack': 202, 'Jennifer': 203, 'Justin': 204, 'Lake': 205, 'N': 206, 'Russell': 207, 'Scott': 208, 'as': 209, 'is': 210, '$': 211, '1982': 212, '2020': 213, 'Green': 214, 'I': 215, 'Jordan': 216, 'Marie': 217, 'Sam': 218, 'San': 219, 'Steve': 220, '1971': 221, '1993': 222, 'British': 223, 'III': 224, 'J.': 225, 'White': 226, 'between': 227, '1969': 228, '1976': 229, 'Andy': 230, 'Anthony': 231, 'E': 232, 'Hall': 233, 'Johnson': 234, 'Spain': 235, '—': 236, 'Asia': 237, 'Brown': 238, 'Georgia': 239, 'House': 240, 'Stephen': 241, 'University': 242, 'W': 243, '1966': 244, '1974': 245, '1986': 246, 'Adam': 247, 'Alexander': 248, 'Annular': 249, 'Barry': 250, 'Boston': 251, 'Chicago': 252, 'Florida': 253, 'Harris': 254, 'International': 255, 'Prince': 256, 'late': 257, 'three': 258, '1954': 259, '1981': 260, '1983': 261, 'Aaron': 262, 'Andrew': 263, 'Brazil': 264, 'Elizabeth': 265, 'General': 266, 'Japan': 267, 'Jason': 268, 'Jim': 269, 'Joe': 270, 'Matthew': 271, 'Patriots': 272, 'Tony': 273, 'Union': 274, 'Young': 275, 'after': 276, '1987': 277, '1996': 278, 'Anne': 279, 'Billy': 280, 'Bobby': 281, 'Dylan': 282, 'East': 283, 'Francisco': 284, 'Houston': 285, 'Howard': 286, 'Pacific': 287, 'Ray': 288, 'Roger': 289, 'Ryan': 290, 'Saint': 291, 'Singh': 292, 'St.': 293, 'Super': 294, 'Tim': 295, 'Tyler': 296, 'early': 297, 'minutes': 298, '1947': 299, '1968': 300, '1979': 301, 'Albert': 302, 'Ann': 303, 'Central': 304, 'County': 305, 'Cristiano': 306, 'Day': 307, 'Episode': 308, 'Eric': 309, 'French': 310, 'Great': 311, 'Hamilton': 312, 'Harry': 313, 'Islands': 314, 'Jeff': 315, 'Kelly': 316, 'Kim': 317, 'Luke': 318, 'Ronaldo': 319, 'Russia': 320, 'Zealand': 321, 'ago': 322, '!': 323, '1942': 324, '1970': 325, '1985': 326, 'Anna': 327, 'Italy': 328, 'Lewis': 329, 'Manchester': 330, 'Matt': 331, 'Moore': 332, 'Rachel': 333, 'S': 334, 'Simon': 335, 'Thompson': 336, 'Wayne': 337, 'episode': 338, \"'\": 339, '1961': 340, '1975': 341, 'Adams': 342, 'Allen': 343, 'Bonnie': 344, 'Bowl': 345, 'Francis': 346, 'Harrison': 347, 'Jay': 348, 'Jean': 349, 'Jerry': 350, 'Kennedy': 351, 'Pennsylvania': 352, 'Philip': 353, 'Red': 354, 'Total': 355, 'Vegas': 356, 'days': 357, 'four': 358, 'one': 359, 'team': 360, 'that': 361, 'their': 362, '1939': 363, '1960': 364, 'A.': 365, 'Alex': 366, 'Antarctica': 367, 'Ben': 368, 'Blue': 369, 'Christian': 370, 'Cleveland': 371, 'Curry': 372, 'Drew': 373, 'Europe': 374, 'Franklin': 375, 'Gary': 376, 'Gordon': 377, 'Island': 378, 'Jimmy': 379, 'Kansas': 380, 'Las': 381, 'Rose': 382, 'Soviet': 383, 'Stewart': 384, 'Studios': 385, 'Timothy': 386, 'Vancouver': 387, 'east': 388, 'near': 389, 'not': 390, 'other': 391, 'seven': 392, '100': 393, '1776': 394, '1946': 395, '1949': 396, '1965': 397, '1989': 398, 'Alabama': 399, 'Anderson': 400, 'Arthur': 401, 'Atlanta': 402, 'Bell': 403, 'Brian': 404, 'Center': 405, 'Congress': 406, 'Dean': 407, 'Emma': 408, 'English': 409, 'Evans': 410, 'Jefferson': 411, 'Lennon': 412, 'Men': 413, 'Michigan': 414, 'Ocean': 415, 'Queen': 416, 'R.': 417, 'Texas': 418, 'billion': 419, 'end': 420, 'six': 421, 'western': 422, '1951': 423, '1955': 424, '1962': 425, 'Brady': 426, 'Charlie': 427, 'Colorado': 428, 'Columbia': 429, 'Company': 430, 'Jersey': 431, 'Jonathan': 432, 'Minnesota': 433, 'Nathan': 434, 'Neil': 435, 'Ohio': 436, 'Old': 437, 'Pittsburgh': 438, 'five': 439, 'northern': 440, '1908': 441, '1967': 442, '35': 443, '?': 444, 'Amy': 445, 'Ashley': 446, 'Benjamin': 447, 'Bonds': 448, 'C.': 449, 'Dan': 450, 'F.': 451, 'Jon': 452, 'Lynn': 453, 'McCartney': 454, 'Miller': 455, 'No': 456, 'One': 457, 'Pakistan': 458, 'Real': 459, 'Sabha': 460, 'School': 461, 'Southern': 462, 'Victoria': 463, 'W.': 464, 'Western': 465, 'Will': 466, 'You': 467, 'are': 468, 'being': 469, '1620': 470, '1917': 471, '1933': 472, '1950': 473, '1957': 474, '1972': 475, 'Brothers': 476, 'Carter': 477, 'Club': 478, 'Constitution': 479, 'Donald': 480, 'Earl': 481, 'Eddie': 482, 'Fred': 483, 'Jeffrey': 484, 'Keith': 485, 'Korea': 486, 'Kumar': 487, 'Massachusetts': 488, 'National': 489, 'Nicholas': 490, 'Obama': 491, 'Paris': 492, 'Point': 493, 'Randy': 494, 'Robinson': 495, 'Roosevelt': 496, 'Samuel': 497, 'Sarah': 498, 'Seth': 499, 'Sunday': 500, 'T.': 501, 'V.': 502, 'Van': 503, 'Warren': 504, 'v.': 505, 'was': 506, '1914': 507, '1941': 508, '1963': 509, 'BC': 510, 'Band': 511, 'Barack': 512, 'Beach': 513, 'Beatles': 514, 'Brooks': 515, 'Bruce': 516, 'Carl': 517, 'Corporation': 518, 'Craig': 519, 'Daniels': 520, 'Danny': 521, 'Denver': 522, 'Douglas': 523, 'Ed': 524, 'Giants': 525, 'High': 526, 'Hunt': 527, 'Jake': 528, 'Jessica': 529, 'Jodi': 530, 'La': 531, 'M.': 532, 'Madrid': 533, 'Max': 534, 'Mitchell': 535, 'Morgan': 536, 'Mount': 537, 'Muhammad': 538, 'Nelson': 539, 'Northern': 540, 'Packers': 541, 'Pete': 542, 'Rob': 543, 'Rock': 544, 'Ronald': 545, 'Ross': 546, 'Sea': 547, 'Sean': 548, 'Speaker': 549, 'Steven': 550, 'Tracy': 551, 'Virginia': 552, 'Walter': 553, 'an': 554, 'cells': 555, 'central': 556, 'south': 557, 'who': 558, '0': 559, '1920': 560, '1936': 561, '1952': 562, '1958': 563, '50': 564, 'Antonio': 565, 'Black': 566, 'Catherine': 567, 'Chamberlain': 568, 'Charlotte': 569, 'Court': 570, 'Duke': 571, 'E.': 572, 'Elena': 573, 'Field': 574, 'German': 575, 'God': 576, 'H.': 577, 'Hale': 578, 'Hawaii': 579, 'K.': 580, 'Khan': 581, 'L.': 582, 'Lady': 583, 'League': 584, 'Lord': 585, 'Michelle': 586, 'Murray': 587, 'Netherlands': 588, 'Nevada': 589, 'Orleans': 590, 'Perry': 591, 'Philadelphia': 592, 'Pierce': 593, 'Princess': 594, 'Rick': 595, 'Robin': 596, 'Santa': 597, 'Second': 598, 'Senate': 599, 'Street': 600, 'Susan': 601, 'Terry': 602, 'USA': 603, 'Utah': 604, 'Valley': 605, 'Wright': 606, 'first': 607, 'football': 608, 'government': 609, 'group': 610, 'it': 611, 'km': 612, 'over': 613, 'per': 614, 'state': 615, 'weeks': 616, '1912': 617, '1938': 618, '1940': 619, '1953': 620, '1978': 621, '1st': 622, 'Abdul': 623, 'Alaska': 624, 'Amendment': 625, 'Army': 626, 'Arsenal': 627, 'Austin': 628, 'Bennett': 629, 'Burt': 630, 'Bush': 631, 'CBS': 632, 'Captain': 633, 'Caroline': 634, 'Colin': 635, 'Dave': 636, 'Deputy': 637, 'Diego': 638, 'Don': 639, 'Dr.': 640, 'Eastern': 641, 'Egypt': 642, 'Freddie': 643, 'G.': 644, 'Graham': 645, 'Hans': 646, 'Illinois': 647, 'Indian': 648, 'Ivan': 649, 'Jackie': 650, 'Jane': 651, 'Josh': 652, 'Karl': 653, 'Laura': 654, 'Lincoln': 655, 'Lisa': 656, 'Lok': 657, 'Middle': 658, 'Murphy': 659, 'Nick': 660, 'Phil': 661, 'Ram': 662, 'Ricky': 663, 'Roberts': 664, 'Rogers': 665, 'Sun': 666, 'Turner': 667, 'When': 668, 'cell': 669, 'his': 670, 'life': 671, 'national': 672, 'nine': 673, 'people': 674, 'system': 675, 'than': 676, '1783': 677, '1913': 678, '1921': 679, '1927': 680, '1931': 681, '1937': 682, '1948': 683, '1959': 684, '32': 685, '60': 686, 'Alison': 687, 'Allan': 688, 'Argentina': 689, 'Arnold': 690, 'Atlantic': 691, 'Audrey': 692, 'Australian': 693, 'Bailey': 694, 'Barbara': 695, 'Brad': 696, 'Brees': 697, 'Cameron': 698, 'Cole': 699, 'Coleman': 700, 'Cullen': 701, 'D.': 702, 'Dallas': 703, 'Dame': 704, 'Dead': 705, 'Disney': 706, 'Elliott': 707, 'Emily': 708, 'Erin': 709, 'Eva': 710, 'Falcons': 711, 'Ferguson': 712, 'Fisher': 713, 'Fort': 714, 'Frederick': 715, 'Greg': 716, 'Herbert': 717, 'Hill': 718, 'Israel': 719, 'Josef': 720, 'Julius': 721, 'Kane': 722, 'Lauren': 723, 'Lindsay': 724, 'Lloyd': 725, 'Lorraine': 726, 'Mann': 727, 'Margaret': 728, 'Morris': 729, 'Mountains': 730, 'Nancy': 731, 'Newman': 732, 'Newton': 733, 'Peninsula': 734, 'Rebecca': 735, 'Reynolds': 736, 'Seattle': 737, 'Show': 738, 'Steph': 739, 'Stevens': 740, 'Theodore': 741, 'Tigers': 742, 'Tommy': 743, 'Two': 744, 'US': 745, 'Walker': 746, 'Wardell': 747, 'Winter': 748, 'Wood': 749, 'al': 750, 'ft': 751, 'mg': 752, 'miles': 753, 'mm': 754, 'new': 755, 'north': 756, 'second': 757, 'ten': 758, 'which': 759, 'year': 760, '1898': 761, '18th': 762, '1906': 763, '1909': 764, '1911': 765, '1928': 766, '1930': 767, '1935': 768, '1944': 769, '33': 770, '40': 771, 'Air': 772, 'Alexandra': 773, 'Amanda': 774, 'Arena': 775, 'Arizona': 776, 'Astros': 777, 'Avenue': 778, 'Battle': 779, 'Benson': 780, 'Berlin': 781, 'Bloom': 782, 'Boys': 783, 'Britain': 784, 'Bryant': 785, 'Butler': 786, 'Canadian': 787, 'Cape': 788, 'Christmas': 789, 'Clemson': 790, 'Congo': 791, 'Cook': 792, 'Crosby': 793, 'Dale': 794, 'Democratic': 795, 'Dick': 796, 'Earth': 797, 'Ellis': 798, 'F.C.': 799, 'Felix': 800, 'First': 801, 'Forest': 802, 'Gaga': 803, 'Gen': 804, 'Glenn': 805, 'Grant': 806, 'Gregory': 807, 'Griffin': 808, 'Guy': 809, 'Harvey': 810, 'Hugh': 811, 'Hughes': 812, 'Ian': 813, 'Indiana': 814, 'Iran': 815, 'Isaac': 816, 'Jacob': 817, 'Jesus': 818, 'Joel': 819, 'José': 820, 'Judy': 821, 'Justice': 822, 'Kathleen': 823, 'Ken': 824, 'Land': 825, 'Mac': 826, 'Man': 827, 'Maria': 828, 'Memorial': 829, 'Milioti': 830, 'Mosby': 831, 'NFL': 832, 'Nath': 833, 'Nehru': 834, 'P.': 835, 'Parker': 836, 'Parliament': 837, 'People': 838, 'PewDiePie': 839, 'Quinn': 840, 'Ralph': 841, 'Raymond': 842, 'Representatives': 843, 'Rodgers': 844, 'Ron': 845, 'Rooney': 846, 'S.': 847, 'Sebastian': 848, 'Sharma': 849, 'Spencer': 850, 'Summer': 851, 'Tennessee': 852, 'Todd': 853, 'Trump': 854, 'USS': 855, 'Ulf': 856, 'V': 857, 'Victor': 858, 'Yankees': 859, 'Zimmer': 860, 'approximately': 861, 'area': 862, 'ball': 863, 'bone': 864, 'cm': 865, 'during': 866, 'energy': 867, 'family': 868, 'featuring': 869, 'have': 870, 'income': 871, 'la': 872, 'light': 873, 'lower': 874, 'more': 875, 'president': 876, 'right': 877, 'southern': 878, 'states': 879, 'they': 880, 'unknown': 881, 'up': 882, 'usually': 883, 'von': 884, '€': 885, '1775': 886, '1871': 887, '1910': 888, '1925': 889, '1956': 890, '2nd': 891, '34': 892, '36': 893, '45': 894, '48': 895, '54': 896, '56': 897, '70': 898, '82': 899, 'AD': 900, 'AM': 901, 'Ali': 902, 'Alicia': 903, 'Alyson': 904, 'Apostle': 905, 'Ariana': 906, 'Article': 907, 'Aveiro': 908, 'B': 909, 'Bart': 910, 'Bone': 911, 'Boseman': 912, 'Breck': 913, 'Brett': 914, 'Broncos': 915, 'Brooke': 916, 'Bryan': 917, 'Byron': 918, 'C': 919, 'CA': 920, 'CE': 921, 'Carey': 922, 'Carlos': 923, 'Chief': 924, 'Chile': 925, 'Christine': 926, 'Clarke': 927, 'Clive': 928, 'Cornwallis': 929, 'Cowell': 930, 'Cup': 931, 'Dana': 932, 'Dance': 933, 'Denis': 934, 'Depp': 935, 'Dodgers': 936, 'Donna': 937, 'El': 938, 'Empire': 939, 'F': 940, 'Finley': 941, 'Fire': 942, 'Ford': 943, 'Foster': 944, 'Four': 945, 'Gabriel': 946, 'Gold': 947, 'Grace': 948, 'Grand': 949, 'Grande': 950, 'Gray': 951, 'Greek': 952, 'Hammond': 953, 'Harold': 954, 'Hart': 955, 'Hollywood': 956, 'Hussein': 957, 'Inc.': 958, 'Indies': 959, 'J': 960, 'Jamie': 961, 'Javier': 962, 'Jeremy': 963, 'Jill': 964, 'Johansson': 965, 'Julie': 966, 'Kate': 967, 'Keira': 968, 'Kentucky': 969, 'Knightley': 970, 'Kohli': 971, 'Kovind': 972, 'Larry': 973, 'Lawrence': 974, 'LeBron': 975, 'Leonard': 976, 'Little': 977, 'Lou': 978, 'Louise': 979, 'Lucas': 980, 'MacFarlane': 981, 'Master': 982, 'Me': 983, 'Megan': 984, 'Messi': 985, 'Miami': 986, 'Miki': 987, 'Missouri': 988, 'Montgomery': 989, 'Montreal': 990, 'Mrs.': 991, 'Mukherjee': 992, 'Nadal': 993, 'Nigeria': 994, 'Orlando': 995, 'PM': 996, 'Palace': 997, 'Pasek': 998, 'Patti': 999, 'Phillip': 1000, 'Phillips': 1001, 'Plate': 1002, 'Polynesia': 1003, 'Portugal': 1004, 'Preston': 1005, 'Price': 1006, 'Rafael': 1007, 'Rice': 1008, 'Richards': 1009, 'Robertson': 1010, 'Russian': 1011, 'Santos': 1012, 'Sinatra': 1013, 'Sisterhood': 1014, 'Spanish': 1015, 'Sr': 1016, 'St': 1017, 'Stuart': 1018, 'Sudo': 1019, 'Sweden': 1020, 'Tampa': 1021, 'Theatre': 1022, 'Timberlake': 1023, 'Tokyo': 1024, 'Trevor': 1025, 'Turkey': 1026, 'U.S.': 1027, 'Vice': 1028, 'Vincent': 1029, 'Virat': 1030, 'Vladimir': 1031, 'Walt': 1032, 'about': 1033, 'acid': 1034, 'along': 1035, 'be': 1036, 'before': 1037, 'capital': 1038, 'city': 1039, 'coast': 1040, 'dos': 1041, 'eastern': 1042, 'eight': 1043, 'fiction': 1044, 'half': 1045, 'her': 1046, 'into': 1047, 'layer': 1048, 'mid': 1049, 'middle': 1050, 'most': 1051, 'muscle': 1052, 'p.m.': 1053, 'rock': 1054, 'space': 1055, 'tax': 1056, 'time': 1057, 'under': 1058, 'were': 1059, 'white': 1060, '125': 1061, '1801': 1062, '1864': 1063, '1870': 1064, '1892': 1065, '1893': 1066, '1901': 1067, '1903': 1068, '1915': 1069, '1990s': 1070, '3rd': 1071, '41': 1072, '44': 1073, '52': 1074, '5th': 1075, '7th': 1076, '80': 1077, 'Acts': 1078, 'Afghanistan': 1079, 'African': 1080, 'After': 1081, 'Annie': 1082, 'Armstrong': 1083, 'Arvid': 1084, 'BCE': 1085, 'Bacharach': 1086, 'Baldwin': 1087, 'Bank': 1088, 'Banks': 1089, 'Barcelona': 1090, 'Beaver': 1091, 'Benj': 1092, 'Bible': 1093, 'Bican': 1094, 'Boyd': 1095, 'Boyz': 1096, 'Bradford': 1097, 'Bradley': 1098, 'Browns': 1099, 'Bruno': 1100, 'Burton': 1101, 'Campbell': 1102, 'Carmen': 1103, 'Cassidy': 1104, 'Catholic': 1105, 'Chad': 1106, 'Chandra': 1107, 'Chelsea': 1108, 'Chestnut': 1109, 'Christie': 1110, 'Church': 1111, 'Clark': 1112, 'Coast': 1113, 'Coliseum': 1114, 'Commission': 1115, 'Convention': 1116, 'Cooper': 1117, 'Corey': 1118, 'Crawford': 1119, 'Cristin': 1120, 'Crystal': 1121, 'Cummings': 1122, 'D.C.': 1123, 'Danielle': 1124, 'Dark': 1125, 'Davidson': 1126, 'Dee': 1127, 'Dennis': 1128, 'Derek': 1129, 'District': 1130, 'Dorothy': 1131, 'Double': 1132, 'Elephant': 1133, 'Elias': 1134, 'Emperor': 1135, 'Entertainment': 1136, 'Ethiopia': 1137, 'Finland': 1138, 'Force': 1139, 'Frances': 1140, 'Friday': 1141, 'G': 1142, 'Gerry': 1143, 'Girls': 1144, 'Glen': 1145, 'Greece': 1146, 'Greenland': 1147, 'Gulf': 1148, 'Hardy': 1149, 'Hong': 1150, 'IV': 1151, 'Indians': 1152, 'Indonesia': 1153, 'Instagram': 1154, 'Joey': 1155, 'Johannes': 1156, 'Johnston': 1157, 'Juan': 1158, 'Jules': 1159, 'K': 1160, 'Katherine': 1161, 'Kenya': 1162, 'Kings': 1163, 'Kjellberg': 1164, 'Knight': 1165, 'Kong': 1166, 'Kristen': 1167, 'Lane': 1168, 'Lanka': 1169, 'Latin': 1170, 'Lautner': 1171, 'Le': 1172, 'Lieutenant': 1173, 'Lionel': 1174, 'Lopez': 1175, 'Lythgoe': 1176, 'MBE': 1177, 'Madison': 1178, 'Magic': 1179, 'Major': 1180, 'Malone': 1181, 'Manuel': 1182, 'Marcus': 1183, 'Marshall': 1184, 'Martha': 1185, 'Maryland': 1186, 'Massey': 1187, 'Mayer': 1188, 'McConnell': 1189, 'McGregor': 1190, 'Mel': 1191, 'Melissa': 1192, 'Melvin': 1193, 'Mercury': 1194, 'Mississippi': 1195, 'Monica': 1196, 'Motor': 1197, 'NBA': 1198, 'Ned': 1199, 'Nigel': 1200, 'Norman': 1201, 'Oliver': 1202, 'On': 1203, 'POWs': 1204, 'Party': 1205, 'Patel': 1206, 'Patricia': 1207, 'Paula': 1208, 'Penguins': 1209, 'Peters': 1210, 'Philippines': 1211, 'Poland': 1212, 'Porter': 1213, 'Quincy': 1214, 'Rhodes': 1215, 'Rodriguez': 1216, 'Roman': 1217, 'Roy': 1218, 'Salt': 1219, 'Saturday': 1220, 'Scarlett': 1221, 'Serkis': 1222, 'Shearer': 1223, 'Sidney': 1224, 'Singapore': 1225, 'Sri': 1226, 'Stanley': 1227, 'Swift': 1228, 'Sydney': 1229, 'Tina': 1230, 'Up': 1231, 'VI': 1232, 'Versailles': 1233, 'Wales': 1234, 'Wallace': 1235, 'Warburton': 1236, 'Water': 1237, 'Wellington': 1238, 'Willie': 1239, 'Wilt': 1240, 'Winston': 1241, 'around': 1242, 'blood': 1243, 'bones': 1244, 'border': 1245, 'center': 1246, 'control': 1247, 'cricket': 1248, 'each': 1249, 'ends': 1250, 'g': 1251, 'game': 1252, 'heart': 1253, 'inches': 1254, 'leader': 1255, 'left': 1256, 'members': 1257, 'monarchy': 1258, 'name': 1259, 'novel': 1260, 'off': 1261, 'old': 1262, 'part': 1263, 'players': 1264, 'republic': 1265, 'small': 1266, 'social': 1267, 'son': 1268, 'southeastern': 1269, 'surface': 1270, 'tea': 1271, 'tendon': 1272, 'third': 1273, 'united': 1274, 'when': 1275, 'where': 1276, 'within': 1277, '+': 1278, '1,000': 1279, '10,000': 1280, '100,000': 1281, '1000': 1282, '11th': 1283, '1789': 1284, '1792': 1285, '1853': 1286, '1857': 1287, '1907': 1288, '1916': 1289, '1918': 1290, '1922': 1291, '1924': 1292, '1932': 1293, '1943': 1294, '1970s': 1295, '19th': 1296, '38': 1297, '4th': 1298, '55': 1299, '65': 1300, 'Aisha': 1301, 'Andre': 1302, 'Arkansas': 1303, 'Attlee': 1304, 'Ball': 1305, 'Bangladesh': 1306, 'Beijing': 1307, 'Belgium': 1308, 'Belinda': 1309, 'Betty': 1310, 'Bolton': 1311, 'Bon': 1312, 'Book': 1313, 'Brandon': 1314, 'Brock': 1315, 'Buffalo': 1316, 'Burns': 1317, 'Burrell': 1318, 'Cardinals': 1319, 'Caribbean': 1320, 'Carol': 1321, 'Carpenter': 1322, 'Carson': 1323, 'Castro': 1324, 'Celtics': 1325, 'Chapman': 1326, 'Chase': 1327, 'Cheryl': 1328, 'Chopra': 1329, 'Christ': 1330, 'Chuck': 1331, 'Clement': 1332, 'College': 1333, 'Collins': 1334, 'Colombia': 1335, 'Connecticut': 1336, 'Cooke': 1337, 'Coolidge': 1338, 'Council': 1339, 'Creek': 1340, 'Crimson': 1341, 'Curtis': 1342, 'DC': 1343, 'Dakota': 1344, 'Davey': 1345, 'Dawn': 1346, 'Do': 1347, 'Dome': 1348, 'Drake': 1349, 'Dutch': 1350, 'Dwight': 1351, 'Edinburgh': 1352, 'Edition': 1353, 'Elaine': 1354, 'Elba': 1355, 'Elvis': 1356, 'End': 1357, 'Ernest': 1358, 'Ernie': 1359, 'Ethel': 1360, 'Everett': 1361, 'Exposition': 1362, 'FIFA': 1363, 'Fast': 1364, 'Faye': 1365, 'Fe': 1366, 'Federer': 1367, 'Felton': 1368, 'Ferdinand': 1369, 'Ferrari': 1370, 'Fiona': 1371, 'Flag': 1372, 'Florence': 1373, 'Floyd': 1374, 'Football': 1375, 'Free': 1376, 'Freeman': 1377, 'Furious': 1378, 'Gabrielle': 1379, 'Gardens': 1380, 'Gates': 1381, 'Gene': 1382, 'Genesis': 1383, 'Giblin': 1384, 'Gilbert': 1385, 'Girl': 1386, 'Golden': 1387, 'Gospel': 1388, 'Gosselaar': 1389, 'Government': 1390, 'Griffith': 1391, 'Group': 1392, 'H': 1393, 'Hal': 1394, 'Haley': 1395, 'Hastings': 1396, 'Health': 1397, 'Helen': 1398, 'Henderson': 1399, 'Hodges': 1400, 'Holland': 1401, 'Hopkins': 1402, 'Howie': 1403, 'Hunter': 1404, 'Idina': 1405, 'In': 1406, 'Ira': 1407, 'Iraq': 1408, 'Ireland': 1409, 'Irons': 1410, 'Irving': 1411, 'Isaiah': 1412, 'Jabbar': 1413, 'Janet': 1414, 'Jared': 1415, 'Jennings': 1416, 'Jeremiah': 1417, 'Joanne': 1418, 'Johann': 1419, 'Johnstone': 1420, 'Joshua': 1421, 'Jude': 1422, 'Just': 1423, 'Kareem': 1424, 'Karen': 1425, 'Kathryn': 1426, 'Kerala': 1427, 'Kerr': 1428, 'Key': 1429, 'Kris': 1430, 'Kurt': 1431, 'Lakers': 1432, 'Lambert': 1433, 'Late': 1434, 'Legend': 1435, 'Leigh': 1436, 'Leo': 1437, 'Lester': 1438, 'Life': 1439, 'Linda': 1440, 'Louisiana': 1441, 'Love': 1442, 'Lucy': 1443, 'Lusail': 1444, 'Luther': 1445, 'Marc': 1446, 'Mars': 1447, 'Marty': 1448, 'Marvin': 1449, 'McDonald': 1450, 'Meghan': 1451, 'Menken': 1452, 'Menzel': 1453, 'Meredith': 1454, 'Miles': 1455, 'Minister': 1456, 'Mohammad': 1457, 'Montana': 1458, 'Moody': 1459, 'Morocco': 1460, 'Moss': 1461, 'Mountain': 1462, 'Mountbatten': 1463, 'Mr.': 1464, 'Network': 1465, 'Nixon': 1466, 'Nolan': 1467, \"O'Donoghue\": 1468, 'Oak': 1469, 'Oakland': 1470, 'Oklahoma': 1471, 'Oregon': 1472, 'Otis': 1473, 'Owen': 1474, 'Paige': 1475, 'Parissi': 1476, 'Pasadena': 1477, 'Patil': 1478, 'Pizarro': 1479, 'Plan': 1480, 'Pleasant': 1481, 'Powell': 1482, 'Pranab': 1483, 'Prasad': 1484, 'Pratibha': 1485, 'Quebec': 1486, 'Rabbit': 1487, 'Raja': 1488, 'Raleigh': 1489, 'Ramirez': 1490, 'Rayyan': 1491, 'Reddy': 1492, 'Reeves': 1493, 'Resort': 1494, 'Richardson': 1495, 'Ridge': 1496, 'Rights': 1497, 'Rita': 1498, 'Rockies': 1499, 'Rodríguez': 1500, 'Romano': 1501, 'Round': 1502, 'Royal': 1503, 'Rupert': 1504, 'Russo': 1505, 'Sabella': 1506, 'Scotland': 1507, 'Seeger': 1508, 'Serena': 1509, 'Shadows': 1510, 'Sheeran': 1511, 'Shepard': 1512, 'Shrek': 1513, 'Sierra': 1514, 'Simmons': 1515, 'Sonny': 1516, 'Southwest': 1517, 'Spring': 1518, 'Stan': 1519, 'Stanton': 1520, 'Star': 1521, 'Starr': 1522, 'Steelers': 1523, 'Sterling': 1524, 'Streisand': 1525, 'Strong': 1526, 'Sudan': 1527, 'Sullivan': 1528, 'Sweet': 1529, 'Taiwan': 1530, 'Team': 1531, 'Testament': 1532, 'Third': 1533, 'Tide': 1534, 'Tiger': 1535, 'Tigress': 1536, 'Time': 1537, 'Toni': 1538, 'Town': 1539, 'Toyota': 1540, 'Travis': 1541, 'Vanessa': 1542, 'Vikings': 1543, 'Watson': 1544, 'Welch': 1545, 'Westbrook': 1546, 'Whitney': 1547, 'Who': 1548, 'Wind': 1549, 'Wisconsin': 1550, 'Woods': 1551, 'back': 1552, 'black': 1553, 'body': 1554, 'but': 1555, 'c.': 1556, 'car': 1557, 'copper': 1558, 'core': 1559, 'current': 1560, 'da': 1561, 'different': 1562, 'do': 1563, 'du': 1564, 'economic': 1565, 'episodes': 1566, 'every': 1567, 'fantasy': 1568, 'feet': 1569, 'forces': 1570, 'hour': 1571, 'hours': 1572, 'human': 1573, 'interest': 1574, 'its': 1575, 'joint': 1576, 'large': 1577, 'last': 1578, 'like': 1579, 'm': 1580, 'mL': 1581, 'manchester': 1582, 'membrane': 1583, 'memory': 1584, 'metal': 1585, 'named': 1586, 'neglect': 1587, 'northeastern': 1588, 'number': 1589, 'oil': 1590, 'percent': 1591, 'present': 1592, 'presidential': 1593, 'regions': 1594, 'required': 1595, 'sales': 1596, 'school': 1597, 'self': 1598, 'series': 1599, 'southwestern': 1600, 'still': 1601, 'unnamed': 1602, 'various': 1603, 'veins': 1604, 'world': 1605, '×': 1606, '#': 1607, '104': 1608, '105': 1609, '108': 1610, '12th': 1611, '131': 1612, '137': 1613, '13:55': 1614, '14th': 1615, '15th': 1616, '1707': 1617, '1793': 1618, '1798': 1619, '1834': 1620, '1836': 1621, '1861': 1622, '1889': 1623, '1895': 1624, '1900': 1625, '1904': 1626, '1923': 1627, '1929': 1628, '1934': 1629, '1:13': 1630, '1⁄2': 1631, '2021': 1632, '39': 1633, '42': 1634, '43': 1635, '47': 1636, '500': 1637, '53': 1638, '6:3': 1639, '78': 1640, '7:3': 1641, '81': 1642, '84': 1643, '8:00': 1644, '90': 1645, '9:00': 1646, ';': 1647, 'AC': 1648, 'ATP': 1649, 'AZ': 1650, 'Abagnale': 1651, 'Abraham': 1652, 'Airlines': 1653, 'Alberta': 1654, 'Alden': 1655, 'Alfred': 1656, 'Algeria': 1657, 'All': 1658, 'Amell': 1659, 'Anaheim': 1660, 'Angel': 1661, 'Angela': 1662, 'Anglo': 1663, 'Angola': 1664, 'Ansari': 1665, 'Antarctic': 1666, 'Apple': 1667, 'Art': 1668, 'Assembly': 1669, 'Associated': 1670, 'Avonlea': 1671, 'BBC': 1672, 'Bahamas': 1673, 'Barbra': 1674, 'Barker': 1675, 'Baron': 1676, 'Barrett': 1677, 'Beatty': 1678, 'Beauregard': 1679, 'Before': 1680, 'Belfast': 1681, 'Benedict': 1682, 'Benny': 1683, 'Bernard': 1684, 'Best': 1685, 'Beth': 1686, 'Beyoncé': 1687, 'Bhattacharya': 1688, 'Big': 1689, 'Birmingham': 1690, 'Blake': 1691, 'Bonham': 1692, 'Bowman': 1693, 'Boy': 1694, 'Bracco': 1695, 'Brenda': 1696, 'Brent': 1697, 'Bridges': 1698, 'Brodeur': 1699, 'Brody': 1700, 'Brolin': 1701, 'Buccaneers': 1702, 'Buck': 1703, 'Bulgaria': 1704, 'Bulldogs': 1705, 'Caesar': 1706, 'Caleb': 1707, 'Canadiens': 1708, 'Carnes': 1709, 'Carroll': 1710, 'Casey': 1711, 'Casino': 1712, 'Century': 1713, 'Chairman': 1714, 'Charleston': 1715, 'Cher': 1716, 'Cherry': 1717, 'Chiefs': 1718, 'Claire': 1719, 'Clapton': 1720, 'Clause': 1721, 'Clayton': 1722, 'Cliff': 1723, 'Clinton': 1724, 'Cobb': 1725, 'Cody': 1726, 'Cohen': 1727, 'Columbus': 1728, 'Confederate': 1729, 'Conrad': 1730, 'Cornelius': 1731, 'Cox': 1732, 'Cravalho': 1733, 'Cruz': 1734, 'Cubs': 1735, 'Cynthia': 1736, 'Cyrus': 1737, 'D': 1738, 'Damon': 1739, 'Debbie': 1740, 'Deep': 1741, 'Devine': 1742, 'Devisingh': 1743, 'Dhawan': 1744, 'Diane': 1745, 'Djokovic': 1746, 'Doctor': 1747, 'Doug': 1748, 'Dragon': 1749, 'Drive': 1750, 'Early': 1751, 'Edwards': 1752, 'Edwin': 1753, 'Edy': 1754, 'Eileen': 1755, 'Elisabeth': 1756, 'Ellen': 1757, 'Emmanuel': 1758, 'Erik': 1759, 'Evan': 1760, 'Eve': 1761, 'Ewan': 1762, 'Excel': 1763, 'Exodus': 1764, 'Family': 1765, 'Federal': 1766, 'Ferrell': 1767, 'Fiennes': 1768, 'Finch': 1769, 'Flower': 1770, 'Flowers': 1771, 'Flynn': 1772, 'Follows': 1773, 'Fourth': 1774, 'Fox': 1775, 'Frey': 1776, 'Friedrich': 1777, 'Fuchur': 1778, 'Galilei': 1779, 'Galileo': 1780, 'Garbiñe': 1781, 'Gardner': 1782, 'Garland': 1783, 'Garuda': 1784, 'Gayle': 1785, 'Geoff': 1786, 'Gerard': 1787, 'Gilmour': 1788, 'Glasgow': 1789, 'Goffin': 1790, 'Gomez': 1791, 'Goodman': 1792, 'Greatest': 1793, 'Grey': 1794, 'Hamid': 1795, 'Hanging': 1796, 'Hanks': 1797, 'Hannah': 1798, 'Hannibal': 1799, 'Hannigan': 1800, 'Harbor': 1801, 'Hard': 1802, 'Harden': 1803, 'Hasdrubal': 1804, 'Hawkins': 1805, 'Hayley': 1806, 'Heather': 1807, 'Hernandez': 1808, 'Hero': 1809, 'Hirohito': 1810, 'Holloway': 1811, 'Hope': 1812, 'Hotel': 1813, 'Hudgens': 1814, 'Hudson': 1815, 'Hugo': 1816, 'Hungary': 1817, 'Hutchinson': 1818, 'Hybrid': 1819, 'Iceland': 1820, 'Industrial': 1821, 'Inn': 1822, 'Iqbal': 1823, 'Irish': 1824, 'Italian': 1825, 'Jade': 1826, 'Jagger': 1827, 'Jenny': 1828, 'Jo': 1829, 'Judah': 1830, 'Judith': 1831, 'Jupiter': 1832, 'Kenny': 1833, 'Kirk': 1834, 'Kristofferson': 1835, 'Kyla': 1836, 'Kyle': 1837, 'L': 1838, 'L4': 1839, 'Lacey': 1840, 'Lamar': 1841, 'Larson': 1842, 'Last': 1843, 'Law': 1844, 'Lesley': 1845, 'Leslie': 1846, 'Levi': 1847, 'Liberty': 1848, 'Libya': 1849, 'Lily': 1850, 'Lindsey': 1851, 'Lion': 1852, 'Lori': 1853, 'Ltd': 1854, 'Lynne': 1855, 'López': 1856, 'Maggie': 1857, 'Mandel': 1858, 'Mane': 1859, 'Marco': 1860, 'Marquis': 1861, 'Martina': 1862, 'Marzorati': 1863, 'Mason': 1864, 'Maurice': 1865, 'McCann': 1866, 'McCoy': 1867, 'Mediterranean': 1868, 'Members': 1869, 'Mick': 1870, 'Mills': 1871, 'Minneapolis': 1872, 'Minutes': 1873, 'Miranda': 1874, 'Molly': 1875, 'Momsen': 1876, 'Morrison': 1877, 'Moscow': 1878, 'Mother': 1879, 'Muguruza': 1880, 'Mulvaney': 1881, 'Music': 1882, 'Myers': 1883, 'NC': 1884, 'Nasim': 1885, 'Natalie': 1886, 'Nathaniel': 1887, 'Nazareth': 1888, 'Nichols': 1889, 'Nicole': 1890, 'Niger': 1891, 'Nina': 1892, 'Noah': 1893, 'Noel': 1894, 'Noelle': 1895, 'None': 1896, 'Normandy': 1897, 'Norway': 1898, 'Novak': 1899, 'O': 1900, \"O'Connor\": 1901, \"O'Mara\": 1902, 'Olivia': 1903, 'Orange': 1904, 'Oscar': 1905, 'Ottoman': 1906, 'PST': 1907, 'Pace': 1908, 'Palmer': 1909, 'Panama': 1910, 'Pants': 1911, 'Parsons': 1912, 'Pat': 1913, 'Patty': 1914, 'Payne': 1915, 'Pepper': 1916, 'Petty': 1917, 'Phelps': 1918, 'Philbin': 1919, 'Phoebe': 1920, 'Pierre': 1921, 'Potts': 1922, 'Power': 1923, 'Pradesh': 1924, 'Pres': 1925, 'Prime': 1926, 'Priscilla': 1927, 'Puerto': 1928, 'Qatar': 1929, 'Queensland': 1930, 'Raiders': 1931, 'Rajaji': 1932, 'Rajya': 1933, 'Rams': 1934, 'Range': 1935, 'Reagan': 1936, 'Redding': 1937, 'Reese': 1938, 'Rene': 1939, 'Revolution': 1940, 'Rex': 1941, 'Richie': 1942, 'Rihanna': 1943, 'Ringo': 1944, 'Rohit': 1945, 'Rome': 1946, 'Rooker': 1947, 'Royals': 1948, 'Rush': 1949, 'Russians': 1950, 'SUV': 1951, 'Sadler': 1952, 'Sally': 1953, 'Samantha': 1954, 'Sanderson': 1955, 'Sands': 1956, 'Sara': 1957, 'Sarandon': 1958, 'Schmit': 1959, 'Schon': 1960, 'Selena': 1961, 'Series': 1962, 'Seven': 1963, 'Seymour': 1964, 'Shane': 1965, 'Sharon': 1966, 'Shaw': 1967, 'Shirley': 1968, 'Singers': 1969, 'Snow': 1970, 'Solomon': 1971, 'Sony': 1972, 'Sophie': 1973, 'Soul': 1974, 'Southeastern': 1975, 'Sox': 1976, 'Spieth': 1977, 'Spotlight': 1978, 'Stacy': 1979, 'Staff': 1980, 'Stars': 1981, 'Station': 1982, 'Stevie': 1983, 'Stone': 1984, 'Stop': 1985, 'Storm': 1986, 'Sunil': 1987, 'Superior': 1988, 'Supreme': 1989, 'Sutton': 1990, 'Suzanne': 1991, 'Switzerland': 1992, 'Syracuse': 1993, 'Tanzania': 1994, 'Tatsuya': 1995, 'Tell': 1996, 'Territory': 1997, 'Thailand': 1998, 'They': 1999, 'Thirteen': 2000, 'This': 2001, 'Thomason': 2002, 'Three': 2003, 'Thunder': 2004, 'Titus': 2005, 'Townsend': 2006, 'Trainor': 2007, 'Traveling': 2008, 'Troy': 2009, 'U2': 2010, 'USC': 2011, 'Urban': 2012, 'VIII': 2013, 'Val': 2014, 'Vaughn': 2015, 'Vettel': 2016, 'Village': 2017, 'Villanova': 2018, 'Wall': 2019, 'Ward': 2020, 'We': 2021, 'Wendy': 2022, 'Wheeler': 2023, 'Wilhelm': 2024, 'Willis': 2025, 'Witch': 2026, 'Wonder': 2027, 'Wyoming': 2028, 'X': 2029, 'XXVI': 2030, 'XXXIX': 2031, 'Years': 2032, 'YouTube': 2033, 'Zaidi': 2034, 'Zoe': 2035, 'a.m.': 2036, 'act': 2037, 'all': 2038, 'areas': 2039, 'army': 2040, 'below': 2041, 'birth': 2042, 'board': 2043, 'both': 2044, 'boundary': 2045, 'cancer': 2046, 'chemical': 2047, 'children': 2048, 'chloride': 2049, 'colon': 2050, 'common': 2051, 'constitutional': 2052, 'court': 2053, 'day': 2054, 'di': 2055, 'ear': 2056, 'eruptions': 2057, 'executive': 2058, 'federal': 2059, 'fifty': 2060, 'finale': 2061, 'formula': 2062, 'found': 2063, 'front': 2064, 'garden': 2065, 'growth': 2066, 'h': 2067, 'had': 2068, 'has': 2069, 'head': 2070, 'help': 2071, 'including': 2072, 'intestine': 2073, 'le': 2074, 'league': 2075, 'legislative': 2076, 'less': 2077, 'long': 2078, 'lungs': 2079, 'majority': 2080, 'meaning': 2081, 'mi': 2082, 'molecular': 2083, 'natural': 2084, 'no': 2085, 'non': 2086, 'now': 2087, 'original': 2088, 'oxygen': 2089, 'parts': 2090, 'period': 2091, 'phase': 2092, 'plate': 2093, 'plates': 2094, 'pm': 2095, 'political': 2096, 'proximal': 2097, 'rangers': 2098, 'rate': 2099, 'red': 2100, 's': 2101, 'sex': 2102, 'shaped': 2103, 'single': 2104, 'southeast': 2105, 'summer': 2106, 'teams': 2107, 'tectonic': 2108, 'territory': 2109, 'tissue': 2110, 'top': 2111, 'town': 2112, 'twelve': 2113, 'twice': 2114, 'union': 2115, 'used': 2116, 'van': 2117, 'very': 2118, 'video': 2119, 'war': 2120, 'water': 2121, 'way': 2122, 'west': 2123, 'you': 2124, 'đồng': 2125, \"'ll\": 2126, '08742': 2127, '1.5': 2128, '103.1': 2129, '106': 2130, '122': 2131, '123': 2132, '124': 2133, '130': 2134, '133': 2135, '136': 2136, '147': 2137, '1492': 2138, '150': 2139, '1560': 2140, '1765': 2141, '1781': 2142, '1790': 2143, '17th': 2144, '1800': 2145, '1835': 2146, '1838': 2147, '1845': 2148, '1847': 2149, '1858': 2150, '186': 2151, '1863': 2152, '1867': 2153, '1876': 2154, '1878': 2155, '1880': 2156, '1882': 2157, '1899': 2158, '1902': 2159, '1926': 2160, '1930s': 2161, '1950s': 2162, '1960s': 2163, '1980s': 2164, '2,000': 2165, '2000s': 2166, '214': 2167, '250': 2168, '258': 2169, '270': 2170, '300,000': 2171, '311': 2172, '370': 2173, '4.5': 2174, '417': 2175, '435': 2176, '441': 2177, '46': 2178, '49': 2179, '597': 2180, '5:00': 2181, '63': 2182, '68': 2183, '69': 2184, '6th': 2185, '71': 2186, '72': 2187, '73': 2188, '9th': 2189, '@BarackObama': 2190, '@instagram': 2191, 'Abigail': 2192, 'Act': 2193, 'Adler': 2194, 'Agency': 2195, 'Agent': 2196, 'Akuna': 2197, 'Alec': 2198, 'Ambedkar': 2199, 'Amsterdam': 2200, 'Anastasia': 2201, 'Andersson': 2202, 'Andrews': 2203, 'Annette': 2204, 'Annika': 2205, 'Antioch': 2206, 'Antoine': 2207, 'Archibald': 2208, 'Arnett': 2209, 'Arsène': 2210, 'Artie': 2211, 'Arya': 2212, 'Ash': 2213, 'Athena': 2214, 'Atlas': 2215, 'Atticus': 2216, 'Auburn': 2217, 'Ava': 2218, 'Avengers': 2219, 'Baahubali': 2220, 'Baker': 2221, 'Baltimore': 2222, 'Banerjee': 2223, 'Barnabas': 2224, 'Barnes': 2225, 'Barrington': 2226, 'Batman': 2227, 'Bear': 2228, 'Bears': 2229, 'Beaumont': 2230, 'Bella': 2231, 'Bengal': 2232, 'Bhaskar': 2233, 'Bhimrao': 2234, 'Bi': 2235, 'Billie': 2236, 'Bills': 2237, 'Blackburn': 2238, 'Blewett': 2239, 'Blood': 2240, 'Blunt': 2241, 'Books': 2242, 'Borneo': 2243, 'Bragg': 2244, 'Braxton': 2245, 'Brendan': 2246, 'Bridge': 2247, 'Brightman': 2248, 'Brijwasi': 2249, 'Bristol': 2250, 'Brittany': 2251, 'Bronson': 2252, 'Bryson': 2253, 'Buchanan': 2254, 'Buckingham': 2255, 'Burbank': 2256, 'Burch': 2257, 'Burgess': 2258, 'Call': 2259, 'Cambridge': 2260, 'Camp': 2261, 'Candice': 2262, 'Cannavale': 2263, 'Canning': 2264, 'Capaldi': 2265, 'Carla': 2266, 'Carly': 2267, 'Carney': 2268, 'Carole': 2269, 'Case': 2270, 'Cash': 2271, 'Cat': 2272, 'Cathy': 2273, 'Cena': 2274, 'Chabert': 2275, 'Chambers': 2276, 'Chandler': 2277, 'Chapter': 2278, 'Chester': 2279, 'Chip': 2280, 'Chiwetel': 2281, 'Chloe': 2282, 'Chori': 2283, 'Christianity': 2284, 'Christina': 2285, 'Christy': 2286, 'Churchill': 2287, 'Clare': 2288, 'Clarence': 2289, 'Clarkson': 2290, 'Claude': 2291, 'Cleary': 2292, 'Clifton': 2293, 'Clint': 2294, 'Clooney': 2295, 'Coe': 2296, 'Collingwood': 2297, 'Colonies': 2298, 'Commercial': 2299, 'Community': 2300, 'Conference': 2301, 'Connick': 2302, 'Conroy': 2303, 'Constituent': 2304, 'Corbin': 2305, 'Cornwall': 2306, 'Costa': 2307, 'Cotton': 2308, 'Coulson': 2309, 'Country': 2310, 'Course': 2311, 'Courtney': 2312, 'Cree': 2313, 'Cropper': 2314, 'Cross': 2315, 'Crown': 2316, 'Crows': 2317, 'Cry': 2318, 'Cuba': 2319, 'Cunningham': 2320, 'Cyril': 2321, 'Czech': 2322, 'Daily': 2323, 'Dalton': 2324, 'Daltrey': 2325, 'Darling': 2326, 'DeAndre': 2327, 'Declaration': 2328, 'Delano': 2329, 'Dempsey': 2330, 'Department': 2331, 'Desai': 2332, 'Desert': 2333, 'Despacito': 2334, 'Detroit': 2335, 'Devon': 2336, 'Dewey': 2337, 'Dexter': 2338, 'Di': 2339, 'Diana': 2340, 'Diary': 2341, 'Diggy': 2342, 'Dillane': 2343, 'Dion': 2344, 'Dixon': 2345, 'Dmitri': 2346, 'Doha': 2347, 'Dollar': 2348, 'Dolly': 2349, 'Dominic': 2350, 'Donny': 2351, 'Donovan': 2352, 'Doves': 2353, 'Downtown': 2354, 'Dr': 2355, 'Dragons': 2356, 'Drummond': 2357, 'Dubois': 2358, 'Dudamel': 2359, 'During': 2360, 'Dyke': 2361, 'Düsseldorf': 2362, 'EST': 2363, 'Ebina': 2364, 'Eddy': 2365, 'Edelman': 2366, 'Eden': 2367, 'Edmund': 2368, 'Efron': 2369, 'Ejiofor': 2370, 'Eldredge': 2371, 'Elton': 2372, 'Emmitt': 2373, 'Energy': 2374, 'Enterprise': 2375, 'Enterprises': 2376, 'Estonia': 2377, 'Eugene': 2378, 'European': 2379, 'Evelyn': 2380, 'Everly': 2381, 'Ezekiel': 2382, 'Fair': 2383, 'Fairley': 2384, 'Falls': 2385, 'Fanning': 2386, 'Feast': 2387, 'Fernandez': 2388, 'Fernández': 2389, 'Fifth': 2390, 'Film': 2391, 'Final': 2392, 'Financial': 2393, 'Fitzgerald': 2394, 'Five': 2395, 'Flack': 2396, 'Fleetwood': 2397, 'Flora': 2398, 'Fonsi': 2399, 'Fontaine': 2400, 'Fontana': 2401, 'Foray': 2402, 'Forks': 2403, 'Forsythe': 2404, 'Fowler': 2405, 'Freed': 2406, 'Friend': 2407, 'Fritz': 2408, 'Fry': 2409, 'Fujiwara': 2410, 'Full': 2411, 'Gad': 2412, 'Gale': 2413, 'Gambon': 2414, 'Games': 2415, 'Gandhi': 2416, 'Garcia': 2417, 'Garth': 2418, 'Gaye': 2419, 'Geneva': 2420, 'Genevieve': 2421, 'Georges': 2422, 'Gerald': 2423, 'Germain': 2424, 'Ghana': 2425, 'Gleason': 2426, 'Gloria': 2427, 'Go': 2428, 'Godfrey': 2429, 'Gore': 2430, 'Greenwood': 2431, 'Guess': 2432, 'Gun': 2433, 'Gunther': 2434, 'Gustavo': 2435, 'Gutenberg': 2436, 'Haiduk': 2437, 'Halliwell': 2438, 'Halloween': 2439, 'Hamill': 2440, 'Hamish': 2441, 'Han': 2442, 'Hank': 2443, 'Hanyu': 2444, 'Harper': 2445, 'Haslam': 2446, 'Hathaway': 2447, 'Hayden': 2448, 'Hebrew': 2449, 'Heder': 2450, 'Heinrich': 2451, 'Helena': 2452, 'Hemant': 2453, 'Hemisphere': 2454, 'Henley': 2455, 'Henri': 2456, 'Herb': 2457, 'Herman': 2458, 'Heroes': 2459, 'Hertz': 2460, 'Highmore': 2461, 'Hiroshima': 2462, 'History': 2463, 'Hobbs': 2464, 'Holdings': 2465, 'Holliday': 2466, 'Holly': 2467, 'Holt': 2468, 'Home': 2469, 'Hoover': 2470, 'Hoskins': 2471, 'Hoyer': 2472, 'Huang': 2473, 'Hurricane': 2474, 'Hurst': 2475, 'Hurwitz': 2476, 'Hussain': 2477, 'Hyderabad': 2478, 'Iain': 2479, 'Idrissa': 2480, 'If': 2481, 'Ike': 2482, 'Indira': 2483, 'Internet': 2484, 'Inverness': 2485, 'Iowa': 2486, 'Irene': 2487, 'Isle': 2488, \"Ja'net\": 2489, 'Jackman': 2490, 'Jacques': 2491, 'Javits': 2492, 'Jenkins': 2493, 'Jericho': 2494, 'Jets': 2495, 'Jews': 2496, 'Joan': 2497, 'Jonah': 2498, 'Jonas': 2499, 'Jorge': 2500, 'Jose': 2501, 'Joshi': 2502, 'Journey': 2503, 'Jovi': 2504, 'Joy': 2505, 'Joyce': 2506, 'Judaism': 2507, 'Judd': 2508, 'Judge': 2509, 'Julia': 2510, 'Kani': 2511, 'Katy': 2512, 'Keller': 2513, 'Kemp': 2514, 'Kendall': 2515, 'Kendrick': 2516, 'Khor': 2517, 'Khrushchev': 2518, 'Kilmer': 2519, 'Kinnaman': 2520, 'Kirshner': 2521, 'Kirsten': 2522, 'Kishan': 2523, 'Klaus': 2524, 'Klementieff': 2525, 'Knowles': 2526, 'Krauss': 2527, 'Kroll': 2528, 'Kunwar': 2529, 'L5': 2530, 'Lafayette': 2531, 'Lambs': 2532, 'Larkin': 2533, 'Lausanne': 2534, 'Lawson': 2535, 'Leah': 2536, 'Leonardo': 2537, 'Lerner': 2538, 'Leto': 2539, 'Letters': 2540, 'Leviticus': 2541, 'Liam': 2542, 'Lightning': 2543, 'Lilly': 2544, 'Limited': 2545, 'Link': 2546, 'Linnaeus': 2547, 'Liprandi': 2548, 'Liverpool': 2549, 'Long': 2550, 'Lonnie': 2551, 'Loren': 2552, 'Lost': 2553, 'Lot': 2554, 'Louvin': 2555, 'Ltd.': 2556, 'Luca': 2557, 'Luxembourg': 2558, 'Maccabees': 2559, 'Mackenzie': 2560, 'Madden': 2561, 'Maddie': 2562, 'Mae': 2563, 'Main': 2564, 'Mali': 2565, 'Mall': 2566, 'Mandela': 2567, 'Manfred': 2568, 'Manning': 2569, 'Manny': 2570, 'Manoj': 2571, 'Marching': 2572, 'Mariana': 2573, 'Marina': 2574, 'Marius': 2575, 'Marjorie': 2576, 'Martini': 2577, 'Mascolo': 2578, 'Masterson': 2579, 'Mastiff': 2580, 'Matula': 2581, 'Maureen': 2582, 'Mayweather': 2583, 'McGee': 2584, 'McGrath': 2585, 'McKinley': 2586, 'Meisner': 2587, 'Melanie': 2588, 'Melbourne': 2589, 'Mendeleev': 2590, 'Merckx': 2591, 'Mercy': 2592, 'Merry': 2593, 'Metropolitan': 2594, 'Mets': 2595, 'Michel': 2596, 'Mildred': 2597, 'Mirza': 2598, 'Missy': 2599, 'Mobile': 2600, 'Monsters': 2601, 'Moran': 2602, 'Morton': 2603, 'Moseley': 2604, 'Motorsports': 2605, 'Mozambique': 2606, 'Mumbai': 2607, 'Museum': 2608, 'My': 2609, 'NJ': 2610, 'Nadu': 2611, 'Naomi': 2612, 'Napoleon': 2613, 'Nat': 2614, 'Nations': 2615, 'Nayyar': 2616, 'Nebraska': 2617, 'Newcastle': 2618, 'Norwich': 2619, 'Not': 2620, 'Noth': 2621, 'Numbers': 2622, \"O'Brien\": 2623, \"O'Loughlin\": 2624, 'O2': 2625, 'OBE': 2626, 'OU': 2627, 'Ontario': 2628, 'Orchestra': 2629, 'Orthodox': 2630, 'Osborne': 2631, 'Osmond': 2632, 'Owens': 2633, 'Oz': 2634, 'Pacey': 2635, 'Page': 2636, 'Panthers': 2637, 'Parrish': 2638, 'Parton': 2639, 'Patton': 2640, 'Peck': 2641, 'Pedro': 2642, 'Pelosi': 2643, 'Perlman': 2644, 'Persian': 2645, 'Petersburg': 2646, 'Phoenix': 2647, 'Pictures': 2648, 'Pini': 2649, 'Piper': 2650, 'Pirates': 2651, 'Pitt': 2652, 'Placid': 2653, 'Poitier': 2654, 'Polk': 2655, 'Pollard': 2656, 'Pope': 2657, 'Povenmire': 2658, 'Powers': 2659, 'Pozzo': 2660, 'Pratt': 2661, 'Prepon': 2662, 'Presley': 2663, 'Priyanka': 2664, 'Proctor': 2665, 'Psychologists': 2666, 'Ptolemy': 2667, 'Pujols': 2668, 'Punjab': 2669, 'Putin': 2670, 'Qin': 2671, 'RAF': 2672, 'Rahman': 2673, 'Rahul': 2674, 'Rai': 2675, 'Ramji': 2676, 'Ramsey': 2677, 'Ranch': 2678, 'Randall': 2679, 'Rangers': 2680, 'Raphael': 2681, 'Rashida': 2682, 'Ratzenberger': 2683, 'Rear': 2684, 'Redden': 2685, 'Regina': 2686, 'Regional': 2687, 'Return': 2688, 'Ricardo': 2689, 'Rich': 2690, 'Richmond': 2691, 'Rickman': 2692, 'Rico': 2693, 'Riley': 2694, 'Ring': 2695, 'Ripka': 2696, 'Robbie': 2697, 'Robbins': 2698, 'Robby': 2699, 'Roberta': 2700, 'Roberto': 2701, 'Robyn': 2702, 'Rogen': 2703, 'Rollins': 2704, 'Romeo': 2705, 'Ronnie': 2706, 'Rory': 2707, 'Rouge': 2708, 'Rouse': 2709, 'Sachin': 2710, 'Sacramento': 2711, 'Salem': 2712, 'Sammy': 2713, 'Sampson': 2714, 'Sandra': 2715, 'Sanjay': 2716, 'Sankt': 2717, 'Saudi': 2718, 'Saul': 2719, 'Save': 2720, 'Scalise': 2721, 'Schumer': 2722, 'Schwartz': 2723, 'Seal': 2724, 'Seasons': 2725, 'Seaview': 2726, 'Secaucus': 2727, 'See': 2728, 'Seely': 2729, 'Sehwag': 2730, 'Serafinowicz': 2731, 'Sergeant': 2732, 'Sergio': 2733, 'Sex': 2734, 'Shada': 2735, 'Shah': 2736, 'Shannon': 2737, 'Shawn': 2738, 'Shelby': 2739, 'Sheldon': 2740, 'Sheng': 2741, 'Sherman': 2742, 'Sherwood': 2743, 'Shetty': 2744, 'Shi': 2745, 'Shield': 2746, 'Shukla': 2747, 'Silence': 2748, 'Simone': 2749, 'Sisters': 2750, 'Sixth': 2751, 'Siza': 2752, 'Skarsgård': 2753, 'Skinner': 2754, 'Slade': 2755, 'Sobolov': 2756, 'Sports': 2757, 'Springfield': 2758, 'Spurs': 2759, 'Stafford': 2760, 'Steele': 2761, 'Steny': 2762, 'Stephens': 2763, 'Stevenson': 2764, 'Stiller': 2765, 'Stonie': 2766, 'Strange': 2767, 'Stranger': 2768, 'Strauss': 2769, 'Studio': 2770, 'Susanna': 2771, 'T': 2772, 'Taj': 2773, 'Tales': 2774, 'Tamil': 2775, 'Teddy': 2776, 'Teller': 2777, 'Temple': 2778, 'Ten': 2779, 'Tendulkar': 2780, 'Tenochtitlan': 2781, 'Terrence': 2782, 'Terrier': 2783, 'There': 2784, 'Theresa': 2785, 'Thornton': 2786, 'Thursday': 2787, 'Tibbets': 2788, 'Tides': 2789, 'Tiffin': 2790, 'Times': 2791, 'Tisdale': 2792, 'Tlacopan': 2793, 'Toby': 2794, 'Toronto': 2795, 'Trade': 2796, 'Tropicana': 2797, 'Truffula': 2798, 'Truman': 2799, 'Trust': 2800, 'Tunisia': 2801, 'Tuscany': 2802, 'Twenty': 2803, 'Uganda': 2804, 'Unexpected': 2805, 'Uruguay': 2806, 'Vallabhbhai': 2807, 'VanderWaal': 2808, 'Venkata': 2809, 'Vermont': 2810, 'Verne': 2811, 'Vinci': 2812, 'Virender': 2813, 'Visa': 2814, 'Viscount': 2815, 'Vittorio': 2816, 'Voyager': 2817, 'Vyas': 2818, 'Wade': 2819, 'Wagner': 2820, 'Wakrah': 2821, 'Walking': 2822, 'Wally': 2823, 'Wang': 2824, 'Wardrobe': 2825, 'Warner': 2826, 'Warnes': 2827, 'Warwick': 2828, 'Wednesday': 2829, 'Weiss': 2830, 'Wenger': 2831, 'Wild': 2832, 'Wildcats': 2833, 'Wilkins': 2834, 'Winds': 2835, 'Work': 2836, 'Wyatt': 2837, 'Yates': 2838, 'Year': 2839, 'Yondu': 2840, 'Yoon': 2841, 'Yorkshire': 2842, 'Yung': 2843, 'Yuzuru': 2844, 'Yvonne': 2845, 'above': 2846, 'action': 2847, 'against': 2848, 'age': 2849, 'air': 2850, 'anterior': 2851, 'apple': 2852, 'band': 2853, 'bank': 2854, 'began': 2855, 'belt': 2856, 'best': 2857, 'business': 2858, 'card': 2859, 'certain': 2860, 'chart': 2861, 'choir': 2862, 'citizens': 2863, 'class': 2864, 'clear': 2865, 'close': 2866, 'club': 2867, 'coaches': 2868, 'coal': 2869, 'communication': 2870, 'company': 2871, 'convert': 2872, 'courts': 2873, 'crossing': 2874, 'd': 2875, 'dark': 2876, 'data': 2877, 'dermis': 2878, 'des': 2879, 'died': 2880, 'direct': 2881, 'directly': 2882, 'down': 2883, 'drama': 2884, 'drive': 2885, 'driver': 2886, 'dry': 2887, 'dynasty': 2888, 'eagle': 2889, 'earth': 2890, 'elected': 2891, 'electron': 2892, 'eleven': 2893, 'ended': 2894, 'farm': 2895, 'father': 2896, 'fertilization': 2897, 'film': 2898, 'fire': 2899, 'flat': 2900, 'following': 2901, 'food': 2902, 'force': 2903, 'formation': 2904, 'freedom': 2905, 'gas': 2906, 'glands': 2907, 'globe': 2908, 'good': 2909, 'gospel': 2910, 'governments': 2911, 'governor': 2912, 'groups': 2913, 'high': 2914, 'house': 2915, 'hyun': 2916, 'iOS': 2917, 'independent': 2918, 'infant': 2919, 'international': 2920, 'iron': 2921, 'island': 2922, 'islands': 2923, 'just': 2924, 'kilometres': 2925, 'land': 2926, 'language': 2927, 'lateral': 2928, 'least': 2929, 'levels': 2930, 'limit': 2931, 'limited': 2932, 'line': 2933, 'living': 2934, 'love': 2935, 'made': 2936, 'management': 2937, 'many': 2938, 'match': 2939, 'may': 2940, 'member': 2941, 'membranes': 2942, 'military': 2943, 'mine': 2944, 'model': 2945, 'months': 2946, 'mosque': 2947, 'motor': 2948, 'mouth': 2949, 'movement': 2950, 'movements': 2951, 'mph': 2952, 'n': 2953, \"n't\": 2954, 'nearly': 2955, 'nervous': 2956, 'network': 2957, 'never': 2958, 'nitrogen': 2959, 'none': 2960, 'northwestern': 2961, 'nucleus': 2962, 'née': 2963, 'ocean': 2964, 'office': 2965, 'official': 2966, 'older': 2967, 'once': 2968, 'origin': 2969, 'outside': 2970, 'page': 2971, 'pancreas': 2972, 'party': 2973, 'person': 2974, 'player': 2975, 'playoffs': 2976, 'point': 2977, 'post': 2978, 'pounds': 2979, 'power': 2980, 'prevent': 2981, 'price': 2982, 'prior': 2983, 'process': 2984, 'property': 2985, 'qualify': 2986, 'radio': 2987, 'raja': 2988, 'region': 2989, 'religion': 2990, 'removed': 2991, 'reverse': 2992, 'rib': 2993, 'rights': 2994, 'river': 2995, 'salamander': 2996, 'same': 2997, 'secondary': 2998, 'seconds': 2999, 'secretary': 3000, 'several': 3001, 'site': 3002, 'sixty': 3003, 'size': 3004, 'skeleton': 3005, 'skin': 3006, 'slip': 3007, 'smaller': 3008, 'smooth': 3009, 'sodium': 3010, 'some': 3011, 'sometimes': 3012, 'speaker': 3013, 'species': 3014, 'student': 3015, 'suburb': 3016, 'sugar': 3017, 'support': 3018, 'symbol': 3019, 'tail': 3020, 'television': 3021, 'test': 3022, 'through': 3023, 'times': 3024, 'tissues': 3025, 'toward': 3026, 'tree': 3027, 'uterus': 3028, 'visible': 3029, 'volcano': 3030, 'well': 3031, 'wheel': 3032, 'women': 3033, 'word': 3034, 'work': 3035, 'young': 3036, 'younger': 3037, '≥': 3038, \"'re\": 3039, '0.4': 3040, '0.5': 3041, '0.928': 3042, '01': 3043, '03': 3044, '06': 3045, '06:00': 3046, '1.050': 3047, '1/2': 3048, '10.3.3': 3049, '10:00': 3050, '10th': 3051, '113': 3052, '118': 3053, '119': 3054, '12,714': 3055, '12,756.3': 3056, '120': 3057, '126': 3058, '127': 3059, '129': 3060, '12:00': 3061, '134': 3062, '135': 3063, '139': 3064, '140': 3065, '141': 3066, '143': 3067, '144': 3068, '145': 3069, '1500': 3070, '151': 3071, '1526': 3072, '1549': 3073, '155': 3074, '156': 3075, '159': 3076, '160': 3077, '1600': 3078, '1602': 3079, '162': 3080, '164': 3081, '167': 3082, '1689': 3083, '169': 3084, '1700': 3085, '1705': 3086, '1706': 3087, '1721': 3088, '173': 3089, '1746': 3090, '175.5': 3091, '1763': 3092, '1769': 3093, '1773': 3094, '1784': 3095, '1787': 3096, '1788': 3097, '1794': 3098, '180': 3099, '1804': 3100, '1805': 3101, '1815': 3102, '1828': 3103, '1837': 3104, '1848': 3105, '1850': 3106, '1862': 3107, '1869': 3108, '1873': 3109, '1879': 3110, '188': 3111, '1884': 3112, '1885': 3113, '1890': 3114, '1894': 3115, '1905': 3116, '1920s': 3117, '1994–95': 3118, '2,620': 3119, '200': 3120, '2016–17': 3121, '2017–18': 3122, '202': 3123, '2020s': 3124, '2028': 3125, '2029': 3126, '2030': 3127, '2040s': 3128, '206': 3129, '207': 3130, '20:00': 3131, '212': 3132, '22nd': 3133, '22–26': 3134, '23.2': 3135, '231': 3136, '237': 3137, '24th': 3138, '276': 3139, '280': 3140, '3.5': 3141, '300': 3142, '304': 3143, '321': 3144, '328': 3145, '330': 3146, '336': 3147, '338': 3148, '350,000': 3149, '3:1': 3150, '4-door': 3151, '400,000': 3152, '410': 3153, '49ers': 3154, '5.8': 3155, '50,000': 3156, '53.5': 3157, '543': 3158, '550': 3159, '570': 3160, '59': 3161, '599': 3162, '5:20': 3163, '6.5': 3164, '600,000': 3165, '61': 3166, '61.0': 3167, '64': 3168, '66': 3169, '6:31': 3170, '7.1.2': 3171, '7.25': 3172, '700': 3173, '704': 3174, '75': 3175, '76.3': 3176, '79': 3177, '7:12': 3178, '80,000': 3179, '87': 3180, '8th': 3181, '9.3.5': 3182, '91.2': 3183, '95': 3184, '96': 3185, '97': 3186, '99': 3187, 'AOC': 3188, 'ATV': 3189, 'Abbott': 3190, 'Aberdeen': 3191, 'Aboud': 3192, 'Abu': 3193, 'According': 3194, 'Aces': 3195, 'Acker': 3196, 'Adcock': 3197, 'Addison': 3198, 'Additions': 3199, 'Adelaide': 3200, 'Adele': 3201, 'Admiral': 3202, 'Adolph': 3203, 'Adrian': 3204, 'Aduwak': 3205, 'Advanced': 3206, 'Affleck': 3207, 'Afton': 3208, 'Again': 3209, 'Ages': 3210, 'Agrawal': 3211, 'Agüero': 3212, 'Ahmed': 3213, 'Ainsworth': 3214, 'Airplanes': 3215, 'Airways': 3216, 'Alam': 3217, 'Alaric': 3218, 'Alastair': 3219, 'Albany': 3220, 'Alecia': 3221, 'Alice': 3222, 'Allied': 3223, 'Allison': 3224, 'Allman': 3225, 'Alone': 3226, 'Aloysius': 3227, 'Alps': 3228, 'Altuve': 3229, 'Amaravati': 3230, 'Amartya': 3231, 'Amelia': 3232, 'Americans': 3233, 'Americas': 3234, 'An': 3235, 'Andersen': 3236, 'Andrea': 3237, 'Andreas': 3238, 'André': 3239, 'Andrés': 3240, 'Anika': 3241, 'Anil': 3242, 'Anita': 3243, 'Annapolis': 3244, 'Ant': 3245, 'Anterior': 3246, 'Antoinette': 3247, 'Antonin': 3248, 'Antonoff': 3249, 'Apollos': 3250, 'Appalachian': 3251, 'Approximately': 3252, 'Arabia': 3253, 'Arabian': 3254, 'Arabic': 3255, 'Arabs': 3256, 'Archduke': 3257, 'Archimedes': 3258, 'Area': 3259, 'Aria': 3260, 'Arlington': 3261, 'Arquette': 3262, 'Arterton': 3263, 'Articles': 3264, 'Arun': 3265, 'Asa': 3266, 'Ashly': 3267, 'Ashok': 3268, 'Ashton': 3269, 'Assam': 3270, 'Assistant': 3271, 'Asteroid': 3272, 'Atal': 3273, 'Atandwa': 3274, 'Athens': 3275, 'Atkins': 3276, 'Atomic': 3277, 'Attic': 3278, 'Attorney': 3279, 'Atwood': 3280, 'Auerbach': 3281, 'Augustine': 3282, 'Aurangzeb': 3283, 'Austria': 3284, 'Authors': 3285, 'Award': 3286, 'Axton': 3287, 'Azerbaijan': 3288, 'Aztecs': 3289, 'B.F.': 3290, 'B.I.G.': 3291, 'Baby': 3292, 'Bachman': 3293, 'Back': 3294, 'Badale': 3295, 'Bader': 3296, 'Baffert': 3297, 'Bagdasarian': 3298, 'Bagley': 3299, 'Bahadur': 3300, 'Bahama': 3301, 'Bald': 3302, 'Banking': 3303, 'Banner': 3304, 'Bannister': 3305, 'Barca': 3306, 'Barden': 3307, 'Barkley': 3308, 'Barks': 3309, 'Barreto': 3310, 'Barton': 3311, 'Baruch': 3312, 'Base': 3313, 'Baseball': 3314, 'Basil': 3315, 'Bastille': 3316, 'Bautista': 3317, 'Bavaria': 3318, 'Baxter': 3319, 'Bayes': 3320, 'Bayt': 3321, 'Be': 3322, 'Beast': 3323, 'Beat': 3324, 'Beavers': 3325, 'Bebe': 3326, 'Began': 3327, 'Belafonte': 3328, 'Belichick': 3329, 'Bellinger': 3330, 'Below': 3331, 'Bender': 3332, 'Bengali': 3333, 'Beni': 3334, 'Benkirane': 3335, 'Bennington': 3336, 'Benoist': 3337, 'Bergen': 3338, 'Berger': 3339, 'Bergin': 3340, 'Bergman': 3341, 'Bernardino': 3342, 'Berry': 3343, 'Bert': 3344, 'Bessemer': 3345, 'Bett': 3346, 'Beulah': 3347, 'Bhide': 3348, 'Bhutan': 3349, 'Bieber': 3350, 'Bihari': 3351, 'Bilson': 3352, 'Bird': 3353, 'Birtwhistle': 3354, 'Bishop': 3355, 'Björn': 3356, 'Blackwood': 3357, 'Blaine': 3358, 'Bland': 3359, 'Blondes': 3360, 'Bloomington': 3361, 'Blues': 3362, 'Bo': 3363, 'Bock': 3364, 'Boente': 3365, 'Boeser': 3366, 'Boggess': 3367, 'Bonaparte': 3368, 'Booke': 3369, 'Bordeaux': 3370, 'Borden': 3371, 'Botswana': 3372, 'Boulevard': 3373, 'Bounty': 3374, 'Bourne': 3375, 'Bournemouth': 3376, 'Boxer': 3377, 'Boyle': 3378, 'Bradbury': 3379, 'Brahma': 3380, 'Brake': 3381, 'Brave': 3382, 'Breda': 3383, 'Bregman': 3384, 'Bridget': 3385, 'Bridgewater': 3386, 'Britney': 3387, 'Britton': 3388, 'Broadway': 3389, 'Bronco': 3390, 'Bronze': 3391, 'Brook': 3392, 'Brookman': 3393, 'Brosnan': 3394, 'Browning': 3395, 'Brunswick': 3396, 'Bryce': 3397, 'Brynn': 3398, 'Buddy': 3399, 'Building': 3400, 'Bulldog': 3401, 'Bullock': 3402, 'Burdon': 3403, 'Burma': 3404, 'Burnett': 3405, 'Burrafato': 3406, 'Buster': 3407, 'Buu': 3408, 'Buzz': 3409, 'C.S.C.': 3410, 'CBE': 3411, 'CK': 3412, 'CSS': 3413, 'CT': 3414, 'Cabot': 3415, 'Cadillac': 3416, 'Cafe': 3417, 'Cage': 3418, 'Caitlin': 3419, 'Calderón': 3420, 'Calgary': 3421, 'Calvin': 3422, 'Camille': 3423, 'Canal': 3424, 'Cara': 3425, 'Cariappa': 3426, 'Carli': 3427, 'Carlin': 3428, 'Carmichael': 3429, 'Carolyn': 3430, 'Carrie': 3431, 'Cars': 3432, 'Carthage': 3433, 'Cary': 3434, 'Castle': 3435, 'Category': 3436, 'Cates': 3437, 'Cathleen': 3438, 'Catskill': 3439, 'Cattell': 3440, 'Cavaliers': 3441, 'Caviezel': 3442, 'Cay': 3443, 'Cayman': 3444, 'Cecil': 3445, 'Cedric': 3446, 'Celine': 3447, 'Chadwick': 3448, 'Challenger': 3449, 'Chamberlin': 3450, 'Chambler': 3451, 'Chamling': 3452, 'Chamonix': 3453, 'Championship': 3454, 'Chan': 3455, 'Chancourtois': 3456, 'Chandan': 3457, 'Change': 3458, 'Channing': 3459, 'Charity': 3460, 'Charli': 3461, 'Charlize': 3462, 'Charter': 3463, 'Chas': 3464, 'Cheap': 3465, 'Check': 3466, 'Chennai': 3467, 'Cheswick': 3468, 'Chhote': 3469, 'Child': 3470, 'Childress': 3471, 'Chilton': 3472, 'Chlumsky': 3473, 'Choir': 3474, 'Choo': 3475, 'Christians': 3476, 'Chukwu': 3477, 'Ciara': 3478, 'Circle': 3479, 'Civil': 3480, 'Civilization': 3481, 'Clara': 3482, 'Claudia': 3483, 'Clearwater': 3484, 'Clegane': 3485, 'Clergy': 3486, 'Clifford': 3487, 'Cloris': 3488, 'Cloud': 3489, 'Clyde': 3490, 'Co.': 3491, 'Cobie': 3492, 'Cocker': 3493, 'Cod': 3494, 'Coghlan': 3495, 'Cohenour': 3496, 'Cold': 3497, 'Coldplay': 3498, 'Colleen': 3499, 'Colman': 3500, 'Colonel': 3501, 'Columbian': 3502, 'Combs': 3503, 'Comics': 3504, 'Commissioner': 3505, 'Commons': 3506, 'Compact': 3507, 'Complex': 3508, 'Computer': 3509, 'Conclusion': 3510, 'Concord': 3511, 'Confederation': 3512, 'Connery': 3513, 'Connie': 3514, 'Connors': 3515, 'Constance': 3516, 'Control': 3517, 'Coots': 3518, 'Corbett': 3519, 'Corinne': 3520, 'Cottrell': 3521, 'Count': 3522, 'Countess': 3523, 'Couple': 3524, 'Cove': 3525, 'Coyote': 3526, 'Creed': 3527, 'Creedence': 3528, 'Crescent': 3529, 'Cretan': 3530, 'Crews': 3531, 'Criminal': 3532, 'Croce': 3533, 'Crocker': 3534, 'Crooked': 3535, 'Croton': 3536, 'Crouch': 3537, 'Crowley': 3538, 'Cruise': 3539, 'Cuccittini': 3540, 'Culver': 3541, 'Curt': 3542, 'Cy': 3543, 'Cyrene': 3544, 'Czechoslavak': 3545, 'César': 3546, \"D'Arcy\": 3547, 'DEFCON': 3548, 'DJ': 3549, 'DVD': 3550, 'Daddario': 3551, 'Daddy': 3552, 'Daei': 3553, 'Dahlia': 3554, 'Daisy': 3555, 'Dane': 3556, 'Darci': 3557, 'Darin': 3558, 'Darrell': 3559, 'Dartmouth': 3560, 'Darwin': 3561, 'Daryl': 3562, 'Davide': 3563, 'Davies': 3564, 'De': 3565, 'DeHaan': 3566, 'DeShannon': 3567, 'Deal': 3568, 'Dearborn': 3569, 'DeathValley': 3570, 'Deborah': 3571, 'Declan': 3572, 'Def': 3573, 'Defense': 3574, 'Delaware': 3575, 'Delhi': 3576, 'Delta': 3577, 'Denise': 3578, 'Denmark': 3579, 'Derby': 3580, 'Derbyshire': 3581, 'Derry': 3582, 'Derulo': 3583, 'Desmond': 3584, 'Destiny': 3585, 'Deuteronomy': 3586, 'Devall': 3587, 'Deventer': 3588, 'Devils': 3589, 'Diamond': 3590, 'Dias': 3591, 'Did': 3592, 'Dido': 3593, 'Diggins': 3594, 'Dil': 3595, 'Dimas': 3596, 'Dingle': 3597, 'Dire': 3598, 'Disneyland': 3599, 'Divas': 3600, 'Division': 3601, 'Dixie': 3602, 'Djawadi': 3603, 'Dodger': 3604, 'Dodig': 3605, 'Doetinchem': 3606, 'Doina': 3607, 'Dolls': 3608, 'Donnie': 3609, 'Doris': 3610, 'Dortmund': 3611, 'Dove': 3612, 'Downey': 3613, 'Downing': 3614, 'Draper': 3615, 'Dravid': 3616, 'Draw': 3617, 'Driscoll': 3618, 'Dryzek': 3619, 'Dub': 3620, 'Dublin': 3621, 'Dumezweni': 3622, 'Dunaway': 3623, 'Duncan': 3624, 'Dunne': 3625, 'Dunst': 3626, 'Duritz': 3627, 'Duty': 3628, 'Dwan': 3629, 'EJ': 3630, 'ET': 3631, 'Eagles': 3632, 'Eamonn': 3633, 'Earle': 3634, 'Earvin': 3635, 'Eaton': 3636, 'Eccleston': 3637, 'Edd': 3638, 'Edge': 3639, 'Edison': 3640, 'Edith': 3641, 'Eduardo': 3642, 'Education': 3643, 'Edwardian': 3644, 'Egyptian': 3645, 'Ehrenreich': 3646, 'Einziger': 3647, 'Ekaterina': 3648, 'Electric': 3649, 'Eleventh': 3650, 'Elfman': 3651, 'Eli': 3652, 'Elijah': 3653, 'Elimination': 3654, 'Eliza': 3655, 'Elle': 3656, 'Elliot': 3657, 'Elsa': 3658, 'Ely': 3659, 'Emden': 3660, 'Emerson': 3661, 'Emil': 3662, 'Emilio': 3663, 'Enlightenment': 3664, 'Enschede': 3665, 'Environmental': 3666, 'Ephron': 3667, 'Eratosthenes': 3668, 'Erika': 3669, 'Eritrea': 3670, 'Ernst': 3671, 'Essam': 3672, 'Estate': 3673, 'Estates': 3674, 'Estelle': 3675, 'Estes': 3676, 'Eswatini': 3677, 'Euless': 3678, 'Euro': 3679, 'Europeans': 3680, 'Evansville': 3681, 'Everton': 3682, 'Evgenia': 3683, 'Executive': 3684, 'Exosphere': 3685, 'FA': 3686, 'FM': 3687, 'Fagerbakke': 3688, 'Farley': 3689, 'Faroe': 3690, 'Farrell': 3691, 'Fata': 3692, 'Fatt': 3693, 'Feb': 3694, 'Felder': 3695, 'Ferlin': 3696, 'Ferrigno': 3697, 'Fertile': 3698, 'Fifi': 3699, 'Fillion': 3700, 'Finals': 3701, 'Finn': 3702, 'Fischer': 3703, 'Flatow': 3704, 'Flemyng': 3705, 'Flores': 3706, 'Fogerty': 3707, 'Fold': 3708, 'Foley': 3709, 'Foote': 3710, 'Foreman': 3711, 'Forever': 3712, 'Fork': 3713, 'Forms': 3714, 'Forrest': 3715, 'Forty': 3716, 'Fraley': 3717, 'Francesca': 3718, 'Franco': 3719, 'Frankie': 3720, 'Franks': 3721, 'Franz': 3722, 'Fray': 3723, 'Freedom': 3724, 'Freedoms': 3725, 'From': 3726, 'Front': 3727, 'Fuller': 3728, 'G1': 3729, 'GW': 3730, 'Gadot': 3731, 'Gaelic': 3732, 'Gage': 3733, 'Gagnon': 3734, 'Gail': 3735, 'Gaiman': 3736, 'Gaines': 3737, 'Gal': 3738, 'Galatians': 3739, 'Galecki': 3740, 'Galloway': 3741, 'Game': 3742, 'Ganesh': 3743, 'Gang': 3744, 'Gangwon': 3745, 'García': 3746, 'Garden': 3747, 'Gareth': 3748, 'Garfield': 3749, 'Gatewood': 3750, 'Gehlot': 3751, 'Gen.': 3752, 'Genny': 3753, 'Gensfleisch': 3754, 'Gentlemen': 3755, 'Geoffrey': 3756, 'Gere': 3757, 'Gershwin': 3758, 'Get': 3759, 'Ghost': 3760, 'Giancarlo': 3761, 'Gibbons': 3762, 'Gielgud': 3763, 'Gil': 3764, 'Giles': 3765, 'Gillan': 3766, 'Gillespie': 3767, 'Gilmore': 3768, 'Gina': 3769, 'Glacier': 3770, 'Gladys': 3771, 'Gloucestershire': 3772, 'Glover': 3773, 'Goggins': 3774, 'Goings': 3775, 'Goldberg': 3776, 'Gooch': 3777, 'Gorge': 3778, 'Gospels': 3779, 'Governor': 3780, 'Governorate': 3781, 'Grammer': 3782, 'Grandma': 3783, 'Graves': 3784, 'Greenbush': 3785, 'Greene': 3786, 'Gregg': 3787, 'Gretzky': 3788, 'Grimes': 3789, 'Grossman': 3790, 'Grounds': 3791, 'Grove': 3792, 'Grover': 3793, 'Grumpy': 3794, 'Guerrero': 3795, 'Guffey': 3796, 'Guglielmo': 3797, 'Gujarat': 3798, 'Gwen': 3799, 'HS': 3800, 'Ha': 3801, 'Haakon': 3802, 'Hackett': 3803, 'Hadary': 3804, 'Hagen': 3805, 'Hahn': 3806, 'Half': 3807, 'Halliday': 3808, 'Hallyday': 3809, 'Hamburg': 3810, 'Hanno': 3811, 'Hansen': 3812, 'Hanspat': 3813, 'Hardin': 3814, 'Harding': 3815, 'Hare': 3816, 'Harmon': 3817, 'Harney': 3818, 'Harnick': 3819, 'Harrell': 3820, 'Harrington': 3821, 'Hartwell': 3822, 'Hassan': 3823, 'Hausa': 3824, 'Havana': 3825, 'Haye': 3826, 'Hayes': 3827, 'Hayward': 3828, 'Hazlewood': 3829, 'Head': 3830, 'Heart': 3831, 'Heathcliff': 3832, 'Heels': 3833, 'Heidi': 3834, 'Helberg': 3835, 'Helensvale': 3836, 'Helms': 3837, 'Henriques': 3838, 'Hepburn': 3839, 'Hernando': 3840, 'Hertfordshire': 3841, 'Hewson': 3842, 'Hieroglyphs': 3843, 'Hillary': 3844, 'Hillis': 3845, 'Himalaya': 3846, 'Himalayas': 3847, 'Hingis': 3848, 'Historical': 3849, 'Hits': 3850, 'Hoffman': 3851, 'Hoffs': 3852, 'Holden': 3853, 'Hollande': 3854, 'Holy': 3855, 'Hood': 3856, 'Hopper': 3857, 'Horton': 3858, 'Hospital': 3859, 'Hoss': 3860, 'Hotels': 3861, 'Houghton': 3862, 'How': 3863, 'Howe': 3864, 'Howell': 3865, 'Hoyt': 3866, 'Hubert': 3867, 'Huddleston': 3868, 'Huey': 3869, 'Humboldt': 3870, 'Humphrey': 3871, 'Hunting': 3872, 'Hurt': 3873, 'Husky': 3874, 'Hutton': 3875, 'INXS': 3876, 'IST': 3877, 'IX': 3878, 'IZ': 3879, 'Ibrahim': 3880, 'Ibrahimović': 3881, 'Ice': 3882, 'Iconic': 3883, 'Imperial': 3884, 'Indus': 3885, 'Information': 3886, 'Inglewood': 3887, 'Ingrid': 3888, 'Insurance': 3889, 'Inva': 3890, 'Investments': 3891, 'Iron': 3892, 'Ironsides': 3893, 'Irwin': 3894, 'Isabel': 3895, 'Isabella': 3896, 'Islamic': 3897, 'Isley': 3898, 'Istanbul': 3899, 'It': 3900, 'Ivory': 3901, 'J.W.': 3902, 'JR': 3903, 'Jacksonville': 3904, 'Jaden': 3905, 'Jain': 3906, 'Jamal': 3907, 'Jammu': 3908, 'Jan': 3909, 'Janeiro': 3910, 'Janoub': 3911, 'Japanese': 3912, 'Jarasandha': 3913, 'Jardine': 3914, 'Jarvis': 3915, 'Jasmin': 3916, 'Jasmine': 3917, 'Jawaharlal': 3918, 'Jeanne': 3919, 'Jedi': 3920, 'Jenna': 3921, 'Jennie': 3922, 'Jerkins': 3923, 'Jerrold': 3924, 'Jesse': 3925, 'Jewel': 3926, 'Jewish': 3927, 'Ji': 3928, 'Joachim': 3929, 'Jobs': 3930, 'Johan': 3931, 'Johannesburg': 3932, 'Johnathan': 3933, 'Jong': 3934, 'Jorrel': 3935, 'Journal': 3936, 'Jr': 3937, 'Julienne': 3938, 'Kagan': 3939, 'Kaitlyn': 3940, 'Kalu': 3941, \"Kamakawiwo'ole\": 3942, 'Kamalesvaran': 3943, 'Kaplan': 3944, 'Kapoor': 3945, 'Kashmir': 3946, 'Kat': 3947, 'Katharine': 3948, 'Kathy': 3949, 'Katung': 3950, 'Kaul': 3951, 'Kaur': 3952, 'Kavanaugh': 3953, 'Kayla': 3954, 'Kazakhstan': 3955, 'Keating': 3956, 'Keenum': 3957, 'Kei': 3958, 'Kelsey': 3959, 'Kenichi': 3960, 'Kepler': 3961, 'Kerry': 3962, 'Kesha': 3963, 'Khalifa': 3964, 'Kida': 3965, 'Kilroy': 3966, 'Kimber': 3967, 'Kimberly': 3968, 'Kingsley': 3969, 'Kingston': 3970, 'Kinkle': 3971, 'Kinnear': 3972, 'Kiran': 3973, 'Kirby': 3974, 'Kiribati': 3975, 'Kline': 3976, 'Klose': 3977, 'Knauff': 3978, 'Koblizkova': 3979, 'Konstantin': 3980, 'Korsakov': 3981, 'Kossoy': 3982, 'Kramer': 3983, 'Kristien': 3984, 'Kristin': 3985, 'Krzyzewski': 3986, 'Kunal': 3987, 'Kuwait': 3988, 'Kyl': 3989, 'Kylo': 3990, 'Kyrkjebo': 3991, 'L&M': 3992, 'L.A.': 3993, 'L3': 3994, 'LaShawn': 3995, 'Labor': 3996, 'Ladakh': 3997, 'Laden': 3998, 'Laker': 3999, 'Lakes': 4000, 'Lalong': 4001, 'Lamb': 4002, 'Lamentations': 4003, 'Lana': 4004, 'Lancashire': 4005, 'Lange': 4006, 'Language': 4007, 'Lapkus': 4008, 'Lara': 4009, 'Latvia': 4010, 'Laurent': 4011, 'Lawes': 4012, 'Lea': 4013, 'Leachman': 4014, 'Leal': 4015, 'Leary': 4016, 'Leavitt': 4017, 'Lebanon': 4018, 'Leeza': 4019, 'Leiber': 4020, 'Leiper': 4021, 'Leipzig': 4022, 'Lenin': 4023, 'Lennox': 4024, 'Lenny': 4025, 'Leon': 4026, 'Leoni': 4027, 'Lepiato': 4028, 'Leppard': 4029, 'Lesnar': 4030, 'Lettermen': 4031, 'Letty': 4032, 'Levine': 4033, 'Lex': 4034, 'Lexington': 4035, 'Li': 4036, 'Liberal': 4037, 'Lightfoot': 4038, 'Lightyear': 4039, 'Limestone': 4040, 'Limit': 4041, 'Lin': 4042, 'Linc': 4043, 'Linden': 4044, 'Linear': 4045, 'Lions': 4046, 'Lisbon': 4047, 'Lithgow': 4048, 'Lithuania': 4049, 'Living': 4050, 'Livingston': 4051, 'Liza': 4052, 'Loaf': 4053, 'Lonely': 4054, 'Longstreet': 4055, 'Lucinda': 4056, 'Ludacris': 4057, 'Ludwig': 4058, 'Luis': 4059, 'Lydia': 4060, 'Lyn': 4061, 'Lyndon': 4062, 'Lyon': 4063, 'Léon': 4064, 'M': 4065, 'MC': 4066, 'Ma': 4067, 'MacArthur': 4068, 'MacDonald': 4069, 'MacLeod': 4070, 'Maciej': 4071, 'Macron': 4072, 'Madagascar': 4073, 'Maddux': 4074, 'Madeline': 4075, 'Mahatma': 4076, 'Maitland': 4077, 'Maj': 4078, 'Majin': 4079, 'Majorino': 4080, 'Majority': 4081, 'Makarova': 4082, 'Mako': 4083, 'Malala': 4084, 'Malcolm': 4085, 'Maldives': 4086, 'Mallard': 4087, 'Mallorca': 4088, 'Mamas': 4089, 'Mamta': 4090, 'Manalapan': 4091, 'Manekshaw': 4092, 'Manila': 4093, 'Manmohan': 4094, 'Mansion': 4095, 'Mara': 4096, 'Marais': 4097, 'Marcel': 4098, 'Marcelo': 4099, 'Marconi': 4100, 'Mare': 4101, 'Margo': 4102, 'Margot': 4103, 'Mariah': 4104, 'Marilyn': 4105, 'Mariyappan': 4106, 'Marks': 4107, 'Marla': 4108, 'Marlo': 4109, 'Marni': 4110, 'Marquess': 4111, 'Marseille': 4112, 'Marsha': 4113, 'Marshal': 4114, 'Marta': 4115, 'Martinez': 4116, 'Martín': 4117, 'Marx': 4118, 'Masala': 4119, 'Mastrantonio': 4120, 'Mathai': 4121, 'Mathews': 4122, 'Matijevic': 4123, 'Mattel': 4124, 'Matthews': 4125, 'Matthey': 4126, 'Matthias': 4127, 'Maudsland': 4128, 'Mauna': 4129, 'Mavalankar': 4130, 'Maximilien': 4131, 'Maximus': 4132, 'Maxwell': 4133, 'Maya': 4134, 'Mbasogo': 4135, 'Mbatha': 4136, 'McCarthy': 4137, 'McDermott': 4138, 'McDorman': 4139, 'McEntire': 4140, 'McEwen': 4141, 'McFly': 4142, 'McGuire': 4143, 'McIntosh': 4144, 'McKenzie': 4145, 'McLean': 4146, 'McMahon': 4147, 'McQueen': 4148, 'McVie': 4149, 'Meat': 4150, 'Medical': 4151, 'Medicine': 4152, 'Medvedeva': 4153, 'Meeker': 4154, 'Meghalaya': 4155, 'Mehta': 4156, 'Melanesia': 4157, 'Melba': 4158, 'Melinte': 4159, 'Melo': 4160, 'Melora': 4161, 'Mera': 4162, 'Merle': 4163, 'Merman': 4164, 'Merrilee': 4165, 'Mervyn': 4166, 'Mesopotamia': 4167, 'MetLife': 4168, 'Methuselah': 4169, 'Metro': 4170, 'Mexican': 4171, 'Mia': 4172, 'Micheal': 4173, 'Mickey': 4174, 'Midway': 4175, 'Mila': 4176, 'Miletus': 4177, 'Miljenko': 4178, 'Millard': 4179, 'Millennium': 4180, 'Milley': 4181, 'Millie': 4182, 'Milton': 4183, 'Mimi': 4184, 'Min': 4185, 'Mindbenders': 4186, 'Minelli': 4187, 'Ministry': 4188, 'Minoru': 4189, 'Minstrels': 4190, 'Minty': 4191, 'Mirrors': 4192, 'Miss': 4193, 'Mission': 4194, 'Mitch': 4195, 'Mitsubishi': 4196, 'Moana': 4197, 'Mohan': 4198, 'Mohawk': 4199, 'Mongolia': 4200, 'Montauk': 4201, 'Moon': 4202, 'Morarji': 4203, 'Moreno': 4204, 'Morwenna': 4205, 'Moulin': 4206, 'Movement': 4207, 'Mughal': 4208, 'Mukhopadhaya': 4209, 'Mula': 4210, 'Muller': 4211, 'Munich': 4212, 'Muralitharan': 4213, 'Murdoch': 4214, 'Musa': 4215, 'Muttiah': 4216, 'Myanmar': 4217, 'Mysore': 4218, 'NADPH': 4219, 'NBC': 4220, 'NHL': 4221, 'NRG': 4222, 'NW': 4223, 'NY': 4224, 'Nadiuska': 4225, 'Nadiya': 4226, 'Naidu': 4227, 'Napes': 4228, 'Naranjo': 4229, 'Narendra': 4230, 'Nash': 4231, 'Nate': 4232, 'Nationals': 4233, 'Natural': 4234, 'Naval': 4235, 'Nazi': 4236, 'Ne': 4237, 'Neal': 4238, 'Neill': 4239, 'Nellie': 4240, 'Netball': 4241, 'Neuer': 4242, 'Never': 4243, 'Newbern': 4244, 'Newell': 4245, 'Newfoundland': 4246, 'Newly': 4247, 'Nguema': 4248, 'Nice': 4249, 'Nicklaus': 4250, 'Nico': 4251, 'Nicolas': 4252, 'Nicolaus': 4253, 'Nicolette': 4254, 'Nielsen': 4255, 'Nieto': 4256, 'Nighy': 4257, 'Nikolai': 4258, 'Nilsson': 4259, 'Nine': 4260, 'Nissan': 4261, 'Niven': 4262, 'Niʻihau': 4263, 'Nobel': 4264, 'Noma': 4265, 'Non': 4266, 'Noni': 4267, 'Noor': 4268, 'Norfolk': 4269, 'Norma': 4270, 'Northampton': 4271, 'Northeastern': 4272, 'Northridge': 4273, 'Northwest': 4274, 'Norton': 4275, 'Notorious': 4276, 'Notre': 4277, 'Nova': 4278, 'Novermber': 4279, 'Now': 4280, 'Nuclear': 4281, 'Nunavut': 4282, \"O'Donnell\": 4283, \"O'Hara\": 4284, \"O'Keefe\": 4285, \"O'Shaughnessy\": 4286, 'Oates': 4287, 'Obiang': 4288, 'Of': 4289, 'Office': 4290, 'Officer': 4291, 'Oilers': 4292, 'Oleg': 4293, 'Olympiastadion': 4294, 'Olympic': 4295, 'Olympics': 4296, 'Ono': 4297, 'Open': 4298, 'Organization': 4299, 'Original': 4300, 'Orioles': 4301, 'Orji': 4302, 'Osment': 4303, 'Over': 4304, 'P': 4305, 'Pablo': 4306, 'Pack': 4307, 'Pagans': 4308, 'Palestinian': 4309, 'Pangea': 4310, 'Paradise': 4311, 'Parks': 4312, 'Parvati': 4313, 'Pass': 4314, 'Paterson': 4315, 'Patrice': 4316, 'Patterson': 4317, 'Pavel': 4318, 'Pavilion': 4319, 'Pawan': 4320, 'Peabo': 4321, 'Peak': 4322, 'Pearl': 4323, 'Pearson': 4324, 'Peg': 4325, 'Pelicans': 4326, 'Pelé': 4327, 'Pena': 4328, 'Pendleton': 4329, 'Pentateuch': 4330, 'Peoria': 4331, 'Pepi': 4332, 'Pepys': 4333, 'Percy': 4334, 'Perpetual': 4335, 'Persia': 4336, 'Petals': 4337, 'Peterburg': 4338, 'Peterson': 4339, 'Petula': 4340, 'Peyton': 4341, 'Philippine': 4342, 'Photic': 4343, 'Physical': 4344, 'Picon': 4345, 'Pig': 4346, 'Pike': 4347, 'Pinewood': 4348, 'Pink': 4349, 'Pisa': 4350, 'Placement': 4351, 'Planck': 4352, 'Plymouth': 4353, 'Poison': 4354, 'Pola': 4355, 'Polo': 4356, 'Pooja': 4357, 'Poppe': 4358, 'Poppy': 4359, 'Porco': 4360, 'Portman': 4361, 'Post': 4362, 'Postl': 4363, 'Potsdam': 4364, 'Potter': 4365, 'Prater': 4366, 'Prayer': 4367, 'Prefer': 4368, 'Presidential': 4369, 'Prima': 4370, 'Princeton': 4371, 'Prine': 4372, 'Prior': 4373, 'Probate': 4374, 'Progress': 4375, 'Protection': 4376, 'Protocol': 4377, 'Province': 4378, 'Public': 4379, 'Punk': 4380, 'Pure': 4381, 'Puri': 4382, 'Puth': 4383, 'Pyle': 4384, 'Pyramid': 4385, 'Quaid': 4386, 'Quan': 4387, 'Quartet': 4388, 'Quentin': 4389, 'Quinton': 4390, 'Raby': 4391, 'Rachael': 4392, 'Radbourn': 4393, 'Radnor': 4394, 'Rafe': 4395, 'Raitt': 4396, 'Rajan': 4397, 'Rajasthan': 4398, 'Rajendra': 4399, 'Rajesh': 4400, 'Rall': 4401, 'Raman': 4402, 'Ramesh': 4403, 'Rami': 4404, 'Ramin': 4405, 'Rampone': 4406, 'Rana': 4407, 'Randle': 4408, 'Randolph': 4409, 'Rani': 4410, 'Rankin': 4411, 'Ras': 4412, 'Rauch': 4413, 'Raven': 4414, 'Raw': 4415, 'Raymonde': 4416, 'Razia': 4417, 'Razorbacks': 4418, 'Reba': 4419, 'Redbone': 4420, 'Redmond': 4421, 'Reggie': 4422, 'Reginald': 4423, 'Reid': 4424, 'Religion': 4425, 'Remember': 4426, 'Remy': 4427, 'Ren': 4428, 'Renaissance': 4429, 'Renault': 4430, 'Renee': 4431, 'Renée': 4432, 'Reports': 4433, 'Republican': 4434, 'Resorts': 4435, 'Revenue': 4436, 'Revival': 4437, 'Revolutionary': 4438, 'Rexha': 4439, 'Rey': 4440, 'Rhea': 4441, 'Rhys': 4442, 'Rica': 4443, 'Ridley': 4444, 'Rimsky': 4445, 'Rings': 4446, 'Rio': 4447, 'Rises': 4448, 'Rising': 4449, 'Ritchie': 4450, 'Ritter': 4451, 'Rivers': 4452, 'Road': 4453, 'Robespierre': 4454, 'Rochester': 4455, 'Rockapella': 4456, 'Rockets': 4457, 'Rod': 4458, 'Roderick': 4459, 'Roiland': 4460, 'Romania': 4461, 'Romantics': 4462, 'Ronan': 4463, 'Root': 4464, 'Rosa': 4465, 'Rosalind': 4466, 'Rosemary': 4467, 'Rosewood': 4468, 'Rotterdam': 4469, 'Rousseau': 4470, 'Route': 4471, 'Rowe': 4472, 'Roxanne': 4473, 'Royalists': 4474, 'Rudolph': 4475, 'Ruffin': 4476, 'Ruth': 4477, 'Rutherford': 4478, 'République': 4479, 'Röntgen': 4480, 'SIM': 4481, 'SZA': 4482, 'Sacha': 4483, 'Saddam': 4484, 'Safety': 4485, 'Saget': 4486, 'Saints': 4487, 'Saiyan': 4488, 'Salenko': 4489, 'Samberg': 4490, 'Samhain': 4491, 'Samira': 4492, 'Sams': 4493, 'Samuels': 4494, 'Sandor': 4495, 'Sania': 4496, 'Santiago': 4497, 'Sardar': 4498, 'Sarstedt': 4499, 'Sasha': 4500, 'Saskatchewan': 4501, 'Sathyaraj': 4502, 'Satish': 4503, 'Sawyer': 4504, 'Saxon': 4505, 'Scampton': 4506, 'Scanavino': 4507, 'Schmidt': 4508, 'Scooter': 4509, 'Scotia': 4510, 'Scottish': 4511, 'Script': 4512, 'Scuderia': 4513, 'Seahawks': 4514, 'Secret': 4515, 'Secretary': 4516, 'Sedgwick': 4517, 'Seeds': 4518, 'Sellers': 4519, 'Sen': 4520, 'Senators': 4521, 'Serge': 4522, 'Serpent': 4523, 'Service': 4524, 'Sessions': 4525, 'Sevilla': 4526, 'Shai': 4527, 'Shamblin': 4528, 'Sharad': 4529, 'Shazwan': 4530, 'Shea': 4531, 'Shelley': 4532, 'Shep': 4533, 'Shepherd': 4534, 'Sher': 4535, 'Sheree': 4536, 'Sherilyn': 4537, 'Shiavone': 4538, 'Shikhar': 4539, 'Shiva': 4540, 'Shivangi': 4541, 'Shoma': 4542, 'Shortbread': 4543, 'Showcase': 4544, 'Shreyan': 4545, 'Shroff': 4546, 'Shue': 4547, 'Sicily': 4548, 'Sidik': 4549, 'Sikandar': 4550, 'Silva': 4551, 'Silver': 4552, 'Silverstone': 4553, 'Silvestri': 4554, 'Simple': 4555, 'Sinai': 4556, 'Sinclair': 4557, 'Sindhu': 4558, 'Sinha': 4559, 'Sirach': 4560, 'Sissel': 4561, 'Sisto': 4562, 'Six': 4563, 'Sky': 4564, 'Slash': 4565, 'Slate': 4566, 'Slater': 4567, 'Sloan': 4568, 'Sloane': 4569, 'Slotkin': 4570, 'Slough': 4571, 'Slowly': 4572, 'Smulders': 4573, 'Smurf': 4574, 'Smyth': 4575, 'Sohra': 4576, 'Sonalika': 4577, 'Song': 4578, 'Soni': 4579, 'Sonia': 4580, 'Sorrell': 4581, 'Soto': 4582, 'Soucie': 4583, 'Southampton': 4584, 'Southeast': 4585, 'Southwestern': 4586, 'Space': 4587, 'Spaniards': 4588, 'Sparrow': 4589, 'Spartans': 4590, 'Spears': 4591, 'Species': 4592, 'Spirit': 4593, 'Springs': 4594, 'Sreejesh': 4595, 'Stalin': 4596, 'Stapleton': 4597, 'Stark': 4598, 'Starlighters': 4599, 'Stefan': 4600, 'Steinman': 4601, 'Stella': 4602, 'Stephanie': 4603, 'Stilgoe': 4604, 'Stirling': 4605, 'Stockton': 4606, 'Stoller': 4607, 'Stoner': 4608, 'Strae': 4609, 'Strait': 4610, 'Straits': 4611, 'Stratosphere': 4612, 'Struth': 4613, 'Stump': 4614, 'Subah': 4615, 'Subhash': 4616, 'Subway': 4617, 'Sue': 4618, 'Sugar': 4619, 'Sultana': 4620, 'Sumatra': 4621, 'Sumerian': 4622, 'Summers': 4623, 'Sumter': 4624, 'Sundarbans': 4625, 'Sung': 4626, 'Sunshine': 4627, 'Superman': 4628, 'Supply': 4629, 'Sutherland': 4630, 'Sweeney': 4631, 'Swinton': 4632, 'Swiss': 4633, 'Swords': 4634, 'Syed': 4635, 'Sylva': 4636, 'Sylvie': 4637, 'Syria': 4638, 'Sánchez': 4639, \"T'Chaka\": 4640, 'TBA': 4641, 'TBD': 4642, 'TLC': 4643, 'TV': 4644, 'Tae': 4645, 'Tammy': 4646, 'Tanakh': 4647, 'Tania': 4648, 'Tanya': 4649, 'Tar': 4650, 'Tara': 4651, 'Taserface': 4652, 'Task': 4653, 'Tavares': 4654, 'Tecna': 4655, 'Ted': 4656, 'Tegan': 4657, 'Teodoro': 4658, 'Terence': 4659, 'Teresa': 4660, 'Texans': 4661, 'Thakur': 4662, 'Thakuri': 4663, 'Thales': 4664, 'Thangavelu': 4665, 'Thanks': 4666, 'Thanksgiving': 4667, 'Thayer': 4668, 'Theron': 4669, 'Thom': 4670, 'Thorns': 4671, 'Thorogood': 4672, 'Thumama': 4673, 'Tilburg': 4674, 'Tilda': 4675, 'Tippin': 4676, 'Titanic': 4677, 'Titans': 4678, 'To': 4679, 'Tobacco': 4680, 'Tolkien': 4681, 'Tomlin': 4682, 'Toné': 4683, 'Tops': 4684, 'Tori': 4685, 'Tottenham': 4686, 'Tournament': 4687, 'Towns': 4688, 'Toya': 4689, 'Tozier': 4690, 'Tracey': 4691, 'Transamerica': 4692, 'Translation': 4693, 'Travie': 4694, 'Treaty': 4695, 'Tribe': 4696, 'Tribes': 4697, 'Trick': 4698, 'Triennium': 4699, 'TruTV': 4700, 'Trumbo': 4701, 'Tsewang': 4702, 'Tuck': 4703, 'Turtle': 4704, 'Twelve': 4705, 'Twins': 4706, 'Ty': 4707, 'Tylo': 4708, 'Type': 4709, 'Tyra': 4710, 'Tyson': 4711, 'Tzu': 4712, 'UEFA': 4713, 'UK': 4714, 'US$': 4715, 'USMC': 4716, 'USOS': 4717, 'Udaipur': 4718, 'Ukrainian': 4719, 'Umeadi': 4720, 'Universal': 4721, 'Uno': 4722, 'Upper': 4723, 'Ural': 4724, 'Urmila': 4725, 'Utrecht': 4726, 'Uttar': 4727, 'VII': 4728, 'Vajpayee': 4729, 'Valerie': 4730, 'Vanshidhar': 4731, 'Vasudev': 4732, 'Veer': 4733, 'Vela': 4734, 'Venezuela': 4735, 'Venkaiah': 4736, 'Ventures': 4737, 'Venus': 4738, 'Verna': 4739, 'Veronica': 4740, 'Vesnina': 4741, 'Vespasian': 4742, 'Vicki': 4743, 'Vidarbha': 4744, 'Vietnam': 4745, 'Vietnamese': 4746, 'Vikander': 4747, 'Virtue': 4748, 'Vishnu': 4749, 'Vivian': 4750, 'Von': 4751, 'Voormann': 4752, 'Václav': 4753, 'Wake': 4754, 'Walas': 4755, 'Walken': 4756, 'Walsh': 4757, 'Walton': 4758, 'Warfield': 4759, 'Warriors': 4760, 'Wat': 4761, 'Waterloo': 4762, 'Waters': 4763, 'Watts': 4764, 'Way': 4765, 'Waylon': 4766, 'Weather': 4767, 'Weatherwax': 4768, 'Weaver': 4769, 'Webster': 4770, 'Weldon': 4771, 'Welles': 4772, 'Wellingtons': 4773, 'Wells': 4774, 'Were': 4775, 'Wessex': 4776, 'Westlake': 4777, 'Whaite': 4778, 'Wheatley': 4779, 'Wheel': 4780, 'Whispers': 4781, 'Whiteside': 4782, 'Whitfield': 4783, 'Wicked': 4784, 'Wicks': 4785, 'Wiggins': 4786, 'Wilder': 4787, 'Wiley': 4788, 'Williamson': 4789, 'Wilmington': 4790, 'Windows': 4791, 'Winfield': 4792, 'Winningham': 4793, 'Winslow': 4794, 'Wish': 4795, 'Witherspoon': 4796, 'Within': 4797, 'Wizard': 4798, 'Wolfe': 4799, 'Wolfgang': 4800, 'Wolter': 4801, 'Woman': 4802, 'Women': 4803, 'Wondolowski': 4804, 'Wong': 4805, 'Woodbury': 4806, 'Woodrow': 4807, 'Works': 4808, 'Worsnop': 4809, 'Wow': 4810, 'Wroblewitz': 4811, 'Wynn': 4812, 'Wynne': 4813, 'Wåhlin': 4814, 'XCX': 4815, 'XI': 4816, 'XIV': 4817, 'XXVII': 4818, 'XXX': 4819, 'Xanthippus': 4820, 'Y.': 4821, 'Yamamoto': 4822, 'Yang': 4823, 'Yee': 4824, 'Yellowstone': 4825, 'Yente': 4826, 'Yes': 4827, 'Yesterday': 4828, 'Yo': 4829, 'Yoren': 4830, 'Yusuf': 4831, 'Yvette': 4832, 'Zachary': 4833, 'Zane': 4834, 'Zerdin': 4835, 'Zhang': 4836, 'Zheng': 4837, 'Zimbabwe': 4838, 'Zlatan': 4839, 'Zolciak': 4840, 'Zone': 4841, 'Zoo': 4842, '^': 4843, 'a.k.a': 4844, 'absolute': 4845, 'acceptance': 4846, 'acetylcholine': 4847, 'acronym': 4848, 'across': 4849, 'activity': 4850, 'administrative': 4851, 'administrator': 4852, 'advice': 4853, 'aged': 4854, 'agencies': 4855, 'agriculture': 4856, 'alcohol': 4857, 'allies': 4858, 'also': 4859, 'among': 4860, 'aorta': 4861, 'apartheid': 4862, 'apparatus': 4863, 'appear': 4864, 'appointment': 4865, 'arid': 4866, 'arm': 4867, 'artery': 4868, 'articulate': 4869, 'artists': 4870, 'associated': 4871, 'assorted': 4872, 'attack': 4873, 'audience': 4874, 'bailiff': 4875, 'banks': 4876, 'bar': 4877, 'base': 4878, 'baseman': 4879, 'basin': 4880, 'beans': 4881, 'beaver': 4882, 'became': 4883, 'because': 4884, 'become': 4885, 'been': 4886, 'beginning': 4887, 'bifurcates': 4888, 'bin': 4889, 'blind': 4890, 'blue': 4891, 'bonds': 4892, 'books': 4893, 'bottom': 4894, 'boundaries': 4895, 'bowling': 4896, 'brain': 4897, 'branch': 4898, 'brass': 4899, 'brother': 4900, 'brown': 4901, 'brush': 4902, 'bum': 4903, 'called': 4904, 'came': 4905, 'canal': 4906, 'canvas': 4907, 'carbon': 4908, 'cat': 4909, 'cathedral': 4910, 'cavity': 4911, 'celtic': 4912, 'centers': 4913, 'chairman': 4914, 'champion': 4915, 'chape': 4916, 'character': 4917, 'charge': 4918, 'chief': 4919, 'child': 4920, 'circa': 4921, 'citric': 4922, 'civil': 4923, 'classical': 4924, 'clock': 4925, 'cloud': 4926, 'coastal': 4927, 'college': 4928, 'come': 4929, 'comedy': 4930, 'comes': 4931, 'committee': 4932, 'compounds': 4933, 'conference': 4934, 'connective': 4935, 'connects': 4936, 'consciousness': 4937, 'consent': 4938, 'constable': 4939, 'continental': 4940, 'continents': 4941, 'converge': 4942, 'converted': 4943, 'crab': 4944, 'creation': 4945, 'cross': 4946, 'crossover': 4947, 'crucifixes': 4948, 'cultural': 4949, 'currently': 4950, 'cytoskeleton': 4951, 'cytosol': 4952, 'database': 4953, 'dead': 4954, 'deep': 4955, 'demand': 4956, 'democratic': 4957, 'der': 4958, 'descendant': 4959, 'descent': 4960, 'desert': 4961, 'designation': 4962, 'designer': 4963, 'diagram': 4964, 'dialect': 4965, 'dialogue': 4966, 'digits': 4967, 'disabilities': 4968, 'disciples': 4969, 'discovered': 4970, 'district': 4971, 'divergent': 4972, 'dollar': 4973, 'doo': 4974, 'dorsal': 4975, 'draft': 4976, 'driving': 4977, 'drought': 4978, 'dynamic': 4979, 'earlier': 4980, 'earthquakes': 4981, 'edition': 4982, 'education': 4983, 'either': 4984, 'elections': 4985, 'electromagnetic': 4986, 'electrons': 4987, 'emperor': 4988, 'empire': 4989, 'en': 4990, 'engine': 4991, 'engineers': 4992, 'entire': 4993, 'entrances': 4994, 'epidermis': 4995, 'epithelium': 4996, 'era': 4997, 'essential': 4998, 'estuary': 4999, 'et': 5000, 'even': 5001, 'event': 5002, 'events': 5003, 'everything': 5004, 'exempt': 5005, 'exercise': 5006, 'exile': 5007, 'expansion': 5008, 'expectancy': 5009, 'explosion': 5010, 'expression': 5011, 'extensive': 5012, 'extracellular': 5013, 'eye': 5014, 'fall': 5015, 'fallopian': 5016, 'farthest': 5017, 'fascia': 5018, 'fashion': 5019, 'fat': 5020, 'feathers': 5021, 'female': 5022, 'few': 5023, 'fibroblasts': 5024, 'fictional': 5025, 'fifteen': 5026, 'flag': 5027, 'florida': 5028, 'flour': 5029, 'fluoride': 5030, 'folk': 5031, 'folklore': 5032, 'foot': 5033, 'form': 5034, 'fourth': 5035, 'frame': 5036, 'fruit': 5037, 'fuel': 5038, 'full': 5039, 'functional': 5040, 'gameplay': 5041, 'gamete': 5042, 'gazing': 5043, 'gestation': 5044, 'gland': 5045, 'golden': 5046, 'governing': 5047, 'grams': 5048, 'grass': 5049, 'gravity': 5050, 'greater': 5051, 'green': 5052, 'grizzlies': 5053, 'gun': 5054, 'gypsum': 5055, 'halite': 5056, 'hard': 5057, 'health': 5058, 'herb': 5059, 'hip': 5060, 'hopping': 5061, 'hormone': 5062, 'if': 5063, 'iliac': 5064, 'ilium': 5065, 'immediately': 5066, 'immunity': 5067, 'increased': 5068, 'individual': 5069, 'industry': 5070, 'inequality': 5071, 'information': 5072, 'infrared': 5073, 'initial': 5074, 'inside': 5075, 'interval': 5076, 'invasion': 5077, 'items': 5078, 'jejunum': 5079, 'judiciary': 5080, 'juxtaglomerular': 5081, 'kidney': 5082, 'kidneys': 5083, 'king': 5084, 'knees': 5085, 'largely': 5086, 'later': 5087, 'law': 5088, 'lb': 5089, 'leading': 5090, 'leaves': 5091, 'level': 5092, 'liberty': 5093, 'ligament': 5094, 'live': 5095, 'liver': 5096, 'local': 5097, 'lung': 5098, 'lynx': 5099, 'make': 5100, 'male': 5101, 'mantle': 5102, 'manuscript': 5103, 'marks': 5104, 'marrow': 5105, 'masking': 5106, 'mate': 5107, 'materials': 5108, 'matter': 5109, 'medial': 5110, 'medially': 5111, 'median': 5112, 'mens\"/\"ladies': 5113, 'merchant': 5114, 'metre': 5115, 'mid-1990s': 5116, 'midnight': 5117, 'militia': 5118, 'millennium': 5119, 'min': 5120, 'miners': 5121, 'minimum': 5122, 'minister': 5123, 'minor': 5124, 'minority': 5125, 'mismanagement': 5126, 'mix': 5127, 'mixture': 5128, 'modern': 5129, 'molecules': 5130, 'monoxide': 5131, 'mountain': 5132, 'move': 5133, 'much': 5134, 'multi': 5135, 'music': 5136, 'na': 5137, 'names': 5138, 'nations': 5139, 'nature': 5140, 'naval': 5141, 'naïve': 5142, 'negative': 5143, 'next': 5144, 'nicotine': 5145, 'nights': 5146, 'nor': 5147, 'northwest': 5148, 'obverse': 5149, 'offices': 5150, 'officials': 5151, 'often': 5152, 'opposition': 5153, 'optic': 5154, 'order': 5155, 'organic': 5156, 'organizations': 5157, 'others': 5158, 'out': 5159, 'outer': 5160, 'overall': 5161, 'overs': 5162, 'owner': 5163, 'owners': 5164, 'p.m': 5165, 'pair': 5166, 'parietal': 5167, 'park': 5168, 'parliamentary': 5169, 'passed': 5170, 'pastry': 5171, 'patellar': 5172, 'patient': 5173, 'peptide': 5174, 'peripheral': 5175, 'personal': 5176, 'personality': 5177, 'physical': 5178, 'piece': 5179, 'plant': 5180, 'plasma': 5181, 'pleura': 5182, 'plus': 5183, 'policies': 5184, 'policy': 5185, 'primary': 5186, 'primeval': 5187, 'princess': 5188, 'prisoners': 5189, 'produced': 5190, 'prohibited': 5191, 'prong': 5192, 'proper': 5193, 'public': 5194, 'quickly': 5195, 'quotation': 5196, 'radiation': 5197, 'ranking': 5198, 'raw': 5199, 'rays': 5200, 'reaches': 5201, 'reaction': 5202, 'reading': 5203, 'recorded': 5204, 'reduced': 5205, 'regular': 5206, 'release': 5207, 'renal': 5208, 'renewable': 5209, 'replacement': 5210, 'representative': 5211, 'reproduction': 5212, 'respective': 5213, 'restaurant': 5214, 'rise': 5215, 'road': 5216, 'rocksteady': 5217, 'rodents': 5218, 'room': 5219, 'rugby': 5220, 'runners': 5221, 'ryegrass': 5222, 'salivary': 5223, 'salt': 5224, 'salts': 5225, 'scattered': 5226, 'scenes': 5227, 'science': 5228, 'sciences': 5229, 'seasons': 5230, 'secret': 5231, 'section': 5232, 'sections': 5233, 'sedan': 5234, 'server': 5235, 'service': 5236, 'severe': 5237, 'she': 5238, 'sheriff': 5239, 'ships': 5240, 'short': 5241, 'shortly': 5242, 'shoulder': 5243, 'since': 5244, 'sinus': 5245, 'sixteen': 5246, 'sixth': 5247, 'slightly': 5248, 'slope': 5249, 'slow': 5250, 'smile': 5251, 'snake': 5252, 'solar': 5253, 'soldiers': 5254, 'solid': 5255, 'southwest': 5256, 'special': 5257, 'spouse': 5258, 'spread': 5259, 'stable': 5260, 'stage': 5261, 'stapes': 5262, 'star': 5263, 'steroid': 5264, 'storm': 5265, 'stratosphere': 5266, 'strength': 5267, 'strikeouts': 5268, 'students': 5269, 'studies': 5270, 'studio': 5271, 'subclavian': 5272, 'subducted': 5273, 'such': 5274, 'sung': 5275, 'sunlight': 5276, 'surrounding': 5277, 'syllables': 5278, 'syndrome': 5279, 'synthase': 5280, 'table': 5281, 'tale': 5282, 'tar': 5283, 'teaching': 5284, 'technology': 5285, 'temperature': 5286, 'tenant': 5287, 'them': 5288, 'theory': 5289, 'thirteen': 5290, 'thoracic': 5291, 'throw': 5292, 'tibiae': 5293, 'tip': 5294, 'titles': 5295, 'together': 5296, 'total': 5297, 'tour': 5298, 'tower': 5299, 'tract': 5300, 'trade': 5301, 'traditions': 5302, 'transport': 5303, 'transverse': 5304, 'tropics': 5305, 'troposphere': 5306, 'trunk': 5307, 'tube': 5308, 'tubes': 5309, 'twin': 5310, 'ud': 5311, 'ultraviolet': 5312, 'uncertain': 5313, 'unitary': 5314, 'unlimited': 5315, 'until': 5316, 'upcoming': 5317, 'use': 5318, 'vapor': 5319, 'vegetable': 5320, 'vehicle': 5321, 'venting': 5322, 'vessel': 5323, 'vessels': 5324, 'victims': 5325, 'vocalists': 5326, 'vote': 5327, 'wage': 5328, 'walking': 5329, 'wall': 5330, 'waste': 5331, 'waters': 5332, 'wealth': 5333, 'week': 5334, 'whip': 5335, 'wife': 5336, 'wild': 5337, 'will': 5338, 'will.i.am': 5339, 'winless': 5340, 'winning': 5341, 'woman': 5342, 'wood': 5343, 'wooden': 5344, 'working': 5345, 'written': 5346, 'y': 5347, 'yes': 5348, 'youth': 5349, 'zone': 5350, 'zum': 5351, 'zur': 5352, '£': 5353, 'Élodie': 5354, 'Владимир': 5355, 'Путин': 5356, '“': 5357, '”': 5358})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D0T6sriIt8V"
      },
      "source": [
        "BATCH_SIZE = 8\r\n",
        "train_iterator_third, valid_iterator_third = data.BucketIterator.splits(\r\n",
        "                                (Train_Dataset_Third, Valid_Dataset_Third), \r\n",
        "                                batch_size = BATCH_SIZE,\r\n",
        "                                sort_key = lambda x: len(x.questions_third),\r\n",
        "                                sort_within_batch=True ,\r\n",
        "                                device = device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hrn6YFVIibd",
        "outputId": "28cb6c36-efc4-406c-cb47-8dc4b61ca2d4"
      },
      "source": [
        "for i, batch in enumerate(train_iterator_third):\r\n",
        "  print(type(batch), type(batch.answers_third), type(batch.questions_third))\r\n",
        "  break"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torchtext.data.batch.Batch'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq7BwOO4EzFd"
      },
      "source": [
        "## Building the Seq2Seq Model\n",
        "\n",
        "### Encoder\n",
        "\n",
        "The encoder is similar to the previous one, with the multi-layer LSTM swapped for a single-layer GRU. We also don't pass the dropout as an argument to the GRU as that dropout is used between each layer of a multi-layered RNN. As we only have a single layer, PyTorch will display a warning if we try and use pass a dropout value to it.\n",
        "\n",
        "Another thing to note about the GRU is that it only requires and returns a hidden state, there is no cell state like in the LSTM.\n",
        "\n",
        "$$\\begin{align*}\n",
        "h_t &= \\text{GRU}(e(x_t), h_{t-1})\\\\\n",
        "(h_t, c_t) &= \\text{LSTM}(e(x_t), h_{t-1}, c_{t-1})\\\\\n",
        "h_t &= \\text{RNN}(e(x_t), h_{t-1})\n",
        "\\end{align*}$$\n",
        "\n",
        "From the equations above, it looks like the RNN and the GRU are identical. Inside the GRU, however, is a number of *gating mechanisms* that control the information flow in to and out of the hidden state (similar to an LSTM). Again, for more info, check out [this](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) excellent post. \n",
        "\n",
        "The rest of the encoder should be very familar from the last session, it takes in a sequence, $X = \\{x_1, x_2, ... , x_T\\}$, passes it through the embedding layer, recurrently calculates hidden states, $H = \\{h_1, h_2, ..., h_T\\}$, and returns a context vector (the final hidden state), $z=h_T$.\n",
        "\n",
        "$$h_t = \\text{EncoderGRU}(e(x_t), h_{t-1})$$\n",
        "\n",
        "This is identical to the encoder of the general seq2seq model, with all the \"magic\" happening inside the GRU (green).\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq5.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11CnDhTkEzFd"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim) #no dropout as only one layer!\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded) #no cell state!\n",
        "        \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhBViJ4-EzFe"
      },
      "source": [
        "## Decoder\n",
        "\n",
        "The decoder is where the implementation differs significantly from the previous model and we alleviate some of the information compression.\n",
        "\n",
        "Instead of the GRU in the decoder taking just the embedded target token, $d(y_t)$ and the previous hidden state $s_{t-1}$ as inputs, it also takes the context vector $z$. \n",
        "\n",
        "$$s_t = \\text{DecoderGRU}(d(y_t), s_{t-1}, z)$$\n",
        "\n",
        "Note how this context vector, $z$, does not have a $t$ subscript, meaning we re-use the same context vector returned by the encoder for every time-step in the decoder. \n",
        "\n",
        "Before, we predicted the next token, $\\hat{y}_{t+1}$, with the linear layer, $f$, only using the top-layer decoder hidden state at that time-step, $s_t$, as $\\hat{y}_{t+1}=f(s_t^L)$. Now, we also pass the embedding of current token, $d(y_t)$ and the context vector, $z$ to the linear layer.\n",
        "\n",
        "$$\\hat{y}_{t+1} = f(d(y_t), s_t, z)$$\n",
        "\n",
        "Thus, our decoder now looks something like this:\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq6.png?raw=1)\n",
        "\n",
        "Note, the initial hidden state, $s_0$, is still the context vector, $z$, so when generating the first token we are actually inputting two identical context vectors into the GRU.\n",
        "\n",
        "How do these two changes reduce the information compression? Well, hypothetically the decoder hidden states, $s_t$, no longer need to contain information about the source sequence as it is always available as an input. Thus, it only needs to contain information about what tokens it has generated so far. The addition of $y_t$ to the linear layer also means this layer can directly see what the token is, without having to get this information from the hidden state. \n",
        "\n",
        "However, this hypothesis is just a hypothesis, it is impossible to determine how the model actually uses the information provided to it (don't listen to anyone that says differently). Nevertheless, it is a solid intuition and the results seem to indicate that this modifications are a good idea!\n",
        "\n",
        "Within the implementation, we will pass $d(y_t)$ and $z$ to the GRU by concatenating them together, so the input dimensions to the GRU are now `emb_dim + hid_dim` (as context vector will be of size `hid_dim`). The linear layer will take $d(y_t), s_t$ and $z$ also by concatenating them together, hence the input dimensions are now `emb_dim + hid_dim*2`. We also don't pass a value of dropout to the GRU as it only uses a single layer.\n",
        "\n",
        "`forward` now takes a `context` argument. Inside of `forward`, we concatenate $y_t$ and $z$ as `emb_con` before feeding to the GRU, and we concatenate $d(y_t)$, $s_t$ and $z$ together as `output` before feeding it through the linear layer to receive our predictions, $\\hat{y}_{t+1}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRA8hkiLEzFh"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim + hid_dim, hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, context):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #context = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n layers and n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        #context = [1, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        emb_con = torch.cat((embedded, context), dim = 2)\n",
        "            \n",
        "        #emb_con = [1, batch size, emb dim + hid dim]\n",
        "            \n",
        "        output, hidden = self.rnn(emb_con, hidden)\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "        \n",
        "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), \n",
        "                           dim = 1)\n",
        "        \n",
        "        #output = [batch size, emb dim + hid dim * 2]\n",
        "        \n",
        "        prediction = self.fc_out(output)\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqR66FM2EzFj"
      },
      "source": [
        "## Seq2Seq Model\n",
        "\n",
        "Putting the encoder and decoder together, we get:\n",
        "\n",
        "![](https://github.com/bentrevett/pytorch-seq2seq/blob/master/assets/seq2seq7.png?raw=1)\n",
        "\n",
        "Again, in this implementation we need to ensure the hidden dimensions in both the encoder and the decoder are the same.\n",
        "\n",
        "Briefly going over all of the steps:\n",
        "- the `outputs` tensor is created to hold all predictions, $\\hat{Y}$\n",
        "- the source sequence, $X$, is fed into the encoder to receive a `context` vector\n",
        "- the initial decoder hidden state is set to be the `context` vector, $s_0 = z = h_T$\n",
        "- we use a batch of `<sos>` tokens as the first `input`, $y_1$\n",
        "- we then decode within a loop:\n",
        "  - inserting the input token $y_t$, previous hidden state, $s_{t-1}$, and the context vector, $z$, into the decoder\n",
        "  - receiving a prediction, $\\hat{y}_{t+1}$, and a new hidden state, $s_t$\n",
        "  - we then decide if we are going to teacher force or not, setting the next input as appropriate (either the ground truth next token in the target sequence or the highest predicted next token)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDyNrQ8VEzFk"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is the context\n",
        "        context = self.encoder(src)\n",
        "        \n",
        "        #context also used as the initial hidden state of the decoder\n",
        "        hidden = context\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state and the context state\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden = self.decoder(input, hidden, context)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxVMMPUyEzFk"
      },
      "source": [
        "# Training the Seq2Seq Model\n",
        "\n",
        "The rest of this session is very similar to the previous one. \n",
        "\n",
        "We initialise our encoder, decoder and seq2seq model (placing it on the GPU if we have one). As before, the embedding dimensions and the amount of dropout used can be different between the encoder and the decoder, but the hidden dimensions must remain the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDG6jOSuEzFk"
      },
      "source": [
        "INPUT_DIM = len(Questions_Third.vocab)\n",
        "OUTPUT_DIM = len(Answers_Third.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLIc1l6CEzFk"
      },
      "source": [
        "Next, we initialize our parameters. The paper states the parameters are initialized from a normal distribution with a mean of 0 and a standard deviation of 0.01, i.e. $\\mathcal{N}(0, 0.01)$. \n",
        "\n",
        "It also states we should initialize the recurrent parameters to a special initialization, however to keep things simple we'll also initialize them to $\\mathcal{N}(0, 0.01)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgqMqq-oEzFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "591cf1fe-5dc9-438f-cd6e-5487e8bb2447"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(9694, 256)\n",
              "    (rnn): GRU(256, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(5359, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc_out): Linear(in_features=1280, out_features=5359, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd1QoOsUEzFl"
      },
      "source": [
        "We print out the number of parameters.\n",
        "\n",
        "Even though we only have a single layer RNN for our encoder and decoder we actually have **more** parameters  than the last model. This is due to the increased size of the inputs to the GRU and the linear layer. However, it is not a significant amount of parameters and causes a minimal amount of increase in training time (~3 seconds per epoch extra)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IggCwIBgEzFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43861c4d-5ff7-4039-e2a7-fb8479aec143"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 13,870,319 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiH54qzHEzFl"
      },
      "source": [
        "We initiaize our optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO1_eoG7EzFl"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwkV3FKlEzFl"
      },
      "source": [
        "We also initialize the loss function, making sure to ignore the loss on `<pad>` tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0DAbGbcEzFl"
      },
      "source": [
        "TRG_PAD_IDX = Answers_Third.vocab.stoi[Answers_Third.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTay7E9rEzFm"
      },
      "source": [
        "We then create the training loop..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5OYuoFdEzFm"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.questions_third\n",
        "        trg = batch.answers_third\n",
        "\n",
        "        src = src.permute(1, 0)\n",
        "        trg = trg.permute(1, 0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:]\n",
        "        trg = torch.reshape(trg, (-1,)) #trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rfUx5lhEzFm"
      },
      "source": [
        "...and the evaluation loop, remembering to set the model to `eval` mode and turn off teaching forcing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw022pw0EzFm"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.questions_third\n",
        "            trg = batch.answers_third\n",
        "\n",
        "            src = src.permute(1, 0)\n",
        "            trg = trg.permute(1, 0)\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:]#.view(-1)\n",
        "            trg = torch.reshape(trg, (-1,))\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E43h8dnQEzFm"
      },
      "source": [
        "We'll also define the function that calculates how long an epoch takes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTAmu3-EEzFm"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbCGyO4ZEzFm"
      },
      "source": [
        "Then, we train our model, saving the parameters that give us the best validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjFyRUK9EzFm"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator_third, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator_third, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}